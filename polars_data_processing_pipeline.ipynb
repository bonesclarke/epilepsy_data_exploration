{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65183a8e",
   "metadata": {},
   "source": [
    "# Epilepsy Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043b8dc",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e78b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epilepsy Data Processing Pipeline\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pywt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MNE Libraries\n",
    "import mne\n",
    "from mne import Epochs, pick_types, events_from_annotations\n",
    "from mne.channels import make_standard_montage\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.datasets import eegbci\n",
    "from mne.decoding import CSP\n",
    "from mne.preprocessing import ICA, create_eog_epochs, create_ecg_epochs\n",
    "from mne_connectivity import spectral_connectivity_epochs\n",
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "# Scipy and Scikit-learn\n",
    "from scipy import signal, stats\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b0576",
   "metadata": {},
   "source": [
    "## Load Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4ad4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata_from_json(data_dir='data'):\n",
    "    \"\"\"Load patient and seizure metadata from JSON files using Polars\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    all_seizures = []\n",
    "    all_patients = []\n",
    "    \n",
    "    json_files = sorted(list(data_dir.glob('**/*.json')))\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract patient-level information\n",
    "            patient_info = {\n",
    "                'patient_id': data['patient_id'],\n",
    "                'sampling_rate_hz': data['sampling_rate_hz'],\n",
    "                'num_channels': len(data['channels']),\n",
    "                'json_file_path': str(json_file)\n",
    "            }\n",
    "            \n",
    "            if 'file_name' in data:\n",
    "                patient_info['file_name'] = data['file_name']\n",
    "                patient_info['registration_start_time'] = data.get('registration_start_time')\n",
    "                patient_info['registration_end_time'] = data.get('registration_end_time')\n",
    "            \n",
    "            all_patients.append(patient_info)\n",
    "            \n",
    "            # Process each seizure\n",
    "            for seizure in data['seizures']:\n",
    "                seizure_record = {\n",
    "                    'patient_id': data['patient_id'],\n",
    "                    'sampling_rate_hz': data['sampling_rate_hz'],\n",
    "                    'seizure_number': seizure['seizure_number']\n",
    "                }\n",
    "                \n",
    "                for key, value in seizure.items():\n",
    "                    seizure_record[key] = value\n",
    "                \n",
    "                if 'file_name' in patient_info and 'file_name' not in seizure:\n",
    "                    seizure_record['file_name'] = patient_info['file_name']\n",
    "                    if 'registration_start_time' in patient_info:\n",
    "                        seizure_record['registration_start_time'] = patient_info['registration_start_time']\n",
    "                        seizure_record['registration_end_time'] = patient_info['registration_end_time']\n",
    "                \n",
    "                all_seizures.append(seizure_record)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to Polars DataFrames\n",
    "    seizures_df = pl.DataFrame(all_seizures)\n",
    "    patients_df = pl.DataFrame(all_patients)\n",
    "    \n",
    "    return seizures_df, patients_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e55050",
   "metadata": {},
   "source": [
    "## Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e890490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_psd_features(raw, fmin=0.5, fmax=50):\n",
    "    \"\"\"Extract PSD features without restrictive conditions\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Get data\n",
    "        data = raw.get_data()\n",
    "        sfreq = raw.info['sfreq']\n",
    "        \n",
    "        # Calculate PSD using multitaper (more robust than welch for non-stationary signals)\n",
    "        psds, freqs = psd_array_multitaper(\n",
    "            data, sfreq, fmin=fmin, fmax=fmax, \n",
    "            adaptive=True, normalization='full', verbose=False\n",
    "        )\n",
    "        \n",
    "        # Extended frequency bands\n",
    "        bands = {\n",
    "            'delta': (0.5, 4),\n",
    "            'theta': (4, 8),\n",
    "            'alpha': (8, 12),\n",
    "            'low_beta': (12, 20),\n",
    "            'high_beta': (20, 30),\n",
    "            'gamma': (30, 50),\n",
    "            'high_gamma': (50, 100)\n",
    "        }\n",
    "        \n",
    "        band_powers = {}\n",
    "        \n",
    "        for band_name, (low_freq, high_freq) in bands.items():\n",
    "            freq_mask = (freqs >= low_freq) & (freqs < high_freq)\n",
    "            if freq_mask.any():\n",
    "                band_power = np.mean(psds[:, freq_mask], axis=1)\n",
    "                band_powers[band_name] = band_power\n",
    "                \n",
    "                # Store band power statistics\n",
    "                features[f'psd_{band_name}_mean'] = float(np.mean(band_power))\n",
    "                features[f'psd_{band_name}_std'] = float(np.std(band_power))\n",
    "                features[f'psd_{band_name}_cv'] = float(np.std(band_power) / (np.mean(band_power) + 1e-10))  # Coefficient of variation\n",
    "        \n",
    "        # Calculate band power ratios (important for seizure detection)\n",
    "        if 'theta' in band_powers and 'alpha' in band_powers:\n",
    "            features['psd_theta_alpha_ratio'] = float(np.mean(band_powers['theta']) / (np.mean(band_powers['alpha']) + 1e-10))\n",
    "        \n",
    "        if 'delta' in band_powers and 'alpha' in band_powers:\n",
    "            features['psd_delta_alpha_ratio'] = float(np.mean(band_powers['delta']) / (np.mean(band_powers['alpha']) + 1e-10))\n",
    "        \n",
    "        if 'low_beta' in band_powers and 'high_beta' in band_powers:\n",
    "            features['psd_beta_ratio'] = float(np.mean(band_powers['high_beta']) / (np.mean(band_powers['low_beta']) + 1e-10))\n",
    "        \n",
    "        # Spectral edge frequencies (SEF50, SEF90, SEF95)\n",
    "        mean_psd = np.mean(psds, axis=0)\n",
    "        cumsum_psd = np.cumsum(mean_psd)\n",
    "        cumsum_psd = cumsum_psd / cumsum_psd[-1]\n",
    "        \n",
    "        for percentile in [50, 75, 90, 95]:\n",
    "            edge_idx = np.where(cumsum_psd >= percentile/100)[0]\n",
    "            if len(edge_idx) > 0:\n",
    "                features[f'psd_sef{percentile}'] = float(freqs[edge_idx[0]])\n",
    "            else:\n",
    "                features[f'psd_sef{percentile}'] = float(freqs[-1])\n",
    "        \n",
    "        # Spectral centroid and spread\n",
    "        freq_weights = freqs * mean_psd\n",
    "        spectral_centroid = np.sum(freq_weights) / (np.sum(mean_psd) + 1e-10)\n",
    "        features['psd_spectral_centroid'] = float(spectral_centroid)\n",
    "        \n",
    "        # Spectral spread (standard deviation around centroid)\n",
    "        spectral_spread = np.sqrt(np.sum(((freqs - spectral_centroid) ** 2) * mean_psd) / (np.sum(mean_psd) + 1e-10))\n",
    "        features['psd_spectral_spread'] = float(spectral_spread)\n",
    "        \n",
    "        # Spectral skewness and kurtosis\n",
    "        if spectral_spread > 0:\n",
    "            spectral_skewness = np.sum(((freqs - spectral_centroid) ** 3) * mean_psd) / ((spectral_spread ** 3) * np.sum(mean_psd) + 1e-10)\n",
    "            spectral_kurtosis = np.sum(((freqs - spectral_centroid) ** 4) * mean_psd) / ((spectral_spread ** 4) * np.sum(mean_psd) + 1e-10)\n",
    "            features['psd_spectral_skewness'] = float(spectral_skewness)\n",
    "            features['psd_spectral_kurtosis'] = float(spectral_kurtosis)\n",
    "        else:\n",
    "            features['psd_spectral_skewness'] = 0.0\n",
    "            features['psd_spectral_kurtosis'] = 0.0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in enhanced PSD calculation: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c7eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_psd_features_by_region(raw, fmin=0.5, fmax=50):\n",
    "    # Define channel groups\n",
    "    channel_groups = {\n",
    "        'frontal': ['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'Fz'],\n",
    "        'central': ['C3', 'C4', 'Cz'],\n",
    "        'parietal': ['P3', 'P4', 'Pz'],\n",
    "        'occipital': ['O1', 'O2'],\n",
    "        'temporal': ['T3', 'T4', 'T5', 'T6']\n",
    "    }\n",
    "    \n",
    "    regional_features = {}\n",
    "    available_channels = raw.ch_names\n",
    "    \n",
    "    for region, channel_list in channel_groups.items():\n",
    "        # Find matching channels (handle different naming conventions)\n",
    "        region_channels = []\n",
    "        for ch in available_channels:\n",
    "            ch_clean = ch.upper().replace('EEG', '').replace('-', '').replace(' ', '').strip()\n",
    "            for target_ch in channel_list:\n",
    "                if target_ch.upper() in ch_clean or ch_clean in target_ch.upper():\n",
    "                    region_channels.append(ch)\n",
    "                    break\n",
    "        \n",
    "        try:\n",
    "            # Pick channels for this region\n",
    "            raw_region = raw.copy().pick(region_channels)\n",
    "            \n",
    "            # Extract features for this region\n",
    "            region_psd_features = extract_psd_features(raw_region, fmin, fmax)\n",
    "            \n",
    "            # Add region prefix to feature names\n",
    "            for feature_name, value in region_psd_features.items():\n",
    "                regional_features[f'{region}_{feature_name}'] = value\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Could not process {region} region: {e}\")   \n",
    "    \n",
    "    return regional_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a5b817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_seizure_time(time_str):\n",
    "    if pd.isna(time_str) or time_str == '' or time_str is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        time_str = str(time_str).strip()\n",
    "        \n",
    "        # Handle HH:MM:SS or MM:SS format\n",
    "        if '.' in time_str:\n",
    "            parts = time_str.split('.')\n",
    "            if len(parts) == 3:\n",
    "                hours, minutes, seconds = map(float, parts)\n",
    "                return hours * 3600 + minutes * 60 + seconds\n",
    "            elif len(parts) == 2:\n",
    "                minutes, seconds = map(float, parts)\n",
    "                return minutes * 60 + seconds\n",
    "        \n",
    "        # Handle pure seconds\n",
    "        return float(time_str)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse time '{time_str}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b7b02",
   "metadata": {},
   "source": [
    "### Differential Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9574fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_differential_entropy(data, sfreq, bands=None):\n",
    "    if bands is None:\n",
    "        bands = {\n",
    "            'delta': (0.5, 4),\n",
    "            'theta': (4, 8),\n",
    "            'alpha': (8, 12),\n",
    "            'beta': (12, 30),\n",
    "            'gamma': (30, 50),\n",
    "            'high_gamma': (50, 100)  # Added high gamma\n",
    "        }\n",
    "    \n",
    "    features = {}\n",
    "    n_channels = data.shape[0]\n",
    "    \n",
    "    for band_name, (low_freq, high_freq) in bands.items():\n",
    "        # Bandpass filter for each band\n",
    "        try:\n",
    "            filtered_data = mne.filter.filter_data(\n",
    "                data, sfreq, l_freq=low_freq, h_freq=high_freq, \n",
    "                verbose=False, method='fir', fir_design='firwin'\n",
    "            )\n",
    "            \n",
    "            # Calculate variance for each channel\n",
    "            variances = np.var(filtered_data, axis=1)\n",
    "            \n",
    "            # Calculate DE: 0.5 * log(2 * pi * e * variance)\n",
    "            # Adding small epsilon to avoid log(0)\n",
    "            de_values = 0.5 * np.log(2 * np.pi * np.e * (variances + 1e-10))\n",
    "            \n",
    "            # Store statistics\n",
    "            features[f'de_{band_name}_mean'] = float(np.mean(de_values))\n",
    "            features[f'de_{band_name}_std'] = float(np.std(de_values))\n",
    "            features[f'de_{band_name}_median'] = float(np.median(de_values))\n",
    "            features[f'de_{band_name}_max'] = float(np.max(de_values))\n",
    "            features[f'de_{band_name}_min'] = float(np.min(de_values))\n",
    "            \n",
    "            # Asymmetry features (frontal asymmetry is important for emotion/seizure)\n",
    "            if n_channels >= 2:\n",
    "                # Calculate hemispheric asymmetry\n",
    "                left_channels = de_values[:n_channels//2]\n",
    "                right_channels = de_values[n_channels//2:]\n",
    "                min_len = min(len(left_channels), len(right_channels))\n",
    "                if min_len > 0:\n",
    "                    asymmetry = left_channels[:min_len] - right_channels[:min_len]\n",
    "                    features[f'de_{band_name}_asymmetry_mean'] = float(np.mean(asymmetry))\n",
    "                    features[f'de_{band_name}_asymmetry_std'] = float(np.std(asymmetry))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating DE for {band_name}: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f767c41b",
   "metadata": {},
   "source": [
    "### Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15fe2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_simple_propagation_features(raw, seizure_start=None, seizure_end=None):\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        data = raw.get_data()\n",
    "        sfreq = raw.info['sfreq']\n",
    "        n_channels, n_samples = data.shape\n",
    "        \n",
    "        # Apply bandpass filter for seizure activity\n",
    "        data_filtered = mne.filter.filter_data(\n",
    "            data, sfreq, l_freq=3, h_freq=30, verbose=False\n",
    "        )\n",
    "        \n",
    "        # Calculate channel-wise power changes\n",
    "        onset_times = []\n",
    "        \n",
    "        for ch_idx in range(n_channels):\n",
    "            channel_data = data_filtered[ch_idx, :]\n",
    "            \n",
    "            # Calculate envelope\n",
    "            analytic_signal = signal.hilbert(channel_data)\n",
    "            envelope = np.abs(analytic_signal)\n",
    "            \n",
    "            # Simple smoothing\n",
    "            window_samples = max(3, int(0.1 * sfreq))\n",
    "            if window_samples % 2 == 0:\n",
    "                window_samples += 1\n",
    "            envelope_smooth = signal.savgol_filter(\n",
    "                envelope, \n",
    "                window_samples, \n",
    "                min(1, window_samples-1)\n",
    "            )\n",
    "            \n",
    "            # Find the time of maximum activity\n",
    "            if seizure_start is not None and seizure_end is not None:\n",
    "                # Look in seizure window\n",
    "                start_sample = max(0, int(seizure_start * sfreq))\n",
    "                end_sample = min(n_samples, int(seizure_end * sfreq))\n",
    "                if start_sample < end_sample:\n",
    "                    seizure_segment = envelope_smooth[start_sample:end_sample]\n",
    "                    if len(seizure_segment) > 0:\n",
    "                        max_idx = np.argmax(seizure_segment) + start_sample\n",
    "                        onset_times.append(max_idx / sfreq)\n",
    "            else:\n",
    "                # Use entire recording\n",
    "                if len(envelope_smooth) > 0:\n",
    "                    max_idx = np.argmax(envelope_smooth)\n",
    "                    onset_times.append(max_idx / sfreq)\n",
    "        \n",
    "        # Only calculate propagation statistics if we have onset times\n",
    "        if len(onset_times) > 0:\n",
    "            # Calculate delays between channels\n",
    "            onset_times = np.array(onset_times)\n",
    "            sorted_onsets = np.sort(onset_times)\n",
    "            delays = np.diff(sorted_onsets)\n",
    "            \n",
    "            # Simple propagation speed estimate (assuming 10cm average distance)\n",
    "            avg_distance_mm = 100  # 100mm = 10cm\n",
    "            speeds = []\n",
    "            \n",
    "            # Calculate speeds only for positive delays\n",
    "            positive_delays = delays[delays > 0]\n",
    "            if len(positive_delays) > 0:\n",
    "                for delay in positive_delays:\n",
    "                    speed = avg_distance_mm / delay\n",
    "                    speeds.append(speed)\n",
    "            \n",
    "            # Calculate speed statistics only if we have speeds\n",
    "            if len(speeds) > 0:\n",
    "                features['mean_propagation_speed'] = float(np.mean(speeds))\n",
    "                features['median_propagation_speed'] = float(np.median(speeds))\n",
    "                features['std_propagation_speed'] = float(np.std(speeds))\n",
    "                features['max_propagation_speed'] = float(np.max(speeds))\n",
    "                features['min_propagation_speed'] = float(np.min(speeds))\n",
    "                features['num_propagation_events'] = len(speeds)\n",
    "            \n",
    "            # Calculate delay statistics\n",
    "            if len(delays) > 0:\n",
    "                features['mean_onset_delay'] = float(np.mean(delays))\n",
    "                features['max_onset_delay'] = float(np.max(delays))\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in propagation calculation: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b9ff2d",
   "metadata": {},
   "source": [
    "### Wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e090633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wavelet_features(data, sfreq, wavelet='db4', levels=5):\n",
    "    features = {}\n",
    "    n_channels = data.shape[0]\n",
    "    \n",
    "    # Store features for each decomposition level\n",
    "    all_channel_features = []\n",
    "    \n",
    "    for ch_idx in range(n_channels):\n",
    "        channel_data = data[ch_idx, :]\n",
    "        \n",
    "        try:\n",
    "            # Perform wavelet decomposition\n",
    "            coeffs = pywt.wavedec(channel_data, wavelet, level=levels)\n",
    "            \n",
    "            # Calculate features for each level\n",
    "            channel_features = []\n",
    "            for level, coeff in enumerate(coeffs):\n",
    "                if len(coeff) > 0:\n",
    "                    # Energy of coefficients\n",
    "                    energy = np.sum(coeff ** 2)\n",
    "                    # Entropy\n",
    "                    entropy = stats.entropy(np.abs(coeff) + 1e-10)\n",
    "                    # Statistical features\n",
    "                    mean_coeff = np.mean(np.abs(coeff))\n",
    "                    std_coeff = np.std(coeff)\n",
    "                    max_coeff = np.max(np.abs(coeff))\n",
    "                    \n",
    "                    channel_features.extend([energy, entropy, mean_coeff, std_coeff, max_coeff])\n",
    "                else:\n",
    "                    channel_features.extend([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "            \n",
    "            all_channel_features.append(channel_features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in wavelet transform for channel {ch_idx}: {e}\")\n",
    "            # Add zeros for failed channel\n",
    "            all_channel_features.append([0.0] * (5 * (levels + 1)))\n",
    "    \n",
    "    # Aggregate across channels\n",
    "    all_channel_features = np.array(all_channel_features)\n",
    "    \n",
    "    # Store aggregated features\n",
    "    feature_names = ['energy', 'entropy', 'mean', 'std', 'max']\n",
    "    for level in range(levels + 1):\n",
    "        for feat_idx, feat_name in enumerate(feature_names):\n",
    "            feat_values = all_channel_features[:, level * 5 + feat_idx]\n",
    "            features[f'wt_level{level}_{feat_name}_mean'] = float(np.mean(feat_values))\n",
    "            features[f'wt_level{level}_{feat_name}_std'] = float(np.std(feat_values))\n",
    "    \n",
    "    # Calculate wavelet packet energy distribution\n",
    "    try:\n",
    "        # Use first channel for packet decomposition (computationally expensive for all)\n",
    "        wp = pywt.WaveletPacket(data[0, :], wavelet, maxlevel=3)\n",
    "        packet_energies = []\n",
    "        for node in wp.get_level(3):\n",
    "            packet_energies.append(np.sum(node.data ** 2))\n",
    "        \n",
    "        if packet_energies:\n",
    "            total_energy = sum(packet_energies)\n",
    "            if total_energy > 0:\n",
    "                normalized_energies = np.array([e/total_energy for e in packet_energies])\n",
    "                features['wt_packet_entropy'] = float(stats.entropy(normalized_energies + 1e-10))\n",
    "            else:\n",
    "                features['wt_packet_entropy'] = 0.0\n",
    "        else:\n",
    "            features['wt_packet_entropy'] = 0.0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in wavelet packet decomposition: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc3c8c7",
   "metadata": {},
   "source": [
    "### Time Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc39684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_time_domain_features(data, sfreq):\n",
    "    features = {}\n",
    "    n_channels, n_samples = data.shape\n",
    "    \n",
    "    try:\n",
    "        # Statistical features for each channel\n",
    "        channel_features = {\n",
    "            'mean': np.mean(data, axis=1),\n",
    "            'std': np.std(data, axis=1),\n",
    "            'skewness': stats.skew(data, axis=1),\n",
    "            'kurtosis': stats.kurtosis(data, axis=1),\n",
    "            'rms': np.sqrt(np.mean(data ** 2, axis=1)),\n",
    "            'peak_to_peak': np.ptp(data, axis=1),\n",
    "            'zero_crossings': np.sum(np.diff(np.sign(data), axis=1) != 0, axis=1) / (n_samples / sfreq),  # Rate per second\n",
    "        }\n",
    "        \n",
    "        # Hjorth parameters\n",
    "        # Activity (variance of signal)\n",
    "        activity = np.var(data, axis=1)\n",
    "        \n",
    "        # Mobility (standard deviation of first derivative / standard deviation of signal)\n",
    "        first_deriv = np.diff(data, axis=1)\n",
    "        mobility = np.std(first_deriv, axis=1) / (np.std(data, axis=1) + 1e-10)\n",
    "        \n",
    "        # Complexity (mobility of first derivative / mobility of signal)\n",
    "        second_deriv = np.diff(first_deriv, axis=1)\n",
    "        mobility_deriv = np.std(second_deriv, axis=1) / (np.std(first_deriv, axis=1) + 1e-10)\n",
    "        complexity = mobility_deriv / (mobility + 1e-10)\n",
    "        \n",
    "        channel_features['hjorth_activity'] = activity\n",
    "        channel_features['hjorth_mobility'] = mobility\n",
    "        channel_features['hjorth_complexity'] = complexity\n",
    "        \n",
    "        # Line length (sum of absolute differences)\n",
    "        line_length = np.sum(np.abs(np.diff(data, axis=1)), axis=1) / n_samples\n",
    "        channel_features['line_length'] = line_length\n",
    "        \n",
    "        # Non-linear energy\n",
    "        nonlinear_energy = []\n",
    "        for ch in range(n_channels):\n",
    "            if n_samples >= 3:\n",
    "                nle = np.mean(data[ch, 1:-1]**2 - data[ch, :-2] * data[ch, 2:])\n",
    "                nonlinear_energy.append(nle)\n",
    "            else:\n",
    "                nonlinear_energy.append(0.0)\n",
    "        channel_features['nonlinear_energy'] = np.array(nonlinear_energy)\n",
    "        \n",
    "        # Aggregate features across channels\n",
    "        for feat_name, feat_values in channel_features.items():\n",
    "            features[f'time_{feat_name}_mean'] = float(np.mean(feat_values))\n",
    "            features[f'time_{feat_name}_std'] = float(np.std(feat_values))\n",
    "            features[f'time_{feat_name}_max'] = float(np.max(feat_values))\n",
    "            features[f'time_{feat_name}_min'] = float(np.min(feat_values))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in time-domain feature calculation: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f5dd3",
   "metadata": {},
   "source": [
    "### Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c8dca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_connectivity_features(raw, fmin=1, fmax=50):\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        data = raw.get_data()\n",
    "        n_channels, n_samples = data.shape\n",
    "        sfreq = raw.info['sfreq']\n",
    "        \n",
    "        # Only calculate if we have multiple channels\n",
    "        if n_channels < 2:\n",
    "            raise ValueError(\"Need at least 2 channels for connectivity\")\n",
    "        \n",
    "        # Reshape data for connectivity calculation (n_epochs, n_channels, n_times)\n",
    "        # Create pseudo-epochs by segmenting the data\n",
    "        epoch_length = int(2 * sfreq)  # 2-second epochs\n",
    "        n_epochs = n_samples // epoch_length\n",
    "        \n",
    "        if n_epochs > 0:\n",
    "            epochs_data = []\n",
    "            for i in range(n_epochs):\n",
    "                start = i * epoch_length\n",
    "                end = start + epoch_length\n",
    "                epochs_data.append(data[:, start:end])\n",
    "            epochs_data = np.array(epochs_data)\n",
    "            \n",
    "            # Calculate spectral connectivity\n",
    "            # Using coherence as the connectivity measure\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_data, method='coh', mode='multitaper',\n",
    "                sfreq=sfreq, fmin=fmin, fmax=fmax,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Get connectivity matrix (n_channels x n_channels x n_freqs)\n",
    "            con_matrix = con.get_data(output='dense')\n",
    "            \n",
    "            # Average across frequencies\n",
    "            mean_connectivity = np.mean(con_matrix, axis=2)\n",
    "            \n",
    "            # Extract upper triangle (excluding diagonal)\n",
    "            upper_tri = np.triu_indices(n_channels, k=1)\n",
    "            connectivity_values = mean_connectivity[upper_tri]\n",
    "            \n",
    "            # Store connectivity statistics\n",
    "            features['connectivity_mean'] = float(np.mean(connectivity_values))\n",
    "            features['connectivity_std'] = float(np.std(connectivity_values))\n",
    "            features['connectivity_max'] = float(np.max(connectivity_values))\n",
    "            features['connectivity_min'] = float(np.min(connectivity_values))\n",
    "            \n",
    "            # Global efficiency (mean of connectivity)\n",
    "            features['global_efficiency'] = float(np.mean(connectivity_values))\n",
    "            \n",
    "            # Node strength (sum of connections for each node)\n",
    "            node_strengths = np.sum(mean_connectivity, axis=0) - 1  # Subtract diagonal\n",
    "            features['node_strength_mean'] = float(np.mean(node_strengths))\n",
    "            features['node_strength_std'] = float(np.std(node_strengths))\n",
    "            features['node_strength_max'] = float(np.max(node_strengths))\n",
    "            \n",
    "            # Clustering coefficient (simplified version)\n",
    "            clustering_coeffs = []\n",
    "            for i in range(n_channels):\n",
    "                neighbors = mean_connectivity[i, :] > 0.3  # Threshold for connection\n",
    "                n_neighbors = np.sum(neighbors) - 1  # Exclude self\n",
    "                if n_neighbors > 1:\n",
    "                    # Count connections between neighbors\n",
    "                    neighbor_indices = np.where(neighbors)[0]\n",
    "                    neighbor_connections = 0\n",
    "                    for j in range(len(neighbor_indices)):\n",
    "                        for k in range(j+1, len(neighbor_indices)):\n",
    "                            if mean_connectivity[neighbor_indices[j], neighbor_indices[k]] > 0.3:\n",
    "                                neighbor_connections += 1\n",
    "                    max_connections = n_neighbors * (n_neighbors - 1) / 2\n",
    "                    if max_connections > 0:\n",
    "                        clustering = neighbor_connections / max_connections\n",
    "                        clustering_coeffs.append(clustering)\n",
    "            \n",
    "            features['clustering_coefficient'] = float(np.mean(clustering_coeffs))\n",
    "  \n",
    "        else:\n",
    "            raise ValueError(\"Not enough data for connectivity analysis\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in connectivity calculation: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5c200",
   "metadata": {},
   "source": [
    "### PAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0491ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pac_features(data, sfreq):\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        n_channels = data.shape[0]\n",
    "        \n",
    "        # Define phase and amplitude frequency bands\n",
    "        phase_bands = {\n",
    "            'theta': (4, 8),\n",
    "            'alpha': (8, 12)\n",
    "        }\n",
    "        \n",
    "        amplitude_bands = {\n",
    "            'gamma': (30, 50),\n",
    "            'high_gamma': (50, 100)\n",
    "        }\n",
    "        \n",
    "        pac_values = []\n",
    "        \n",
    "        for phase_name, (phase_low, phase_high) in phase_bands.items():\n",
    "            for amp_name, (amp_low, amp_high) in amplitude_bands.items():\n",
    "                \n",
    "                channel_pac = []\n",
    "                for ch in range(min(n_channels, 10)):  # Limit to first 10 channels for speed\n",
    "                    # Extract phase\n",
    "                    phase_filtered = mne.filter.filter_data(\n",
    "                        data[ch:ch+1, :], sfreq, \n",
    "                        l_freq=phase_low, h_freq=phase_high,\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    phase = np.angle(signal.hilbert(phase_filtered[0]))\n",
    "                    \n",
    "                    # Extract amplitude\n",
    "                    amp_filtered = mne.filter.filter_data(\n",
    "                        data[ch:ch+1, :], sfreq,\n",
    "                        l_freq=amp_low, h_freq=amp_high,\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    amplitude = np.abs(signal.hilbert(amp_filtered[0]))\n",
    "                    \n",
    "                    # Calculate PAC using Modulation Index\n",
    "                    n_bins = 18\n",
    "                    phase_bins = np.linspace(-np.pi, np.pi, n_bins + 1)\n",
    "                    amp_by_phase = []\n",
    "                    \n",
    "                    for i in range(n_bins):\n",
    "                        mask = (phase >= phase_bins[i]) & (phase < phase_bins[i+1])\n",
    "                        if np.sum(mask) > 0:\n",
    "                            amp_by_phase.append(np.mean(amplitude[mask]))\n",
    "                        else:\n",
    "                            amp_by_phase.append(0)\n",
    "                    \n",
    "                    # Normalize and calculate entropy\n",
    "                    amp_by_phase = np.array(amp_by_phase)\n",
    "                    if np.sum(amp_by_phase) > 0:\n",
    "                        amp_by_phase = amp_by_phase / np.sum(amp_by_phase)\n",
    "                        # Kullback-Leibler divergence from uniform distribution\n",
    "                        uniform = np.ones(n_bins) / n_bins\n",
    "                        kl_div = np.sum(amp_by_phase * np.log((amp_by_phase + 1e-10) / uniform))\n",
    "                        mi = kl_div / np.log(n_bins)  # Normalized MI\n",
    "                        channel_pac.append(mi)\n",
    "                    else:\n",
    "                        channel_pac.append(0.0)\n",
    "                \n",
    "                if channel_pac:\n",
    "                    pac_value = np.mean(channel_pac)\n",
    "                    features[f'pac_{phase_name}_{amp_name}'] = float(pac_value)\n",
    "                    pac_values.append(pac_value)\n",
    "                else:\n",
    "                    features[f'pac_{phase_name}_{amp_name}'] = 0.0\n",
    "        \n",
    "        # Overall PAC statistics\n",
    "        features['pac_mean'] = float(np.mean(pac_values))\n",
    "        features['pac_max'] = float(np.max(pac_values))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in PAC calculation: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c95e6",
   "metadata": {},
   "source": [
    "### Sample, Permutation, and Approximate Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ec13c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy_features(data, sfreq):\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        n_channels = data.shape[0]\n",
    "        \n",
    "        # Sample Entropy\n",
    "        sample_entropies = []\n",
    "        for ch in range(min(n_channels, 10)):  # Limit channels for speed\n",
    "            # Downsample for faster computation\n",
    "            downsampled = signal.resample(data[ch, :], min(1000, len(data[ch, :])))\n",
    "            # Simple sample entropy approximation\n",
    "            se = calculate_sample_entropy(downsampled, m=2, r=0.2*np.std(downsampled))\n",
    "            sample_entropies.append(se)\n",
    "        \n",
    "        features['sample_entropy_mean'] = float(np.mean(sample_entropies))\n",
    "        features['sample_entropy_std'] = float(np.std(sample_entropies))\n",
    "        \n",
    "        # Permutation Entropy\n",
    "        perm_entropies = []\n",
    "        for ch in range(min(n_channels, 10)):\n",
    "            pe = calculate_permutation_entropy(data[ch, :], order=3, delay=1)\n",
    "            perm_entropies.append(pe)\n",
    "        \n",
    "        features['permutation_entropy_mean'] = float(np.mean(perm_entropies))\n",
    "        features['permutation_entropy_std'] = float(np.std(perm_entropies))\n",
    "        \n",
    "        # Approximate Entropy (simplified)\n",
    "        approx_entropies = []\n",
    "        for ch in range(min(n_channels, 10)):\n",
    "            downsampled = signal.resample(data[ch, :], min(1000, len(data[ch, :])))\n",
    "            ae = calculate_approx_entropy(downsampled, m=2, r=0.2*np.std(downsampled))\n",
    "            approx_entropies.append(ae)\n",
    "        \n",
    "        features['approx_entropy_mean'] = float(np.mean(approx_entropies))\n",
    "        features['approx_entropy_std'] = float(np.std(approx_entropies))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in entropy calculation: {e}\")\n",
    "        features['sample_entropy_mean'] = 0.0\n",
    "        features['sample_entropy_std'] = 0.0\n",
    "        features['permutation_entropy_mean'] = 0.0\n",
    "        features['permutation_entropy_std'] = 0.0\n",
    "        features['approx_entropy_mean'] = 0.0\n",
    "        features['approx_entropy_std'] = 0.0\n",
    "    \n",
    "    return features\n",
    "\n",
    "def calculate_sample_entropy(signal_data, m=2, r=0.2):\n",
    "    \"\"\"Calculate sample entropy\"\"\"\n",
    "    N = len(signal_data)\n",
    "    \n",
    "    def _maxdist(x_i, x_j, m):\n",
    "        return max([abs(ua - va) for ua, va in zip(x_i, x_j)])\n",
    "    \n",
    "    def _phi(m):\n",
    "        patterns = np.array([signal_data[i:i+m] for i in range(N - m + 1)])\n",
    "        C = 0\n",
    "        for i in range(len(patterns)):\n",
    "            for j in range(i+1, len(patterns)):\n",
    "                if _maxdist(patterns[i], patterns[j], m) <= r:\n",
    "                    C += 1\n",
    "        return C\n",
    "    \n",
    "    try:\n",
    "        phi_m = _phi(m)\n",
    "        phi_m1 = _phi(m + 1)\n",
    "        \n",
    "        if phi_m == 0:\n",
    "            return 0\n",
    "        return -np.log(phi_m1 / phi_m) if phi_m1 > 0 else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def calculate_permutation_entropy(signal_data, order=3, delay=1):\n",
    "    \"\"\"Calculate permutation entropy\"\"\"\n",
    "    try:\n",
    "        n = len(signal_data)\n",
    "        permutations = []\n",
    "        \n",
    "        for i in range(n - delay * (order - 1)):\n",
    "            indices = [i + j * delay for j in range(order)]\n",
    "            sorted_indices = np.argsort(signal_data[indices])\n",
    "            permutations.append(tuple(sorted_indices))\n",
    "        \n",
    "        # Count occurrences\n",
    "        unique, counts = np.unique(permutations, axis=0, return_counts=True)\n",
    "        probs = counts / len(permutations)\n",
    "        \n",
    "        # Calculate entropy\n",
    "        return -np.sum(probs * np.log(probs + 1e-10))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def calculate_approx_entropy(signal_data, m=2, r=0.2):\n",
    "    \"\"\"Calculate approximate entropy\"\"\"\n",
    "    try:\n",
    "        N = len(signal_data)\n",
    "        \n",
    "        def _maxdist(x_i, x_j, m):\n",
    "            return max([abs(ua - va) for ua, va in zip(x_i, x_j)])\n",
    "        \n",
    "        def _phi(m):\n",
    "            patterns = np.array([signal_data[i:i+m] for i in range(N - m + 1)])\n",
    "            C = np.zeros(N - m + 1)\n",
    "            \n",
    "            for i in range(N - m + 1):\n",
    "                matches = 0\n",
    "                for j in range(N - m + 1):\n",
    "                    if _maxdist(patterns[i], patterns[j], m) <= r:\n",
    "                        matches += 1\n",
    "                C[i] = matches / (N - m + 1)\n",
    "            \n",
    "            return np.mean(np.log(C + 1e-10))\n",
    "        \n",
    "        return _phi(m) - _phi(m + 1)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f58a41",
   "metadata": {},
   "source": [
    "## Feature Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9049746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comprehensive_features(raw, seizure_start=None, seizure_end=None):\n",
    "    all_features = {}\n",
    "    \n",
    "    # Get data\n",
    "    data = raw.get_data()\n",
    "    sfreq = raw.info['sfreq']\n",
    "    \n",
    "    de_features = calculate_differential_entropy(data, sfreq)\n",
    "    all_features.update(de_features)\n",
    "    \n",
    "    psd_features = extract_psd_features(raw)\n",
    "    all_features.update(psd_features)\n",
    "    \n",
    "    wt_features = calculate_wavelet_features(data, sfreq)\n",
    "    all_features.update(wt_features)\n",
    "    \n",
    "    time_features = calculate_time_domain_features(data, sfreq)\n",
    "    all_features.update(time_features)\n",
    "    \n",
    "    connectivity_features = calculate_connectivity_features(raw)\n",
    "    all_features.update(connectivity_features)\n",
    "    \n",
    "    pac_features = calculate_pac_features(data, sfreq)\n",
    "    all_features.update(pac_features)\n",
    "    \n",
    "    entropy_features = calculate_entropy_features(data, sfreq)\n",
    "    all_features.update(entropy_features)\n",
    "    \n",
    "    # Add seizure-specific features if seizure times are provided\n",
    "    if seizure_start is not None and seizure_end is not None and seizure_start > 0 and seizure_end > 0:\n",
    "        # Extract pre-ictal, ictal, and post-ictal features\n",
    "        pre_ictal_start = max(0, seizure_start - 30)  # 30 seconds before\n",
    "        pre_ictal_end = seizure_start\n",
    "        \n",
    "        post_ictal_start = seizure_end\n",
    "        post_ictal_end = min(data.shape[1] / sfreq, seizure_end + 30)  # 30 seconds after\n",
    "        \n",
    "        # Extract features for different periods\n",
    "        try:\n",
    "            # Pre-ictal\n",
    "            pre_start_sample = int(pre_ictal_start * sfreq)\n",
    "            pre_end_sample = int(pre_ictal_end * sfreq)\n",
    "            if pre_end_sample > pre_start_sample:\n",
    "                pre_data = data[:, pre_start_sample:pre_end_sample]\n",
    "                pre_de = calculate_differential_entropy(pre_data, sfreq)\n",
    "                for key, value in pre_de.items():\n",
    "                    all_features[f'preictal_{key}'] = value\n",
    "            \n",
    "            # Ictal\n",
    "            ictal_start_sample = int(seizure_start * sfreq)\n",
    "            ictal_end_sample = int(seizure_end * sfreq)\n",
    "            if ictal_end_sample > ictal_start_sample:\n",
    "                ictal_data = data[:, ictal_start_sample:ictal_end_sample]\n",
    "                ictal_de = calculate_differential_entropy(ictal_data, sfreq)\n",
    "                for key, value in ictal_de.items():\n",
    "                    all_features[f'ictal_{key}'] = value\n",
    "            \n",
    "            # Post-ictal\n",
    "            post_start_sample = int(post_ictal_start * sfreq)\n",
    "            post_end_sample = int(post_ictal_end * sfreq)\n",
    "            if post_end_sample > post_start_sample:\n",
    "                post_data = data[:, post_start_sample:post_end_sample]\n",
    "                post_de = calculate_differential_entropy(post_data, sfreq)\n",
    "                for key, value in post_de.items():\n",
    "                    all_features[f'postictal_{key}'] = value\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting seizure period features: {e}\")\n",
    "    \n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0097497",
   "metadata": {},
   "source": [
    "## Process Single EDF File Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c6a972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_edf(edf_path, seizure_info):\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Load EDF file\n",
    "        raw = mne.io.read_raw_edf(str(edf_path), preload=True, verbose=False)\n",
    "        \n",
    "        # Basic file info\n",
    "        features['file_path'] = str(edf_path)\n",
    "        features['num_channels'] = len(raw.ch_names)\n",
    "        features['sampling_rate'] = raw.info['sfreq']\n",
    "        features['duration_seconds'] = raw.n_times / raw.info['sfreq']\n",
    "        \n",
    "        # Apply bandpass filter\n",
    "        raw.filter(0.5, 100, fir_design='firwin', verbose=False)\n",
    "        \n",
    "        # Parse seizure times\n",
    "        seizure_start = None\n",
    "        seizure_end = None\n",
    "        \n",
    "        if isinstance(seizure_info, (dict, pd.Series)):\n",
    "            if 'seizure_start_time' in seizure_info:\n",
    "                time_str = str(seizure_info.get('seizure_start_time')).strip()\n",
    "                if ':' in time_str or '.' in time_str:\n",
    "                    time_str = time_str.replace('.', ':')\n",
    "                    parts = time_str.split(':')\n",
    "                    if len(parts) == 3:\n",
    "                        hours, minutes, seconds = map(float, parts)\n",
    "                        seizure_start = hours * 3600 + minutes * 60 + seconds\n",
    "                    elif len(parts) == 2:\n",
    "                        minutes, seconds = map(float, parts)\n",
    "                        seizure_start = minutes * 60 + seconds\n",
    "                else:\n",
    "                    try:\n",
    "                        seizure_start = float(time_str)\n",
    "                    except:\n",
    "                        seizure_start = None\n",
    "            \n",
    "            if 'seizure_end_time' in seizure_info:\n",
    "                time_str = str(seizure_info.get('seizure_end_time')).strip()\n",
    "                if ':' in time_str or '.' in time_str:\n",
    "                    time_str = time_str.replace('.', ':')\n",
    "                    parts = time_str.split(':')\n",
    "                    if len(parts) == 3:\n",
    "                        hours, minutes, seconds = map(float, parts)\n",
    "                        seizure_end = hours * 3600 + minutes * 60 + seconds\n",
    "                    elif len(parts) == 2:\n",
    "                        minutes, seconds = map(float, parts)\n",
    "                        seizure_end = minutes * 60 + seconds\n",
    "                else:\n",
    "                    try:\n",
    "                        seizure_end = float(time_str)\n",
    "                    except:\n",
    "                        seizure_end = None\n",
    "        \n",
    "        # Extract comprehensive features\n",
    "        comprehensive_features = extract_comprehensive_features(raw, seizure_start, seizure_end)\n",
    "        features.update(comprehensive_features)\n",
    "        \n",
    "        # Propagation features\n",
    "        propagation_features = calculate_simple_propagation_features(raw, seizure_start, seizure_end)\n",
    "        features.update(propagation_features)\n",
    "        \n",
    "        # Add seizure timing info\n",
    "        features['seizure_start_seconds'] = seizure_start if seizure_start is not None else 0.0\n",
    "        features['seizure_end_seconds'] = seizure_end if seizure_end is not None else 0.0\n",
    "        if seizure_end and seizure_start:\n",
    "            features['seizure_duration'] = seizure_end - seizure_start\n",
    "        else:\n",
    "            features['seizure_duration'] = 0.0\n",
    "        \n",
    "        features['processing_success'] = True\n",
    "        features['error_message'] = ''\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {edf_path}: {e}\")\n",
    "        features['processing_success'] = False\n",
    "        features['error_message'] = str(e)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5645b995",
   "metadata": {},
   "source": [
    "## Build Comprehensive Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c14cc12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_comprehensive_dataset(seizures_df, patients_df, \n",
    "                              data_root_paths=['data/siena_scalp', 'data/chb-mit']):\n",
    "    all_records = []\n",
    "    \n",
    "    # Convert Polars to pandas for iteration (if needed)\n",
    "    if isinstance(seizures_df, pl.DataFrame):\n",
    "        seizures_pd = seizures_df.to_pandas()\n",
    "        patients_pd = patients_df.to_pandas()\n",
    "    else:\n",
    "        seizures_pd = seizures_df\n",
    "        patients_pd = patients_df\n",
    "    \n",
    "    print(f\"Processing {len(seizures_pd)} seizure records...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for idx, seizure_row in tqdm(seizures_pd.iterrows(), total=len(seizures_pd)):\n",
    "        record = {}\n",
    "        \n",
    "        # Add all seizure metadata\n",
    "        for col in seizure_row.index:\n",
    "            record[f'seizure_{col}'] = seizure_row[col]\n",
    "        \n",
    "        # Find corresponding patient info\n",
    "        patient_id = seizure_row['patient_id']\n",
    "        patient_info = patients_pd[patients_pd['patient_id'] == patient_id]\n",
    "        \n",
    "        if not patient_info.empty:\n",
    "            for col in patient_info.columns:\n",
    "                if col != 'patient_id':  # Avoid duplication\n",
    "                    record[f'patient_{col}'] = patient_info.iloc[0][col]\n",
    "        \n",
    "        # Find EDF file\n",
    "        file_name = seizure_row['file_name']\n",
    "        edf_path = None\n",
    "        \n",
    "        for root_path in data_root_paths:\n",
    "            possible_paths = [\n",
    "                os.path.join(root_path, patient_id, file_name),\n",
    "                os.path.join(root_path, patient_id.lower(), file_name),\n",
    "                os.path.join(root_path, patient_id.upper(), file_name),\n",
    "            ]\n",
    "            \n",
    "            for path in possible_paths:\n",
    "                if os.path.exists(path):\n",
    "                    edf_path = path\n",
    "                    break\n",
    "            \n",
    "            if edf_path:\n",
    "                break\n",
    "        \n",
    "        if edf_path:\n",
    "            # Process EDF and extract features - pass the entire row as dict\n",
    "            edf_features = process_single_edf(edf_path, seizure_row.to_dict())\n",
    "            record.update(edf_features)\n",
    "        else:\n",
    "            record['processing_success'] = False\n",
    "            record['error_message'] = 'EDF file not found'\n",
    "        \n",
    "        all_records.append(record)\n",
    "    \n",
    "    # Create comprehensive dataframe using Polars\n",
    "    comprehensive_df = pl.DataFrame(all_records)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total records: {len(comprehensive_df)}\")\n",
    "    \n",
    "    if 'processing_success' in comprehensive_df.columns:\n",
    "        success_count = comprehensive_df.filter(pl.col('processing_success')).height\n",
    "        print(f\"Successfully processed: {success_count}\")\n",
    "        print(f\"Failed: {len(comprehensive_df) - success_count}\")\n",
    "    \n",
    "    return comprehensive_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91625555",
   "metadata": {},
   "source": [
    "## Fill in Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9472fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values_polars(df):\n",
    "    # Get column types\n",
    "    str_cols = [col for col in df.columns if df[col].dtype == pl.Utf8]\n",
    "    num_cols = [col for col in df.columns if df[col].dtype in [pl.Float32, pl.Float64, pl.Int8, pl.Int16, pl.Int32, pl.Int64]]\n",
    "    \n",
    "    # Fill string columns with \"N/A\"\n",
    "    for col in str_cols:\n",
    "        df = df.with_columns(pl.col(col).fill_null(\"N/A\"))\n",
    "    \n",
    "    # Fill numeric columns with 0\n",
    "    for col in num_cols:\n",
    "        df = df.with_columns(\n",
    "            pl.col(col).fill_null(0).fill_nan(0)\n",
    "        )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd397b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(df, output_path='comprehensive_df.csv'):\n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        df_pd = df.to_pandas()\n",
    "    else:\n",
    "        df_pd = df\n",
    "    \n",
    "    df_pd.to_csv(output_path, index=False)\n",
    "    print(f\"Dataset saved to: {output_path}\")\n",
    "    print(f\"File size: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "def save_to_parquet(df, output_path='comprehensive_eeg_features.parquet'):\n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        df.write_parquet(output_path)\n",
    "    else:\n",
    "        df.to_parquet(output_path, index=False, engine='pyarrow')\n",
    "    \n",
    "    print(f\"Dataset saved to: {output_path}\")\n",
    "    print(f\"File size: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21eb5731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from JSON files...\n",
      "Found 1 JSON files\n",
      "Loaded 1 patients and 3 seizures\n"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "print(\"Loading metadata from JSON files...\")\n",
    "seizures_df, patients_df = load_metadata_from_json('data/processing_test_files')\n",
    "print(f\"Loaded {len(patients_df)} patients and {len(seizures_df)} seizures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a783dd6",
   "metadata": {},
   "source": [
    "## Main Usage Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f7ba49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3 seizure records...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [36:38<00:00, 732.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETE\n",
      "============================================================\n",
      "Total records: 3\n",
      "Successfully processed: 3\n",
      "Failed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build comprehensive dataset\n",
    "comprehensive_df = build_comprehensive_dataset(\n",
    "    seizures_df, \n",
    "    patients_df,\n",
    "    data_root_paths=[\n",
    "        #'data/siena_scalp', \n",
    "        #'data/chb-mit',\n",
    "        'data/processing_test_files'\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "#print(\"\\nFilling missing values...\")\n",
    "#comprehensive_df = fill_missing_values_polars(comprehensive_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc4a3ec",
   "metadata": {},
   "source": [
    "## Export Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Parquet\n",
    "#print(\"\\nSaving results...\")\n",
    "#save_to_parquet(comprehensive_df, 'processed_data/comprehensive_eeg_features.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28605b3d",
   "metadata": {},
   "source": [
    "## Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab25b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data set\n",
    "processed_df = pl.read_parquet(\"processed_data/comprehensive_eeg_features.parquet\")\n",
    "\n",
    "print(f\"Shape: {processed_df.shape}\")\n",
    "print(f\"Columns: {processed_df.columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0904f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
