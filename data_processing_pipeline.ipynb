{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8778948c",
   "metadata": {},
   "source": [
    "# Epilepsy Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c23b17",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MNE Libraries\n",
    "import mne\n",
    "from mne import Epochs, pick_types, events_from_annotations\n",
    "from mne.channels import make_standard_montage\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.datasets import eegbci\n",
    "from mne.decoding import CSP\n",
    "from mne.preprocessing import ICA, create_eog_epochs, create_ecg_epochs\n",
    "\n",
    "# Scipy libraries\n",
    "from scipy import signal\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab82f7",
   "metadata": {},
   "source": [
    "## Data Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220995c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata_from_json(data_dir='data'):\n",
    "    \"\"\"Load patient and seizure metadata from JSON files\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    all_seizures = []\n",
    "    all_patients = []\n",
    "    \n",
    "    json_files = sorted(list(data_dir.glob('**/*.json')))\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract patient-level information\n",
    "            patient_info = {\n",
    "                'patient_id': data['patient_id'],\n",
    "                'sampling_rate_hz': data['sampling_rate_hz'],\n",
    "                'num_channels': len(data['channels']),\n",
    "                'json_file_path': str(json_file)\n",
    "            }\n",
    "            \n",
    "            if 'file_name' in data:\n",
    "                patient_info['file_name'] = data['file_name']\n",
    "                patient_info['registration_start_time'] = data.get('registration_start_time')\n",
    "                patient_info['registration_end_time'] = data.get('registration_end_time')\n",
    "            \n",
    "            all_patients.append(patient_info)\n",
    "            \n",
    "            # Process each seizure\n",
    "            for seizure in data['seizures']:\n",
    "                seizure_record = {\n",
    "                    'patient_id': data['patient_id'],\n",
    "                    'sampling_rate_hz': data['sampling_rate_hz'],\n",
    "                    'seizure_number': seizure['seizure_number']\n",
    "                }\n",
    "                \n",
    "                for key, value in seizure.items():\n",
    "                    seizure_record[key] = value\n",
    "                \n",
    "                if 'file_name' in patient_info and 'file_name' not in seizure:\n",
    "                    seizure_record['file_name'] = patient_info['file_name']\n",
    "                    if 'registration_start_time' in patient_info:\n",
    "                        seizure_record['registration_start_time'] = patient_info['registration_start_time']\n",
    "                        seizure_record['registration_end_time'] = patient_info['registration_end_time']\n",
    "                \n",
    "                all_seizures.append(seizure_record)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    seizures_df = pd.DataFrame(all_seizures)\n",
    "    patients_df = pd.DataFrame(all_patients)\n",
    "    \n",
    "    return seizures_df, patients_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1050b5",
   "metadata": {},
   "source": [
    "## Data Processing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5765adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(df):\n",
    "    # Fill categorical columns with 'N/A'\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns\n",
    "    df[cat_cols] = df[cat_cols].fillna(\"N/A\")\n",
    "    \n",
    "    # Fill all numerical columns with 0 (int8, int16, int32, int64, uint8, uint16, uint32, uint64, float16, float32, float64, complex64, complex128)\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns  \n",
    "    df[num_cols] = df[num_cols].replace([np.inf, -np.inf, '', None], np.nan).fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd83f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to convert complex numerical types to simpler ones\n",
    "def simplify_dtypes(df):\n",
    "    df_simplified = df.copy()\n",
    "    \n",
    "    # Dictionary mapping complex types to simpler ones\n",
    "    dtype_mapping = {\n",
    "        'float64': 'float32',\n",
    "        'int64': 'int32',\n",
    "        'int32': 'int16', \n",
    "        'float32': 'float32', \n",
    "    }\n",
    "    \n",
    "    for column in df_simplified.columns:\n",
    "        current_dtype = str(df_simplified[column].dtype)\n",
    "        \n",
    "        # Convert float64 to float32\n",
    "        if current_dtype == 'float64':\n",
    "            df_simplified[column] = df_simplified[column].astype('float32')\n",
    "            print(f\"Converted {column}: {current_dtype} -> float32\")\n",
    "        \n",
    "        # Convert int64 to int32 (check range first)\n",
    "        elif current_dtype == 'int64':\n",
    "            col_min = df_simplified[column].min()\n",
    "            col_max = df_simplified[column].max()\n",
    "            \n",
    "            # Check if values fit in int32 range\n",
    "            if col_min >= -2147483648 and col_max <= 2147483647:\n",
    "                df_simplified[column] = df_simplified[column].astype('int32')\n",
    "                print(f\"Converted {column}: {current_dtype} -> int32\")\n",
    "            else:\n",
    "                print(f\"Kept {column} as {current_dtype} (values too large for int32)\")\n",
    "        \n",
    "        # Convert int32 to int16 if values are small enough\n",
    "        elif current_dtype == 'int32':\n",
    "            col_min = df_simplified[column].min()\n",
    "            col_max = df_simplified[column].max()\n",
    "            \n",
    "            # Check if values fit in int16 range\n",
    "            if col_min >= -32768 and col_max <= 32767:\n",
    "                df_simplified[column] = df_simplified[column].astype('int16')\n",
    "                print(f\"Converted {column}: {current_dtype} -> int16\")\n",
    "    \n",
    "    return df_simplified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a485653",
   "metadata": {},
   "source": [
    "## PSD Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda26c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_psd_features(raw, fmin=0.5, fmax=50):\n",
    "    \"\"\"\n",
    "    Extract PSD features from raw EEG data\n",
    "    \n",
    "    Returns a dictionary with power features for different frequency bands\n",
    "    \"\"\"\n",
    "    # Compute PSD using multitaper method\n",
    "    psd = raw.compute_psd(method='multitaper', fmin=fmin, fmax=fmax, verbose=False)\n",
    "    psds, freqs = psd.get_data(return_freqs=True)\n",
    "    \n",
    "    # Define frequency bands\n",
    "    bands = {\n",
    "        'delta': (0.5, 4),\n",
    "        'theta': (4, 8),\n",
    "        'alpha': (8, 12),\n",
    "        'beta': (12, 30),\n",
    "        'gamma': (30, 50)\n",
    "    }\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Calculate band powers for each channel\n",
    "    for band_name, (low_freq, high_freq) in bands.items():\n",
    "        # Find frequency indices\n",
    "        freq_mask = (freqs >= low_freq) & (freqs < high_freq)\n",
    "        \n",
    "        # Calculate mean power in band for each channel\n",
    "        band_power = np.mean(psds[:, freq_mask], axis=1)\n",
    "        \n",
    "        # Store statistics across channels\n",
    "        features[f'{band_name}_power_mean'] = np.mean(band_power)\n",
    "        features[f'{band_name}_power_std'] = np.std(band_power)\n",
    "        features[f'{band_name}_power_median'] = np.median(band_power)\n",
    "        features[f'{band_name}_power_max'] = np.max(band_power)\n",
    "        features[f'{band_name}_power_min'] = np.min(band_power)\n",
    "    \n",
    "    # Calculate total power\n",
    "    total_power = np.mean(psds, axis=1)\n",
    "    features['total_power_mean'] = np.mean(total_power)\n",
    "    features['total_power_std'] = np.std(total_power)\n",
    "    \n",
    "    # Calculate relative band powers\n",
    "    for band_name in bands.keys():\n",
    "        features[f'{band_name}_relative_power'] = features[f'{band_name}_power_mean'] / features['total_power_mean']\n",
    "    \n",
    "    # Calculate peak frequency\n",
    "    mean_psd = np.mean(psds, axis=0)\n",
    "    peak_idx = np.argmax(mean_psd)\n",
    "    features['peak_frequency'] = freqs[peak_idx]\n",
    "    features['peak_power'] = mean_psd[peak_idx]\n",
    "    \n",
    "    # Calculate spectral entropy\n",
    "    psd_norm = psds / psds.sum(axis=1, keepdims=True)\n",
    "    spectral_entropy = -np.sum(psd_norm * np.log(psd_norm + 1e-15), axis=1)\n",
    "    features['spectral_entropy_mean'] = np.mean(spectral_entropy)\n",
    "    features['spectral_entropy_std'] = np.std(spectral_entropy)\n",
    "    \n",
    "    # Calculate spectral edge frequency (95% of power)\n",
    "    cumsum_psd = np.cumsum(mean_psd)\n",
    "    cumsum_psd = cumsum_psd / cumsum_psd[-1]\n",
    "    edge_idx = np.where(cumsum_psd >= 0.95)[0][0]\n",
    "    features['spectral_edge_95'] = freqs[edge_idx]\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_psd_features_by_region(raw, fmin=0.5, fmax=50):\n",
    "    \"\"\"\n",
    "    Extract PSD features by brain region\n",
    "    \"\"\"\n",
    "    # Define channel groups\n",
    "    channel_groups = {\n",
    "        'frontal': ['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'Fz'],\n",
    "        'central': ['C3', 'C4', 'Cz'],\n",
    "        'parietal': ['P3', 'P4', 'Pz'],\n",
    "        'occipital': ['O1', 'O2'],\n",
    "        'temporal': ['T3', 'T4', 'T5', 'T6']\n",
    "    }\n",
    "    \n",
    "    regional_features = {}\n",
    "    available_channels = raw.ch_names\n",
    "    \n",
    "    for region, channel_list in channel_groups.items():\n",
    "        # Find matching channels (handle different naming conventions)\n",
    "        region_channels = []\n",
    "        for ch in available_channels:\n",
    "            ch_clean = ch.upper().replace('EEG', '').replace('-', '').strip()\n",
    "            for target_ch in channel_list:\n",
    "                if target_ch.upper() in ch_clean:\n",
    "                    region_channels.append(ch)\n",
    "                    break\n",
    "        \n",
    "        if not region_channels:\n",
    "            continue\n",
    "        \n",
    "        # Pick channels for this region\n",
    "        try:\n",
    "            raw_region = raw.copy().pick(region_channels)\n",
    "            \n",
    "            # Extract features for this region\n",
    "            region_psd_features = extract_psd_features(raw_region, fmin, fmax)\n",
    "            \n",
    "            # Add region prefix to feature names\n",
    "            for feature_name, value in region_psd_features.items():\n",
    "                regional_features[f'{region}_{feature_name}'] = value\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Could not process {region} region: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return regional_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c97fd",
   "metadata": {},
   "source": [
    "## Propagation Speed Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8efc20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_channel_onsets(data, sfreq, threshold=2.0, window_size=1.0, \n",
    "                         seizure_start=None, seizure_end=None):\n",
    "    \"\"\"Detect seizure onset times for each channel\"\"\"\n",
    "    n_channels, n_samples = data.shape\n",
    "    window_samples = int(window_size * sfreq)\n",
    "    onset_times = {}\n",
    "    \n",
    "    for ch_idx in range(n_channels):\n",
    "        channel_data = data[ch_idx, :]\n",
    "        \n",
    "        # Calculate envelope using Hilbert transform\n",
    "        analytic_signal = signal.hilbert(channel_data)\n",
    "        envelope = np.abs(analytic_signal)\n",
    "        \n",
    "        # Smooth envelope\n",
    "        if window_samples > 3:\n",
    "            envelope_smooth = signal.savgol_filter(envelope, window_samples, 3)\n",
    "        else:\n",
    "            envelope_smooth = envelope\n",
    "        \n",
    "        # Calculate baseline and threshold\n",
    "        if seizure_start is not None and seizure_start > 10:\n",
    "            baseline_end = int((seizure_start - 1) * sfreq)\n",
    "            baseline = envelope_smooth[:baseline_end]\n",
    "        else:\n",
    "            baseline = envelope_smooth[:int(10 * sfreq)]\n",
    "        \n",
    "        if len(baseline) == 0:\n",
    "            continue\n",
    "            \n",
    "        baseline_mean = np.mean(baseline)\n",
    "        baseline_std = np.std(baseline)\n",
    "        onset_threshold = baseline_mean + threshold * baseline_std\n",
    "        \n",
    "        # Find onset\n",
    "        if seizure_start is not None:\n",
    "            search_start = max(0, int((seizure_start - 5) * sfreq))\n",
    "            search_end = min(n_samples, int((seizure_start + 10) * sfreq))\n",
    "            search_region = envelope_smooth[search_start:search_end]\n",
    "            \n",
    "            crossings = np.where(search_region > onset_threshold)[0]\n",
    "            if len(crossings) > 0:\n",
    "                onset_sample = search_start + crossings[0]\n",
    "                onset_times[ch_idx] = onset_sample / sfreq\n",
    "        else:\n",
    "            crossings = np.where(envelope_smooth > onset_threshold)[0]\n",
    "            if len(crossings) > 0:\n",
    "                for crossing in crossings:\n",
    "                    if crossing + window_samples < n_samples:\n",
    "                        if np.mean(envelope_smooth[crossing:crossing+window_samples]) > onset_threshold:\n",
    "                            onset_times[ch_idx] = crossing / sfreq\n",
    "                            break\n",
    "    \n",
    "    return onset_times\n",
    "\n",
    "def estimate_electrode_positions(ch_names):\n",
    "    \"\"\"Estimate electrode positions based on 10-20 system\"\"\"\n",
    "    standard_positions = {\n",
    "        'FP1': (-0.3, 0.9), 'FP2': (0.3, 0.9),\n",
    "        'F3': (-0.5, 0.6), 'F4': (0.5, 0.6),\n",
    "        'F7': (-0.8, 0.5), 'F8': (0.8, 0.5),\n",
    "        'C3': (-0.5, 0), 'C4': (0.5, 0),\n",
    "        'T3': (-0.9, 0), 'T4': (0.9, 0),\n",
    "        'T5': (-0.8, -0.5), 'T6': (0.8, -0.5),\n",
    "        'P3': (-0.5, -0.6), 'P4': (0.5, -0.6),\n",
    "        'O1': (-0.3, -0.9), 'O2': (0.3, -0.9),\n",
    "        'FZ': (0, 0.7), 'CZ': (0, 0), 'PZ': (0, -0.7)\n",
    "    }\n",
    "    \n",
    "    positions = {}\n",
    "    for idx, ch_name in enumerate(ch_names):\n",
    "        ch_clean = ch_name.upper().replace('-', '').replace('EEG', '').strip()\n",
    "        \n",
    "        for std_name, pos in standard_positions.items():\n",
    "            if std_name in ch_clean:\n",
    "                positions[idx] = pos\n",
    "                break\n",
    "        \n",
    "        if idx not in positions:\n",
    "            angle = 2 * np.pi * idx / len(ch_names)\n",
    "            positions[idx] = (np.cos(angle), np.sin(angle))\n",
    "    \n",
    "    return positions\n",
    "\n",
    "def calculate_propagation_speeds(onset_times, ch_names):\n",
    "    \"\"\"Calculate propagation speeds between channels\"\"\"\n",
    "    if len(onset_times) < 2:\n",
    "        return {}\n",
    "    \n",
    "    speeds = []\n",
    "    delays = []\n",
    "    \n",
    "    sorted_onsets = sorted(onset_times.items(), key=lambda x: x[1])\n",
    "    positions = estimate_electrode_positions(ch_names)\n",
    "    \n",
    "    for i in range(len(sorted_onsets) - 1):\n",
    "        ch1_idx, time1 = sorted_onsets[i]\n",
    "        ch2_idx, time2 = sorted_onsets[i + 1]\n",
    "        \n",
    "        delay = time2 - time1\n",
    "        delays.append(delay)\n",
    "        \n",
    "        if ch1_idx in positions and ch2_idx in positions:\n",
    "            pos1 = positions[ch1_idx]\n",
    "            pos2 = positions[ch2_idx]\n",
    "            distance = euclidean(pos1, pos2) * 100  # Convert to mm\n",
    "            \n",
    "            if delay > 0:\n",
    "                speed = distance / delay\n",
    "                speeds.append(speed)\n",
    "    \n",
    "    results = {}\n",
    "    if speeds:\n",
    "        results['mean_propagation_speed'] = np.mean(speeds)\n",
    "        results['median_propagation_speed'] = np.median(speeds)\n",
    "        results['std_propagation_speed'] = np.std(speeds)\n",
    "        results['max_propagation_speed'] = np.max(speeds)\n",
    "        results['min_propagation_speed'] = np.min(speeds)\n",
    "        results['num_propagation_events'] = len(speeds)\n",
    "    \n",
    "    if delays:\n",
    "        results['mean_onset_delay'] = np.mean(delays)\n",
    "        results['max_onset_delay'] = np.max(delays)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c0cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spike and sharpwave detection\n",
    "def detect_spike_sharpwave_events(data, sfreq, threshold=2.0, min_duration_ms=20, max_duration_ms=250):\n",
    "    \"\"\"\n",
    "    Detect spike and sharp wave events in EEG data based on duration and morphology.\n",
    "    Returns a dictionary with counts of spikes and sharp waves per channel.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: 2D numpy array of shape (n_channels, n_samples)\n",
    "    - sfreq: Sampling frequency in Hz\n",
    "    - threshold: Number of standard deviations above baseline to consider a peak\n",
    "    - min_duration_ms: Minimum duration of a detectable event in milliseconds\n",
    "    - max_duration_ms: Maximum duration of a detectable event in milliseconds\n",
    "    \"\"\"\n",
    "    n_channels, n_samples = data.shape\n",
    "    results = {}\n",
    "\n",
    "    for ch_idx in range(n_channels):\n",
    "        channel_data = data[ch_idx, :]\n",
    "\n",
    "        # Calculate envelope using Hilbert transform\n",
    "        analytic_signal = signal.hilbert(channel_data)\n",
    "        envelope = np.abs(analytic_signal)\n",
    "\n",
    "        # Smooth envelope\n",
    "        window_samples = int(0.05 * sfreq)  # 50 ms smoothing window\n",
    "        if window_samples > 3:\n",
    "            envelope_smooth = signal.savgol_filter(envelope, window_samples, 3)\n",
    "        else:\n",
    "            envelope_smooth = envelope\n",
    "\n",
    "        # Calculate baseline and threshold\n",
    "        baseline = envelope_smooth[:int(10 * sfreq)]\n",
    "        baseline_mean = np.mean(baseline)\n",
    "        baseline_std = np.std(baseline)\n",
    "        onset_threshold = baseline_mean + threshold * baseline_std\n",
    "\n",
    "        # Detect peaks above threshold\n",
    "        peaks, _ = signal.find_peaks(envelope_smooth, height=onset_threshold, distance=int(0.02 * sfreq))\n",
    "\n",
    "        spike_count = 0\n",
    "        sharpwave_count = 0\n",
    "        for peak in peaks:\n",
    "            # Estimate duration by looking at width at baseline level\n",
    "            left_base = peak\n",
    "            while left_base > 0 and envelope_smooth[left_base] > baseline_mean:\n",
    "                left_base -= 1\n",
    "\n",
    "            right_base = peak\n",
    "            while right_base < n_samples and envelope_smooth[right_base] > baseline_mean:\n",
    "                right_base += 1\n",
    "\n",
    "            duration_samples = right_base - left_base\n",
    "            duration_ms = (duration_samples / sfreq) * 1000\n",
    "\n",
    "            if duration_ms < 70:\n",
    "                spike_count += 1\n",
    "            elif 70 <= duration_ms <= 200:\n",
    "                sharpwave_count += 1\n",
    "\n",
    "        results[ch_idx] = {\n",
    "            'num_spikes': spike_count,\n",
    "            'num_sharpwaves': sharpwave_count\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e91dec",
   "metadata": {},
   "source": [
    "## Comprehensive Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde4c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_seizure_time(time_str):\n",
    "    \"\"\"Parse seizure time string to seconds\"\"\"\n",
    "    if pd.isna(time_str) or time_str == '':\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if ':' in str(time_str):\n",
    "            parts = str(time_str).split(':')\n",
    "            if len(parts) == 3:\n",
    "                hours, minutes, seconds = map(float, parts)\n",
    "                return hours * 3600 + minutes * 60 + seconds\n",
    "            elif len(parts) == 2:\n",
    "                minutes, seconds = map(float, parts)\n",
    "                return minutes * 60 + seconds\n",
    "        else:\n",
    "            return float(time_str)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def process_single_edf(edf_path, seizure_info):\n",
    "    \"\"\"\n",
    "    Process a single EDF file and extract all features\n",
    "    \n",
    "    Returns a dictionary with all extracted features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Load EDF file\n",
    "        raw = mne.io.read_raw_edf(str(edf_path), preload=True, verbose=False)\n",
    "        \n",
    "        # Basic file info\n",
    "        features['file_path'] = str(edf_path)\n",
    "        features['num_channels'] = len(raw.ch_names)\n",
    "        features['sampling_rate'] = raw.info['sfreq']\n",
    "        features['duration_seconds'] = raw.n_times / raw.info['sfreq']\n",
    "        \n",
    "        # Apply bandpass filter\n",
    "        raw.filter(0.5, 50, fir_design='firwin', verbose=False)\n",
    "        \n",
    "        # Extract global PSD features\n",
    "        psd_features = extract_psd_features(raw)\n",
    "        features.update(psd_features)\n",
    "        \n",
    "        # Extract regional PSD features\n",
    "        regional_features = extract_psd_features_by_region(raw)\n",
    "        features.update(regional_features)\n",
    "        \n",
    "        # If seizure info is provided, calculate propagation speeds\n",
    "        if seizure_info.any():\n",
    "            seizure_start = parse_seizure_time(seizure_info.get('seizure_start_time'))\n",
    "            seizure_end = parse_seizure_time(seizure_info.get('seizure_end_time'))\n",
    "            \n",
    "            if seizure_start is not None:\n",
    "                # Extract seizure segment\n",
    "                data = raw.get_data()\n",
    "                \n",
    "                # Apply additional filtering for seizure detection\n",
    "                data_filtered = mne.filter.filter_data(\n",
    "                    data, raw.info['sfreq'], l_freq=3, h_freq=30, verbose=False\n",
    "                )\n",
    "                \n",
    "                # Detect onsets\n",
    "                onset_times = detect_channel_onsets(\n",
    "                    data_filtered, \n",
    "                    raw.info['sfreq'],\n",
    "                    threshold=2.0,\n",
    "                    window_size=1.0,\n",
    "                    seizure_start=seizure_start,\n",
    "                    seizure_end=seizure_end\n",
    "                )\n",
    "                \n",
    "                # Calculate propagation speeds\n",
    "                propagation_features = calculate_propagation_speeds(\n",
    "                    onset_times, \n",
    "                    raw.ch_names\n",
    "                )\n",
    "                features.update(propagation_features)\n",
    "                \n",
    "                # Add seizure timing info\n",
    "                features['seizure_start_seconds'] = seizure_start\n",
    "                features['seizure_end_seconds'] = seizure_end\n",
    "                if seizure_end and seizure_start:\n",
    "                    features['seizure_duration'] = seizure_end - seizure_start\n",
    "        \n",
    "        features['processing_success'] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {edf_path}: {e}\")\n",
    "        features['processing_success'] = False\n",
    "        features['error_message'] = str(e)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573aa173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_comprehensive_dataset(seizures_df, patients_df, \n",
    "                              data_root_paths=['data/seina_scalp', 'data/chb-mit']):\n",
    "    \"\"\"\n",
    "    Build comprehensive dataset with all features\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    \n",
    "    print(f\"Processing {len(seizures_df)} seizure records...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for idx, seizure_row in tqdm(seizures_df.iterrows(), total=len(seizures_df)):\n",
    "        record = {}\n",
    "        \n",
    "        # Add all seizure metadata\n",
    "        for col in seizure_row.index:\n",
    "            record[f'seizure_{col}'] = seizure_row[col]\n",
    "        \n",
    "        # Find corresponding patient info\n",
    "        patient_id = seizure_row['patient_id']\n",
    "        patient_info = patients_df[patients_df['patient_id'] == patient_id]\n",
    "        \n",
    "        if not patient_info.empty:\n",
    "            for col in patient_info.columns:\n",
    "                if col != 'patient_id':  # Avoid duplication\n",
    "                    record[f'patient_{col}'] = patient_info.iloc[0][col]\n",
    "        \n",
    "        # Find EDF file\n",
    "        file_name = seizure_row['file_name']\n",
    "        edf_path = None\n",
    "        \n",
    "        for root_path in data_root_paths:\n",
    "            possible_paths = [\n",
    "                os.path.join(root_path, patient_id, file_name),\n",
    "                os.path.join(root_path, patient_id.lower(), file_name),\n",
    "                os.path.join(root_path, patient_id.upper(), file_name),\n",
    "            ]\n",
    "            \n",
    "            for path in possible_paths:\n",
    "                if os.path.exists(path):\n",
    "                    edf_path = path\n",
    "                    break\n",
    "            \n",
    "            if edf_path:\n",
    "                break\n",
    "        \n",
    "        if edf_path:\n",
    "            # Process EDF and extract features\n",
    "            edf_features = process_single_edf(edf_path, seizure_row)\n",
    "            record.update(edf_features)\n",
    "        else:\n",
    "            record['processing_success'] = False\n",
    "            record['error_message'] = 'EDF file not found'\n",
    "        \n",
    "        all_records.append(record)\n",
    "    \n",
    "    # Create comprehensive dataframe\n",
    "    comprehensive_df = pd.DataFrame(all_records)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total records: {len(comprehensive_df)}\")\n",
    "    print(f\"Successfully processed: {comprehensive_df['processing_success'].sum()}\")\n",
    "    print(f\"Failed: {(~comprehensive_df['processing_success']).sum()}\")\n",
    "    \n",
    "    return comprehensive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87501ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_parquet(df, output_path='comprehensive_eeg_features.parquet'):\n",
    "    \"\"\"\n",
    "    Save dataframe to parquet format\n",
    "    \"\"\"\n",
    "    # Convert any object columns that should be numeric\n",
    "    numeric_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in numeric_columns:\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Save to parquet\n",
    "    df.to_parquet(output_path, index=False, compression='snappy')\n",
    "    print(f\"Dataset saved to: {output_path}\")\n",
    "    print(f\"File size: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Create and save summary statistics\n",
    "    summary_stats = df.describe()\n",
    "    summary_stats.to_csv(output_path.replace('.parquet', '_summary.csv'))\n",
    "    print(f\"Summary statistics saved to: {output_path.replace('.parquet', '_summary.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf61a2b6",
   "metadata": {},
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf590106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "print(\"Loading metadata from JSON files...\")\n",
    "seizures_df, patients_df = load_metadata_from_json('data')\n",
    "print(f\"Loaded {len(patients_df)} patients and {len(seizures_df)} seizures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6297210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comprehensive dataset\n",
    "comprehensive_df = build_comprehensive_dataset(\n",
    "    seizures_df, \n",
    "    patients_df,\n",
    "    data_root_paths=['data/seina_scalp', 'data/chb-mit']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d6187",
   "metadata": {},
   "source": [
    "## Fill in NaN and Simplify Dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b101a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehensive_df = fill_missing_values(comprehensive_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f670a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehensive_df = simplify_dtypes(comprehensive_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddd2593",
   "metadata": {},
   "source": [
    "## Export to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be4329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to parquet\n",
    "save_to_parquet(comprehensive_df, 'comprehensive_eeg_features.parquet')\n",
    "\n",
    "# Display sample of features\n",
    "print(\"\\nSample of extracted features:\")\n",
    "print(comprehensive_df.columns.tolist()[:20])\n",
    "\n",
    "print(\"\\nDataset shape:\", comprehensive_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
