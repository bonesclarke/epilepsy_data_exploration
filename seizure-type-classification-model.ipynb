{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-09-15T17:04:56.929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    balanced_accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-15T17:04:56.929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"model_output\")\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-15T17:04:56.929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "USE_OPTUNA = False  # Toggle for hyperparameter tuning\n",
    "N_TRIALS = 10  # Number of Optuna trials\n",
    "RANDOM_STATE = 42\n",
    "TARGET_COLUMN = 'seizure_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-15T17:04:56.929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Feature list\n",
    "main_features = [\n",
    "            'seizure_de_delta_std',\n",
    "            'seizure_de_delta_max',\n",
    "            'seizure_de_delta_asymmetry_std',\n",
    "            'seizure_de_theta_std',\n",
    "            #'seizure_de_theta_max',\n",
    "            'seizure_de_theta_asymmetry_std',\n",
    "            'seizure_de_alpha_std',\n",
    "            'seizure_de_alpha_max',\n",
    "            'seizure_de_alpha_asymmetry_std',\n",
    "            'seizure_de_low_beta_std',\n",
    "            'seizure_de_low_beta_max',\n",
    "            'seizure_de_low_beta_asymmetry_std',\n",
    "            'seizure_de_high_beta_std',\n",
    "            'seizure_de_high_beta_max',\n",
    "            'seizure_de_high_beta_asymmetry_std',\n",
    "            'seizure_de_gamma_std',\n",
    "            'seizure_de_gamma_max',\n",
    "            'seizure_de_gamma_asymmetry_std',\n",
    "            'seizure_de_high_gamma_std',\n",
    "            'seizure_de_high_gamma_max',\n",
    "            'seizure_de_high_gamma_asymmetry_std',\n",
    "            #'seizure_psd_sef50',\n",
    "            #'seizure_psd_sef75', # high for all\n",
    "            #'seizure_psd_sef90', # high for all\n",
    "            #'seizure_psd_sef95',\n",
    "            'frontal_seizure_psd_delta_cv',\n",
    "            #'frontal_seizure_psd_theta_cv',\n",
    "            'frontal_seizure_psd_alpha_cv',\n",
    "            'frontal_seizure_psd_low_beta_cv',\n",
    "            'frontal_seizure_psd_high_beta_cv',\n",
    "            #'frontal_seizure_psd_gamma_mean', # higher for seizure type\n",
    "            #'frontal_seizure_psd_gamma_std',\n",
    "            #'frontal_seizure_psd_gamma_cv',\n",
    "            #'frontal_seizure_psd_high_gamma_cv',\n",
    "            #'frontal_seizure_psd_sef50', # high for seizure type\n",
    "            'frontal_seizure_psd_sef75', # high for seizure type, highish for other two\n",
    "            #'frontal_seizure_psd_sef90',\n",
    "            #'frontal_seizure_psd_spectral_centroid', # high for seizure type\n",
    "            'frontal_seizure_psd_spectral_spread',\n",
    "            'temporal_seizure_psd_high_gamma_cv',\n",
    "            'temporal_seizure_psd_sef50',\n",
    "            'parietal_seizure_psd_high_gamma_cv',\n",
    "            #'occipital_seizure_psd_sef50', # high for all\n",
    "            #'occipital_seizure_psd_sef75', # hihg for all\n",
    "            #'occipital_seizure_psd_sef90', # high for all\n",
    "            'occipital_seizure_psd_sef95',\n",
    "            'central_seizure_psd_gamma_cv',\n",
    "            'central_seizure_psd_high_gamma_cv',\n",
    "            #'central_seizure_psd_sef50', # high for seizure type\n",
    "            #'central_seizure_psd_sef75', # high for seizure type\n",
    "            'central_seizure_psd_sef90', # high for seizure type and lateralization\n",
    "            #'central_seizure_psd_sef95',\n",
    "            'central_seizure_psd_spectral_centroid', # high for seizure type\n",
    "            'central_seizure_psd_spectral_spread',\n",
    "            #'left_seizure_psd_delta_cv', \n",
    "            'left_seizure_psd_alpha_cv',\n",
    "            #'left_seizure_psd_high_beta_cv',\n",
    "            'left_seizure_psd_gamma_cv',\n",
    "            'left_seizure_psd_high_gamma_cv',\n",
    "            'left_seizure_psd_sef50', # high for seizure type\n",
    "            'left_seizure_psd_sef75', # high for seizure type\n",
    "            'left_seizure_psd_sef90',\n",
    "            #'left_seizure_psd_spectral_centroid', # high for seizure type\n",
    "            #'left_seizure_psd_spectral_spread',\n",
    "            'right_seizure_psd_sef50', # high for all\n",
    "            'right_seizure_psd_sef75', # high for all\n",
    "            'right_seizure_psd_sef90', # high for all\n",
    "            'right_seizure_psd_sef95',\n",
    "            'seizure_wt_level0_mean_std',\n",
    "            'seizure_wt_level0_std_std',\n",
    "            'seizure_wt_level0_max_std',\n",
    "            'seizure_wt_level1_entropy_std',\n",
    "            'seizure_wt_level1_std_std',\n",
    "            'seizure_wt_level1_max_std',\n",
    "            #'seizure_wt_level2_entropy_std',\n",
    "            'seizure_wt_level2_mean_std',\n",
    "            'seizure_wt_level2_std_std',\n",
    "            'seizure_wt_level2_max_std',\n",
    "            #'seizure_wt_level3_entropy_std', # high for all\n",
    "            'seizure_wt_level3_mean_mean', # high for all\n",
    "            'seizure_wt_level3_mean_std', # high for all\n",
    "            'seizure_wt_level3_std_mean', # high for all\n",
    "            'seizure_wt_level3_std_std',\n",
    "            #'seizure_wt_level3_max_std', # high for all\n",
    "            'seizure_wt_level4_energy_mean',\n",
    "            'seizure_wt_level4_mean_std',\n",
    "            'seizure_wt_level4_std_std',\n",
    "            'seizure_wt_level5_mean_std',\n",
    "            #'seizure_time_mean_std', # high for seizure type\n",
    "            #'seizure_time_mean_max',\n",
    "            #'seizure_time_std_std', # high for all\n",
    "            #'seizure_time_std_max',\n",
    "            'seizure_time_rms_std', # high for all\n",
    "            'seizure_time_rms_max',\n",
    "            'seizure_time_peak_to_peak_std', # high for all\n",
    "            #'seizure_time_peak_to_peak_max',\n",
    "            'seizure_time_zero_crossings_mean', # high for all\n",
    "            'seizure_time_zero_crossings_std', # high for all\n",
    "            #'seizure_time_zero_crossings_max',\n",
    "            'seizure_time_line_length_std', # high for all\n",
    "            'seizure_time_line_length_max', # high for all\n",
    "            'seizure_time_line_length_min',\n",
    "            'seizure_rhythmic_fast_theta_ratio',\n",
    "            'seizure_mean_propagation_speed',\n",
    "            'seizure_std_propagation_speed', # high for all\n",
    "            'seizure_max_propagation_speed',\n",
    "]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-15T17:04:56.929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pl.read_parquet(\"processed_data/comprehensive_eeg_features.parquet\")\n",
    "print(f\"Data shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-15T17:04:56.929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(df, target_column, features):\n",
    "    \"\"\"Prepare data for training\"\"\"\n",
    "    # Filter out samples without labels\n",
    "    labeled_df = df.filter(pl.col(target_column) != \"\")\n",
    "    \n",
    "    # If no features specified, use all except targets\n",
    "    if not features:\n",
    "        exclude_cols = ['seizure_id', 'seizure_index', 'seizure_type', \n",
    "                       'localization', 'lateralization']\n",
    "        features = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    X = labeled_df.select(features).to_numpy()\n",
    "    y = labeled_df.select(target_column).to_numpy().ravel()\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Calculate class weights for imbalance\n",
    "    unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "    class_weights = {}\n",
    "    for cls, count in zip(unique, counts):\n",
    "        class_weights[cls] = len(y_encoded) / (len(unique) * count)\n",
    "    \n",
    "    print(f\"Features: {len(features)}\")\n",
    "    print(f\"Samples: {len(X)}\")\n",
    "    print(f\"Classes: {label_encoder.classes_}\")\n",
    "    print(f\"Class distribution: {dict(zip(unique, counts))}\")\n",
    "    print(f\"Class weights: {class_weights}\")\n",
    "    \n",
    "    return X, y_encoded, label_encoder, features, class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-15T17:04:56.930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def objective(trial, X_train, y_train, X_val, y_val, class_weights):\n",
    "    \"\"\"Optuna objective function for hyperparameter tuning\"\"\"\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'multi:softprob' if n_classes > 2 else 'binary:logistic',\n",
    "        'num_class': n_classes if n_classes > 2 else None,\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 4),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.03, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 0.1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 0.1, 0.5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 0.4),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 4.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1000, 3000),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'tree_method': 'approx',\n",
    "        'device': 'cpu'\n",
    "    }\n",
    "    \n",
    "    # Remove None values\n",
    "    params = {k: v for k, v in params.items() if v is not None}\n",
    "    \n",
    "    # Add sample weights for training\n",
    "    sample_weights = np.array([class_weights[y] for y in y_train])\n",
    "    \n",
    "    # Train model\n",
    "    if n_classes > 2:\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "    else:\n",
    "        model = xgb.XGBClassifier(**params, scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        sample_weight=sample_weights,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_val)\n",
    "    score = balanced_accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-15T17:04:56.930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(X, y, label_encoder, features, class_weights):\n",
    "    \"\"\"Train XGBoost model with optional Optuna tuning\"\"\"\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    \n",
    "    n_classes = len(np.unique(y))\n",
    "    \n",
    "    # Default parameters\n",
    "    best_params = {\n",
    "        'objective': 'multi:softprob' if n_classes > 2 else 'binary:logistic',\n",
    "        'num_class': n_classes if n_classes > 2 else None,\n",
    "        'max_depth': 1,\n",
    "        'learning_rate': 0.017945399517256322,\n",
    "        'subsample': 0.8142447990706547,\n",
    "        'colsample_bytree': 0.8067253171735458,\n",
    "        'min_child_weight': 4,\n",
    "        'gamma': 0.3506098841684803,\n",
    "        'reg_alpha': 0.20782777353226756,\n",
    "        'reg_lambda': 2.972148679745178,\n",
    "        'n_estimators': 958,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'cpu'\n",
    "    }\n",
    "    \n",
    "    # Optuna hyperparameter tuning\n",
    "    if USE_OPTUNA:\n",
    "        print(\"\\nStarting Optuna hyperparameter tuning...\")\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "        study.optimize(\n",
    "            lambda trial: objective(trial, X_train, y_train, X_test, y_test, class_weights),\n",
    "            n_trials=N_TRIALS,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # Get best parameters\n",
    "        best_params.update(study.best_params)\n",
    "        print(f\"Best trial score: {study.best_value:.4f}\")\n",
    "        print(f\"Best parameters: {study.best_params}\")\n",
    "        \n",
    "        # Save parameters\n",
    "        with open(output_dir / 'best_params.json', 'w') as f:\n",
    "            json.dump(study.best_params, f, indent=2)\n",
    "        print(f\"Parameters saved to {output_dir / 'best_params.json'}\")\n",
    "    \n",
    "    # Remove None values\n",
    "    best_params = {k: v for k, v in best_params.items() if v is not None}\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(\"\\nTraining final model...\")\n",
    "    sample_weights = np.array([class_weights[y] for y in y_train])\n",
    "    \n",
    "    if n_classes > 2:\n",
    "        model = xgb.XGBClassifier(**best_params)\n",
    "    else:\n",
    "        model = xgb.XGBClassifier(**best_params, scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        sample_weight=sample_weights,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        #early_stopping_rounds=100,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return model, X_test, y_test, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-15T17:04:56.930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, label_encoder):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {bal_acc:.4f}\")\n",
    "    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "    print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "    print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=label_encoder.classes_,\n",
    "                yticklabels=label_encoder.classes_)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'confusion_matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy': bal_acc,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'mcc': mcc\n",
    "    }\n",
    "    \n",
    "    # Save metrics\n",
    "    with open(output_dir / 'metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-15T17:04:56.930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, features, top_n=50):\n",
    "    \"\"\"Plot and save feature importance\"\"\"\n",
    "    # Get feature importance\n",
    "    importance = model.feature_importances_\n",
    "    indices = np.argsort(importance)[::-1][:top_n]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(indices)), importance[indices])\n",
    "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top {top_n} Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'feature_importance.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save feature importance to file\n",
    "    importance_dict = {features[i]: float(importance[i]) for i in range(len(features))}\n",
    "    importance_sorted = dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    with open(output_dir / 'feature_importance.json', 'w') as f:\n",
    "        json.dump(importance_sorted, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nTop {min(top_n, len(features))} features:\")\n",
    "    for i, idx in enumerate(indices[:top_n]):\n",
    "        print(f\"{features[idx]}: {importance[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-15T17:04:56.930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_model(X, y, features, class_weights, best_params, n_splits=12):\n",
    "    \"\"\"Perform stratified cross-validation\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    n_classes = len(np.unique(y))\n",
    "    \n",
    "    scores = {\n",
    "        'accuracy': [],\n",
    "        'balanced_accuracy': [],\n",
    "        'f1_macro': [],\n",
    "        'mcc': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{n_splits}-Fold Cross-Validation\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Sample weights\n",
    "        sample_weights = np.array([class_weights[y] for y in y_train])\n",
    "        \n",
    "        # Train model\n",
    "        if n_classes > 2:\n",
    "            model = xgb.XGBClassifier(**best_params)\n",
    "        else:\n",
    "            model = xgb.XGBClassifier(**best_params, scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            sample_weight=sample_weights,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            #early_stopping_rounds=100,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_val)\n",
    "        scores['accuracy'].append(accuracy_score(y_val, y_pred))\n",
    "        scores['balanced_accuracy'].append(balanced_accuracy_score(y_val, y_pred))\n",
    "        scores['f1_macro'].append(f1_score(y_val, y_pred, average='macro'))\n",
    "        scores['mcc'].append(matthews_corrcoef(y_val, y_pred))\n",
    "        \n",
    "        print(f\"Fold {fold}: Bal Acc={scores['balanced_accuracy'][-1]:.4f}, \"\n",
    "              f\"F1={scores['f1_macro'][-1]:.4f}, MCC={scores['mcc'][-1]:.4f}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\nCross-Validation Summary:\")\n",
    "    for metric, values in scores.items():\n",
    "        mean_score = np.mean(values)\n",
    "        std_score = np.std(values)\n",
    "        print(f\"{metric}: {mean_score:.4f} (+/- {std_score:.4f})\")\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-15T17:04:56.930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X, y, label_encoder, features, class_weights = prepare_data(df, TARGET_COLUMN, main_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-15T17:04:56.930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "model, X_test, y_test, best_params = train_model(X, y, label_encoder, features, class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-15T17:04:56.931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "metrics = evaluate_model(model, X_test, y_test, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-15T17:04:56.931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "plot_feature_importance(model, features, top_n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-15T17:04:56.931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "cv_scores = cross_validate_model(X, y, features, class_weights, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-15T17:04:56.931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_model(str(output_dir / 'xgboost_model.json'))\n",
    "joblib.dump(label_encoder, output_dir / 'label_encoder.pkl')\n",
    "print(f\"\\nModel saved to {output_dir}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8199215,
     "sourceId": 13040090,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
