{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65183a8e",
   "metadata": {},
   "source": [
    "# Epilepsy Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043b8dc",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78b7c0",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Epilepsy Data Processing Pipeline\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pywt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MNE Libraries\n",
    "import mne\n",
    "from mne import Epochs, pick_types, events_from_annotations\n",
    "from mne.channels import make_standard_montage\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.datasets import eegbci\n",
    "from mne.decoding import CSP\n",
    "from mne.preprocessing import ICA, create_eog_epochs, create_ecg_epochs\n",
    "from mne_connectivity import spectral_connectivity_epochs\n",
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "# Scipy and Scikit-learn\n",
    "from scipy import signal, stats\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b0576",
   "metadata": {},
   "source": [
    "## Load Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ad4e1",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_metadata_from_json(data_dir='data'):\n",
    "    \"\"\"Load patient and seizure metadata from JSON files using Polars\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    all_seizures = []\n",
    "    all_patients = []\n",
    "    all_non_seizures = []\n",
    "    \n",
    "    json_files = sorted(list(data_dir.glob('**/*.json')))\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract patient-level information\n",
    "            patient_info = {\n",
    "                'patient_id': data['patient_id'],\n",
    "                'age': data['age'],\n",
    "                'gender': data['gender'],\n",
    "                'sampling_rate_hz': data['sampling_rate_hz'],\n",
    "                'num_channels': len(data['channels']),\n",
    "                'json_file_path': str(json_file),\n",
    "                'eeg_channel': data['eeg_channel'],\n",
    "                'seizure_type': data['seizure_type'],\n",
    "                'localization': data['localization'],\n",
    "                'lateralization': data['lateralization'],\n",
    "            }\n",
    "            \n",
    "            if 'file_name' in data:\n",
    "                patient_info['file_name'] = data['file_name']\n",
    "                patient_info['registration_start_time'] = data.get('registration_start_time')\n",
    "                patient_info['registration_end_time'] = data.get('registration_end_time')\n",
    "            \n",
    "            all_patients.append(patient_info)\n",
    "            \n",
    "            # Process each seizure\n",
    "            for seizure in data['seizures']:\n",
    "                seizure_record = {\n",
    "                    'patient_id': data['patient_id'],\n",
    "                    'seizure_bool': True,\n",
    "                    'sampling_rate_hz': data['sampling_rate_hz'],\n",
    "                    'seizure_number': seizure['seizure_number']\n",
    "                }\n",
    "                \n",
    "                for key, value in seizure.items():\n",
    "                    seizure_record[key] = value\n",
    "                \n",
    "                if 'file_name' in patient_info and 'file_name' not in seizure:\n",
    "                    seizure_record['file_name'] = patient_info['file_name']\n",
    "                    if 'registration_start_time' in patient_info:\n",
    "                        seizure_record['registration_start_time'] = patient_info['registration_start_time']\n",
    "                        seizure_record['registration_end_time'] = patient_info['registration_end_time']\n",
    "                \n",
    "                all_seizures.append(seizure_record)\n",
    "                \n",
    "            # Check if non_seizures exists before processing\n",
    "            if 'non-seizures' in data:\n",
    "                for non_seizure in data['non-seizures']:\n",
    "                    non_seizure_record = {\n",
    "                        'patient_id': data['patient_id'],\n",
    "                        'seizure_bool': False,\n",
    "                        'file_name': non_seizure['file_name'],\n",
    "                        'registration_start_time': non_seizure['registration_start_time'],\n",
    "                        'registration_end_time': non_seizure['registration_end_time']\n",
    "                    }\n",
    "                    \n",
    "                    all_non_seizures.append(non_seizure_record)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to Polars DataFrames\n",
    "    seizures_df = pl.DataFrame(all_seizures)\n",
    "    patients_df = pl.DataFrame(all_patients)\n",
    "    non_seizures_df = pl.DataFrame(all_non_seizures)\n",
    "    \n",
    "    return seizures_df, patients_df, non_seizures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb5731",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "print(\"Loading metadata from JSON files...\")\n",
    "\n",
    "# change to other folder for full set\n",
    "seizures_df, patients_df, non_seizures_df = load_metadata_from_json(\n",
    "    'data'\n",
    "    )\n",
    "print(f\"Loaded {len(patients_df)} patients and {len(seizures_df)} seizures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ba1b6",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fix_time_columns(df, time_cols):\n",
    "    for col in time_cols:\n",
    "        if col in df.columns:\n",
    "            # First replace \"N/A\" with null\n",
    "            df = df.with_columns(\n",
    "                pl.when(pl.col(col) == \"N/A\")\n",
    "                .then(None)\n",
    "                .otherwise(pl.col(col))\n",
    "                .alias(col)\n",
    "            )\n",
    "            \n",
    "            # Replace literal '.' with ':' for non-null values\n",
    "            df = df.with_columns(\n",
    "                pl.when(pl.col(col).is_not_null())\n",
    "                .then(pl.col(col).str.replace_all(\".\", \":\", literal=True))\n",
    "                .otherwise(None)\n",
    "                .alias(col)\n",
    "            )\n",
    "            \n",
    "            # Split time string and wrap hours > 24 for non-null values\n",
    "            df = df.with_columns(\n",
    "                pl.when(pl.col(col).is_not_null())\n",
    "                .then(pl.col(col).str.split(\":\").list.to_struct()\n",
    "                      .struct.rename_fields([\"hour\", \"minute\", \"second\"]))\n",
    "                .otherwise(None)\n",
    "                .alias(col + \"_split\")\n",
    "            )\n",
    "            \n",
    "            # Reconstruct time with wrapped hours for non-null values\n",
    "            df = df.with_columns(\n",
    "                pl.when(pl.col(col + \"_split\").is_not_null())\n",
    "                .then(\n",
    "                    pl.concat_str(\n",
    "                        [\n",
    "                            (pl.col(col + \"_split\").struct.field(\"hour\").cast(pl.Int32) % 24).cast(pl.Utf8).str.zfill(2),\n",
    "                            pl.col(col + \"_split\").struct.field(\"minute\").cast(pl.Utf8).str.zfill(2),\n",
    "                            pl.col(col + \"_split\").struct.field(\"second\").cast(pl.Utf8).str.zfill(2)\n",
    "                        ],\n",
    "                        separator=\":\"\n",
    "                    )\n",
    "                )\n",
    "                .otherwise(None)\n",
    "                .alias(col)\n",
    "            ).drop(col + \"_split\")\n",
    "            \n",
    "            # Convert to time dtype for non-null values\n",
    "            df = df.with_columns(\n",
    "                pl.when(pl.col(col).is_not_null())\n",
    "                .then(pl.col(col).str.to_time(\"%H:%M:%S\"))\n",
    "                .otherwise(None)\n",
    "                .alias(col)\n",
    "            )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfafa88b",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "time_cols = [\n",
    "    'registration_start_time',\n",
    "    'registration_end_time',\n",
    "    'seizure_start_time',\n",
    "    'seizure_end_time'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5cfb0",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seizures_df = fix_time_columns(seizures_df, time_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efafc6e3-beef-478f-8e9b-ed738e40b92f",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seizures_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd397b94",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_to_csv(df, output_path='comprehensive_df.csv'):\n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        df_pd = df.to_pandas()\n",
    "    else:\n",
    "        df_pd = df\n",
    "    \n",
    "    df_pd.to_csv(output_path, index=False)\n",
    "    print(f\"Dataset saved to: {output_path}\")\n",
    "    print(f\"File size: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "def save_to_parquet(df, output_path='comprehensive_eeg_features.parquet'):\n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        df.write_parquet(output_path)\n",
    "    else:\n",
    "        df.to_parquet(output_path, index=False, engine='pyarrow')\n",
    "    \n",
    "    print(f\"Dataset saved to: {output_path}\")\n",
    "    print(f\"File size: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e55050",
   "metadata": {},
   "source": [
    "## Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059a592e",
   "metadata": {},
   "source": [
    "### PSD Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e890490",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_psd_features(raw, fmin=0.4, fmax=50, prefix=\"seizure\"):\n",
    "    \"\"\"Extract PSD features without restrictive conditions\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Get data\n",
    "        data = raw.get_data()\n",
    "        sfreq = raw.info['sfreq']\n",
    "        \n",
    "        # Calculate PSD using multitaper (more robust than welch for non-stationary signals)\n",
    "        psds, freqs = psd_array_multitaper(\n",
    "            data, sfreq, fmin=fmin, fmax=fmax, \n",
    "            adaptive=True, normalization='full', verbose=False\n",
    "        )\n",
    "        \n",
    "        # Extended frequency bands\n",
    "        bands = {\n",
    "            'delta': (0.4, 4),\n",
    "            'theta': (4, 7),\n",
    "            'alpha': (7, 12),\n",
    "            'low_beta': (12, 20),\n",
    "            'high_beta': (20, 30),\n",
    "            'gamma': (30, 50),\n",
    "            'high_gamma': (50, 60)\n",
    "        }\n",
    "        \n",
    "        band_powers = {}\n",
    "        \n",
    "        for band_name, (low_freq, high_freq) in bands.items():\n",
    "            freq_mask = (freqs >= low_freq) & (freqs < high_freq)\n",
    "            if freq_mask.any():\n",
    "                band_power = np.mean(psds[:, freq_mask], axis=1)\n",
    "                band_powers[band_name] = band_power\n",
    "                \n",
    "                # Store band power statistics\n",
    "                features[f'{prefix}_psd_{band_name}_mean'] = float(np.mean(band_power) * 1e+8)\n",
    "                features[f'{prefix}_psd_{band_name}_std'] = float(np.std(band_power) * 1e+8)\n",
    "                features[f'{prefix}_psd_{band_name}_cv'] = float(np.std(band_power) / (np.mean(band_power) + 1e-10))  # Coefficient of variation\n",
    "        \n",
    "        # Calculate band power ratios (important for seizure detection)\n",
    "        if 'theta' in band_powers and 'alpha' in band_powers:\n",
    "            features[f'{prefix}_psd_theta_alpha_ratio'] = float(np.mean(band_powers['theta']) / (np.mean(band_powers['alpha']) + 1e-8))\n",
    "        \n",
    "        if 'delta' in band_powers and 'alpha' in band_powers:\n",
    "            features[f'{prefix}_psd_delta_alpha_ratio'] = float(np.mean(band_powers['delta']) / (np.mean(band_powers['alpha']) + 1e-8))\n",
    "        \n",
    "        if 'low_beta' in band_powers and 'high_beta' in band_powers:\n",
    "            features[f'{prefix}_psd_beta_ratio'] = float(np.mean(band_powers['high_beta']) / (np.mean(band_powers['low_beta']) + 1e-8))\n",
    "        \n",
    "        # Spectral edge frequencies (SEF50, SEF90, SEF95)\n",
    "        mean_psd = np.mean(psds, axis=0)\n",
    "        cumsum_psd = np.cumsum(mean_psd)\n",
    "        cumsum_psd = cumsum_psd / cumsum_psd[-1]\n",
    "        \n",
    "        for percentile in [50, 75, 90, 95]:\n",
    "            edge_idx = np.where(cumsum_psd >= percentile/100)[0]\n",
    "            if len(edge_idx) > 0:\n",
    "                features[f'{prefix}_psd_sef{percentile}'] = float(freqs[edge_idx[0]])\n",
    "            else:\n",
    "                features[f'{prefix}_psd_sef{percentile}'] = float(freqs[-1])\n",
    "        \n",
    "        # Spectral centroid and spread\n",
    "        freq_weights = freqs * mean_psd\n",
    "        spectral_centroid = np.sum(freq_weights) / (np.sum(mean_psd) + 1e-8)\n",
    "        features[f'{prefix}_psd_spectral_centroid'] = float(spectral_centroid)\n",
    "        \n",
    "        # Spectral spread (standard deviation around centroid)\n",
    "        spectral_spread = np.sqrt(np.sum(((freqs - spectral_centroid) ** 2) * mean_psd) / (np.sum(mean_psd) + 1e-8))\n",
    "        features[f'{prefix}_psd_spectral_spread'] = float(spectral_spread)\n",
    "        \n",
    "        # Spectral skewness and kurtosis\n",
    "        if spectral_spread > 0:\n",
    "            spectral_skewness = np.sum(((freqs - spectral_centroid) ** 3) * mean_psd) / ((spectral_spread ** 3) * np.sum(mean_psd) + 1e-8)\n",
    "            spectral_kurtosis = np.sum(((freqs - spectral_centroid) ** 4) * mean_psd) / ((spectral_spread ** 4) * np.sum(mean_psd) + 1e-8)\n",
    "            features[f'{prefix}_psd_spectral_skewness'] = float(spectral_skewness)\n",
    "            features[f'{prefix}_psd_spectral_kurtosis'] = float(spectral_kurtosis)\n",
    "        else:\n",
    "            features[f'{prefix}_psd_spectral_skewness'] = 0.0\n",
    "            features[f'{prefix}_psd_spectral_kurtosis'] = 0.0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in enhanced PSD calculation: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7eace",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_psd_features_by_region(raw, fmin=0.4, fmax=60, prefix=\"seizure\"):\n",
    "    # Define channel groups for standard monopolar montage\n",
    "    monopolar_channel_groups = {\n",
    "        'frontal': ['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'Fz'],\n",
    "        'central': ['C3', 'C4', 'Cz'],\n",
    "        'parietal': ['P3', 'P4', 'Pz'],\n",
    "        'occipital': ['O1', 'O2'],\n",
    "        'temporal': ['T3', 'T4', 'T5', 'T6'],\n",
    "        'left': ['Fp1', 'F3', 'F7', 'C3', 'P3', 'O1', 'T3', 'T5'],\n",
    "        'right': ['Fp2', 'F4', 'F8', 'C4', 'P4', 'O2', 'T4', 'T6']\n",
    "    }\n",
    "    \n",
    "    # Define channel groups for bipolar montage\n",
    "    bipolar_channel_groups = {\n",
    "        'frontal': ['FP1-F7', 'FP1-F3', 'FP2-F4', 'FP2-F8', 'F7-T7', 'F3-C3', 'F4-C4', 'F8-T8', 'FZ-CZ'],\n",
    "        'temporal': ['F7-T7', 'F8-T8', 'T7-P7', 'T8-P8-0', 'T8-P8-1', 'P7-T7', 'T7-FT9', 'FT10-T8'],\n",
    "        'frontotemporal': ['T7-FT9', 'FT9-FT10', 'FT10-T8'],\n",
    "        'parietal': ['T7-P7', 'C3-P3', 'C4-P4', 'T8-P8-0', 'T8-P8-1', 'P7-O1', 'P3-O1', 'P4-O2', 'P8-O2', 'CZ-PZ', 'P7-T7'],\n",
    "        'occipital': ['P7-O1', 'P3-O1', 'P4-O2', 'P8-O2'],\n",
    "        'central': ['F3-C3', 'C3-P3', 'F4-C4', 'C4-P4', 'FZ-CZ', 'CZ-PZ'],\n",
    "        'left': ['FP1-F7', 'F7-T7', 'T7-P7', 'P7-O1', 'FP1-F3', 'F3-C3', 'C3-P3', 'P3-O1', 'P7-T7', 'T7-FT9'],\n",
    "        'right': ['FP2-F4', 'F4-C4', 'C4-P4', 'P4-O2', 'FP2-F8', 'F8-T8', 'T8-P8-0', 'P8-O2', 'FT10-T8', 'T8-P8-1'],\n",
    "        'midline': ['FZ-CZ', 'CZ-PZ'],\n",
    "        'cross_hemisphere': ['FT9-FT10'],\n",
    "        'left_lateral_chain': ['FP1-F7', 'F7-T7', 'T7-P7', 'P7-O1'],\n",
    "        'left_parasagittal_chain': ['FP1-F3', 'F3-C3', 'C3-P3', 'P3-O1'],\n",
    "        'right_parasagittal_chain': ['FP2-F4', 'F4-C4', 'C4-P4', 'P4-O2'],\n",
    "        'right_lateral_chain': ['FP2-F8', 'F8-T8', 'T8-P8-0', 'P8-O2'],\n",
    "        'midline_chain': ['FZ-CZ', 'CZ-PZ']\n",
    "    }\n",
    "    \n",
    "    available_channels = raw.ch_names\n",
    "    \n",
    "    # Detect channel type based on presence of '-' in channel names\n",
    "    def detect_channel_type(channels):\n",
    "        bipolar_count = sum(1 for ch in channels if '-' in ch and not ch.startswith('EEG'))\n",
    "        monopolar_count = sum(1 for ch in channels if '-' not in ch or ch.startswith('EEG'))\n",
    "        \n",
    "        # If majority of channels contain '-', it's likely bipolar montage\n",
    "        if bipolar_count > monopolar_count * 0.5:\n",
    "            return 'bipolar'\n",
    "        else:\n",
    "            return 'monopolar'\n",
    "    \n",
    "    # Determine which channel groups to use\n",
    "    channel_type = detect_channel_type(available_channels)\n",
    "    \n",
    "    if channel_type == 'bipolar':\n",
    "        print(\"Detected bipolar montage channels\")\n",
    "        channel_groups = bipolar_channel_groups\n",
    "        \n",
    "        # Function to find matching bipolar channels\n",
    "        def find_matching_channels(available_channels, channel_list):\n",
    "            region_channels = []\n",
    "            for ch in available_channels:\n",
    "                ch_clean = ch.upper().replace('EEG', '').replace(' ', '').strip()\n",
    "                for target_ch in channel_list:\n",
    "                    target_clean = target_ch.upper()\n",
    "                    # Check for exact match after normalization\n",
    "                    if ch_clean == target_clean:\n",
    "                        region_channels.append(ch)\n",
    "                        break\n",
    "                    # Check for P8 variations (P8-O2 vs T8-P8-0/T8-P8-1)\n",
    "                    elif 'P8' in target_clean and 'P8' in ch_clean:\n",
    "                        # Handle the special case of P8 channels\n",
    "                        if (target_clean in ch_clean) or (ch_clean in target_clean):\n",
    "                            region_channels.append(ch)\n",
    "                            break\n",
    "            return region_channels\n",
    "            \n",
    "    else:\n",
    "        print(\"Detected monopolar/standard montage channels\")\n",
    "        channel_groups = monopolar_channel_groups\n",
    "        \n",
    "        # Function to find matching monopolar channels (your original logic)\n",
    "        def find_matching_channels(available_channels, channel_list):\n",
    "            region_channels = []\n",
    "            for ch in available_channels:\n",
    "                ch_clean = ch.upper().replace('EEG', '').replace('-', '').replace(' ', '').strip()\n",
    "                for target_ch in channel_list:\n",
    "                    if target_ch.upper() in ch_clean or ch_clean in target_ch.upper():\n",
    "                        region_channels.append(ch)\n",
    "                        break\n",
    "            return region_channels\n",
    "    \n",
    "    # Process each region\n",
    "    regional_features = {}\n",
    "    \n",
    "    for region, channel_list in channel_groups.items():\n",
    "        # Find matching channels for this region\n",
    "        region_channels = find_matching_channels(available_channels, channel_list)\n",
    "        \n",
    "        if region_channels:\n",
    "            try:\n",
    "                # Pick channels for this region\n",
    "                raw_region = raw.copy().pick(region_channels)\n",
    "                \n",
    "                # Extract features for this region\n",
    "                region_psd_features = extract_psd_features(raw_region, fmin, fmax, prefix)\n",
    "                \n",
    "                # Add region prefix to feature names\n",
    "                for feature_name, value in region_psd_features.items():\n",
    "                    regional_features[f'{prefix}_{region}_{feature_name}'] = value\n",
    "                    \n",
    "                #print(f\"Processed {region} region with {len(region_channels)} channels\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Could not process {region} region: {e}\")\n",
    "        else:\n",
    "            print(f\"No channels found for {region} region\")\n",
    "    \n",
    "    print(f\"Total regional features extracted: {len(regional_features)}\")\n",
    "    return regional_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b817e",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def parse_seizure_time(time_str):\n",
    "    if pd.isna(time_str) or time_str == '' or time_str is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        time_str = str(time_str).strip()\n",
    "        \n",
    "        # Handle HH:MM:SS or MM:SS format\n",
    "        if '.' in time_str:\n",
    "            parts = time_str.split('.')\n",
    "            if len(parts) == 3:\n",
    "                hours, minutes, seconds = map(float, parts)\n",
    "                return hours * 3600 + minutes * 60 + seconds\n",
    "            elif len(parts) == 2:\n",
    "                minutes, seconds = map(float, parts)\n",
    "                return minutes * 60 + seconds\n",
    "        \n",
    "        # Handle pure seconds\n",
    "        return float(time_str)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse time '{time_str}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b7b02",
   "metadata": {},
   "source": [
    "### Differential Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574fb55",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_differential_entropy(data, sfreq, bands=None, prefix=\"seizure\"):\n",
    "    if bands is None:\n",
    "        bands = {\n",
    "            'delta': (0.4, 4),\n",
    "            'theta': (4, 7),\n",
    "            'alpha': (7, 12),\n",
    "            'low_beta': (12, 20),\n",
    "            'high_beta': (20, 30),\n",
    "            'gamma': (30, 50),\n",
    "            'high_gamma': (50, 60)\n",
    "        }\n",
    "    \n",
    "    features = {}\n",
    "    n_channels = data.shape[0]\n",
    "    \n",
    "    for band_name, (low_freq, high_freq) in bands.items():\n",
    "        # Bandpass filter for each band\n",
    "        try:\n",
    "            filtered_data = mne.filter.filter_data(\n",
    "                data, sfreq, l_freq=low_freq, h_freq=high_freq, \n",
    "                verbose=False, method='fir', fir_design='firwin'\n",
    "            )\n",
    "            \n",
    "            # Calculate variance for each channel\n",
    "            variances = np.var(filtered_data, axis=1)\n",
    "            \n",
    "            # Calculate DE: 0.5 * log(2 * pi * e * variance)\n",
    "            # Adding small epsilon to avoid log(0)\n",
    "            de_values = 0.5 * np.log(2 * np.pi * np.e * (variances + 1e-8))\n",
    "            \n",
    "            # Store statistics\n",
    "            features[f'{prefix}_de_{band_name}_mean'] = float(np.mean(de_values))\n",
    "            features[f'{prefix}_de_{band_name}_std'] = float(np.std(de_values))\n",
    "            features[f'{prefix}_de_{band_name}_median'] = float(np.median(de_values))\n",
    "            features[f'{prefix}_de_{band_name}_max'] = float(np.max(de_values))\n",
    "            features[f'{prefix}_de_{band_name}_min'] = float(np.min(de_values))\n",
    "            \n",
    "            # Asymmetry features (frontal asymmetry is important for emotion/seizure)\n",
    "            if n_channels >= 2:\n",
    "                # Calculate hemispheric asymmetry\n",
    "                left_channels = de_values[:n_channels//2]\n",
    "                right_channels = de_values[n_channels//2:]\n",
    "                min_len = min(len(left_channels), len(right_channels))\n",
    "                if min_len > 0:\n",
    "                    asymmetry = left_channels[:min_len] - right_channels[:min_len]\n",
    "                    features[f'{prefix}_de_{band_name}_asymmetry_mean'] = float(np.mean(asymmetry))\n",
    "                    features[f'{prefix}_de_{band_name}_asymmetry_std'] = float(np.std(asymmetry))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating DE for {band_name}: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f767c41b",
   "metadata": {},
   "source": [
    "### Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe2c52",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_simple_propagation_features(raw, seizure_start=None, seizure_end=None, prefix=\"seizure\"):\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        data = raw.get_data()\n",
    "        sfreq = raw.info['sfreq']\n",
    "        n_channels, n_samples = data.shape\n",
    "        \n",
    "        # Apply bandpass filter for seizure activity\n",
    "        data_filtered = mne.filter.filter_data(\n",
    "            data, sfreq, l_freq=3, h_freq=30, verbose=False\n",
    "        )\n",
    "        \n",
    "        # Calculate channel-wise power changes\n",
    "        onset_times = []\n",
    "        \n",
    "        for ch_idx in range(n_channels):\n",
    "            channel_data = data_filtered[ch_idx, :]\n",
    "            \n",
    "            # Calculate envelope\n",
    "            analytic_signal = signal.hilbert(channel_data)\n",
    "            envelope = np.abs(analytic_signal)\n",
    "            \n",
    "            # Simple smoothing\n",
    "            window_samples = max(6, int(0.1 * sfreq))\n",
    "            if window_samples % 2 == 0:\n",
    "                window_samples += 1\n",
    "            envelope_smooth = signal.savgol_filter(\n",
    "                envelope, \n",
    "                window_samples, \n",
    "                min(1, window_samples-1)\n",
    "            )\n",
    "            \n",
    "            # Find the time of maximum activity\n",
    "            if seizure_start is not None and seizure_end is not None:\n",
    "                # Look in seizure window\n",
    "                start_sample = max(0, int(seizure_start * sfreq))\n",
    "                end_sample = min(n_samples, int(seizure_end * sfreq))\n",
    "                if start_sample < end_sample:\n",
    "                    seizure_segment = envelope_smooth[start_sample:end_sample]\n",
    "                    if len(seizure_segment) > 0:\n",
    "                        max_idx = np.argmax(seizure_segment) + start_sample\n",
    "                        onset_times.append(max_idx / sfreq)\n",
    "            else:\n",
    "                # Use entire recording\n",
    "                if len(envelope_smooth) > 0:\n",
    "                    max_idx = np.argmax(envelope_smooth)\n",
    "                    onset_times.append(max_idx / sfreq)\n",
    "        \n",
    "        # Only calculate propagation statistics if we have onset times\n",
    "        if len(onset_times) > 0:\n",
    "            # Calculate delays between channels\n",
    "            onset_times = np.array(onset_times)\n",
    "            sorted_onsets = np.sort(onset_times)\n",
    "            delays = np.diff(sorted_onsets)\n",
    "            \n",
    "            # Simple propagation speed estimate \n",
    "            avg_distance_mm = 1  # 1mm = 1cm\n",
    "            speeds = []\n",
    "            \n",
    "            # Calculate speeds only for positive delays\n",
    "            positive_delays = delays[delays > 0]\n",
    "            if len(positive_delays) > 0:\n",
    "                for delay in positive_delays:\n",
    "                    speed = avg_distance_mm / delay\n",
    "                    speeds.append(speed)\n",
    "            \n",
    "            # Calculate speed statistics only if we have speeds\n",
    "            if len(speeds) > 0:\n",
    "                features[f'{prefix}_mean_propagation_speed'] = float(np.mean(speeds))\n",
    "                features[f'{prefix}_median_propagation_speed'] = float(np.median(speeds))\n",
    "                features[f'{prefix}_std_propagation_speed'] = float(np.std(speeds))\n",
    "                features[f'{prefix}_max_propagation_speed'] = float(np.max(speeds))\n",
    "                features[f'{prefix}_min_propagation_speed'] = float(np.min(speeds))\n",
    "                features[f'{prefix}_num_propagation_events'] = len(speeds)\n",
    "            \n",
    "            # Calculate delay statistics\n",
    "            if len(delays) > 0:\n",
    "                features[f'{prefix}_mean_onset_delay'] = float(np.mean(delays))\n",
    "                features[f'{prefix}_max_onset_delay'] = float(np.max(delays))\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in propagation calculation: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b9ff2d",
   "metadata": {},
   "source": [
    "### Wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e090633",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_wavelet_features(data, sfreq, wavelet='db4', levels=5, prefix=\"seizure\"):\n",
    "    features = {}\n",
    "    n_channels = data.shape[0]\n",
    "    \n",
    "    # Store features for each decomposition level\n",
    "    all_channel_features = []\n",
    "    \n",
    "    for ch_idx in range(n_channels):\n",
    "        channel_data = data[ch_idx, :]\n",
    "        \n",
    "        try:\n",
    "            # Perform wavelet decomposition\n",
    "            coeffs = pywt.wavedec(channel_data, wavelet, level=levels)\n",
    "            \n",
    "            # Calculate features for each level\n",
    "            channel_features = []\n",
    "            for level, coeff in enumerate(coeffs):\n",
    "                if len(coeff) > 0:\n",
    "                    # Energy of coefficients\n",
    "                    energy = np.sum(coeff ** 2)\n",
    "                    # Entropy\n",
    "                    entropy = stats.entropy(np.abs(coeff) + 1e-8)\n",
    "                    # Statistical features\n",
    "                    mean_coeff = np.mean(np.abs(coeff))\n",
    "                    std_coeff = np.std(coeff)\n",
    "                    max_coeff = np.max(np.abs(coeff))\n",
    "                    \n",
    "                    channel_features.extend([energy, entropy, mean_coeff, std_coeff, max_coeff])\n",
    "                else:\n",
    "                    channel_features.extend([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "            \n",
    "            all_channel_features.append(channel_features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in wavelet transform for channel {ch_idx}: {e}\")\n",
    "            # Add zeros for failed channel\n",
    "            all_channel_features.append([0.0] * (5 * (levels + 1)))\n",
    "    \n",
    "    # Aggregate across channels\n",
    "    all_channel_features = np.array(all_channel_features)\n",
    "    \n",
    "    # Store aggregated features\n",
    "    feature_names = ['energy', 'entropy', 'mean', 'std', 'max']\n",
    "    for level in range(levels + 1):\n",
    "        for feat_idx, feat_name in enumerate(feature_names):\n",
    "            feat_values = all_channel_features[:, level * 5 + feat_idx]\n",
    "            features[f'{prefix}_wt_level{level}_{feat_name}_mean'] = round(float(np.mean(feat_values)), 12)\n",
    "            features[f'{prefix}_wt_level{level}_{feat_name}_std'] = round(float(np.std(feat_values)), 12)\n",
    "    \n",
    "    # Calculate wavelet packet energy distribution\n",
    "    try:\n",
    "        # Use first channel for packet decomposition (computationally expensive for all)\n",
    "        wp = pywt.WaveletPacket(data[0, :], wavelet, maxlevel=3)\n",
    "        packet_energies = []\n",
    "        for node in wp.get_level(3):\n",
    "            packet_energies.append(np.sum(node.data ** 2))\n",
    "        \n",
    "        if packet_energies:\n",
    "            total_energy = sum(packet_energies)\n",
    "            if total_energy > 0:\n",
    "                normalized_energies = np.array([e/total_energy for e in packet_energies])\n",
    "                features[f'{prefix}_wt_packet_entropy'] = float(stats.entropy(normalized_energies + 1e-8))\n",
    "            else:\n",
    "                features[f'{prefix}_wt_packet_entropy'] = 0.0\n",
    "        else:\n",
    "            features[f'{prefix}_wt_packet_entropy'] = 0.0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in wavelet packet decomposition: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc3c8c7",
   "metadata": {},
   "source": [
    "### Time Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc39684b",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_time_domain_features(data, sfreq, prefix=\"seizure\"):\n",
    "    features = {}\n",
    "    n_channels, n_samples = data.shape\n",
    "    \n",
    "    try:\n",
    "        # Statistical features for each channel\n",
    "        channel_features = {\n",
    "            'mean': np.mean(data, axis=1),\n",
    "            'std': np.std(data, axis=1),\n",
    "            'var': np.var(data, axis=1),\n",
    "            'skewness': stats.skew(data, axis=1),\n",
    "            'kurtosis': stats.kurtosis(data, axis=1),\n",
    "            'rms': np.sqrt(np.mean(data ** 2, axis=1)),\n",
    "            'peak_to_peak': np.ptp(data, axis=1),\n",
    "            'zero_crossings': np.sum(np.diff(np.sign(data), axis=1) != 0, axis=1) / (n_samples / sfreq),  # Rate per second\n",
    "        }\n",
    "        \n",
    "        # Hjorth parameters\n",
    "        # Activity (variance of signal)\n",
    "        activity = (np.var(data, axis=1) * 1e+8)\n",
    "        \n",
    "        # Mobility (standard deviation of first derivative / standard deviation of signal)\n",
    "        first_deriv = np.diff(data, axis=1)\n",
    "        mobility = np.std(first_deriv, axis=1) / (np.std(data, axis=1) + 1e-8)\n",
    "        \n",
    "        # Complexity (mobility of first derivative / mobility of signal)\n",
    "        second_deriv = np.diff(first_deriv, axis=1)\n",
    "        mobility_deriv = np.std(second_deriv, axis=1) / (np.std(first_deriv, axis=1) + 1e-8)\n",
    "        complexity = mobility_deriv / (mobility + 1e-8)\n",
    "        \n",
    "        channel_features['hjorth_activity'] = activity\n",
    "        channel_features['hjorth_mobility'] = mobility\n",
    "        channel_features['hjorth_complexity'] = complexity\n",
    "        \n",
    "        # Line length (sum of absolute differences)\n",
    "        line_length = np.sum(np.abs(np.diff(data, axis=1)), axis=1) / n_samples\n",
    "        channel_features['line_length'] = line_length\n",
    "        \n",
    "        # Non-linear energy\n",
    "        nonlinear_energy = []\n",
    "        for ch in range(n_channels):\n",
    "            if n_samples >= 3:\n",
    "                nle = np.mean(data[ch, 1:-1]**2 - data[ch, :-2] * data[ch, 2:])\n",
    "                nonlinear_energy.append(nle)\n",
    "            else:\n",
    "                nonlinear_energy.append(0.0)\n",
    "        channel_features['nonlinear_energy'] = np.array(nonlinear_energy)\n",
    "        \n",
    "        # Aggregate features across channels\n",
    "        for feat_name, feat_values in channel_features.items():\n",
    "            features[f'{prefix}_time_{feat_name}_mean'] = round(float(np.mean(feat_values)), 12)\n",
    "            features[f'{prefix}_time_{feat_name}_std'] = round(float(np.std(feat_values)), 12)\n",
    "            features[f'{prefix}_time_{feat_name}_max'] = round(float(np.max(feat_values)), 12)\n",
    "            features[f'{prefix}_time_{feat_name}_min'] = round(float(np.min(feat_values)), 12)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in time-domain feature calculation: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f5dd3",
   "metadata": {},
   "source": [
    "### Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8dca95",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_connectivity_features(raw, fmin=0.4, fmax=50, prefix=\"seizure\"):\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        data = raw.get_data()\n",
    "        n_channels, n_samples = data.shape\n",
    "        sfreq = raw.info['sfreq']\n",
    "        \n",
    "        # Only calculate if we have multiple channels\n",
    "        if n_channels < 2:\n",
    "            raise ValueError(\"Need at least 2 channels for connectivity\")\n",
    "        \n",
    "        # Reshape data for connectivity calculation (n_epochs, n_channels, n_times)\n",
    "        # Create pseudo-epochs by segmenting the data\n",
    "        epoch_length = int(2 * sfreq)  # 2-second epochs\n",
    "        n_epochs = n_samples // epoch_length\n",
    "        \n",
    "        if n_epochs > 0:\n",
    "            epochs_data = []\n",
    "            for i in range(n_epochs):\n",
    "                start = i * epoch_length\n",
    "                end = start + epoch_length\n",
    "                epochs_data.append(data[:, start:end])\n",
    "            epochs_data = np.array(epochs_data)\n",
    "            \n",
    "            # Calculate spectral connectivity\n",
    "            # Using coherence as the connectivity measure\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_data, method='coh', mode='multitaper',\n",
    "                sfreq=sfreq, fmin=fmin, fmax=fmax,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Get connectivity matrix (n_channels x n_channels x n_freqs)\n",
    "            con_matrix = con.get_data(output='dense')\n",
    "            \n",
    "            # Average across frequencies\n",
    "            mean_connectivity = np.mean(con_matrix, axis=2)\n",
    "            print(\"Mean connectivity shape:\", mean_connectivity.shape)\n",
    "            print(\"Number of epochs: \", n_epochs)\n",
    "            \n",
    "            # Extract upper triangle (excluding diagonal)\n",
    "            upper_tri = np.triu_indices_from(mean_connectivity, k=1)\n",
    "            connectivity_values = mean_connectivity[upper_tri]\n",
    "            \n",
    "            # Store connectivity statistics\n",
    "            features[f'{prefix}_connectivity_mean'] = np.mean(connectivity_values)\n",
    "            features[f'{prefix}_connectivity_std'] = np.std(connectivity_values)\n",
    "            features[f'{prefix}_connectivity_max'] = np.max(connectivity_values)\n",
    "            features[f'{prefix}_connectivity_min'] = np.min(connectivity_values)\n",
    "            \n",
    "            # Global efficiency (mean of connectivity)\n",
    "            features[f'{prefix}_global_efficiency'] = np.mean(connectivity_values)\n",
    "            \n",
    "            # Node strength (sum of connections for each node)\n",
    "            node_strengths = np.sum(mean_connectivity, axis=0) - 1  # Subtract diagonal\n",
    "            features[f'{prefix}_node_strength_mean'] = float(np.mean(node_strengths))\n",
    "            features[f'{prefix}_node_strength_std'] = float(np.std(node_strengths))\n",
    "            features[f'{prefix}_node_strength_max'] = float(np.max(node_strengths))\n",
    "            \n",
    "            # Clustering coefficient (simplified version)\n",
    "            clustering_coeffs = []\n",
    "            for i in range(n_channels):\n",
    "                neighbors = mean_connectivity[i, :] > 0.01  # Threshold for connection\n",
    "                n_neighbors = np.sum(neighbors) - 1  # Exclude self\n",
    "                if n_neighbors > 1:\n",
    "                    # Count connections between neighbors\n",
    "                    neighbor_indices = np.where(neighbors)[0]\n",
    "                    neighbor_connections = 0\n",
    "                    for j in range(len(neighbor_indices)):\n",
    "                        for k in range(j+1, len(neighbor_indices)):\n",
    "                            if mean_connectivity[neighbor_indices[j], neighbor_indices[k]] > 0.3:\n",
    "                                neighbor_connections += 1\n",
    "                    max_connections = n_neighbors * (n_neighbors - 1) / 2\n",
    "                    if max_connections > 0:\n",
    "                        clustering = neighbor_connections / max_connections\n",
    "                        clustering_coeffs.append(clustering)\n",
    "            \n",
    "            features[f'{prefix}_clustering_coefficient'] = float(np.mean(clustering_coeffs))\n",
    "  \n",
    "        else:\n",
    "            raise ValueError(\"Not enough data for connectivity analysis\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in connectivity calculation: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5c200",
   "metadata": {},
   "source": [
    "### PAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491ad9f",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_pac_features(data, sfreq, prefix=\"seizure\"):\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        n_channels = data.shape[0]\n",
    "        \n",
    "        # Define phase and amplitude frequency bands\n",
    "        phase_bands = {\n",
    "            'theta': (4, 7),\n",
    "            'alpha': (7, 12)\n",
    "        }\n",
    "        \n",
    "        amplitude_bands = {\n",
    "            'gamma': (30, 50),\n",
    "            'high_gamma': (50, 100)\n",
    "        }\n",
    "        \n",
    "        pac_values = []\n",
    "        \n",
    "        for phase_name, (phase_low, phase_high) in phase_bands.items():\n",
    "            for amp_name, (amp_low, amp_high) in amplitude_bands.items():\n",
    "                \n",
    "                channel_pac = []\n",
    "                for ch in range(min(n_channels, 10)):  # Limit to first 10 channels for speed\n",
    "                    # Extract phase\n",
    "                    phase_filtered = mne.filter.filter_data(\n",
    "                        data[ch:ch+1, :], sfreq, \n",
    "                        l_freq=phase_low, h_freq=phase_high,\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    phase = np.angle(signal.hilbert(phase_filtered[0]))\n",
    "                    \n",
    "                    # Extract amplitude\n",
    "                    amp_filtered = mne.filter.filter_data(\n",
    "                        data[ch:ch+1, :], sfreq,\n",
    "                        l_freq=amp_low, h_freq=amp_high,\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    amplitude = np.abs(signal.hilbert(amp_filtered[0]))\n",
    "                    \n",
    "                    # Calculate PAC using Modulation Index\n",
    "                    n_bins = 18\n",
    "                    phase_bins = np.linspace(-np.pi, np.pi, n_bins + 1)\n",
    "                    amp_by_phase = []\n",
    "                    \n",
    "                    for i in range(n_bins):\n",
    "                        mask = (phase >= phase_bins[i]) & (phase < phase_bins[i+1])\n",
    "                        if np.sum(mask) > 0:\n",
    "                            amp_by_phase.append(np.mean(amplitude[mask]))\n",
    "                        else:\n",
    "                            amp_by_phase.append(0)\n",
    "                    \n",
    "                    # Normalize and calculate entropy\n",
    "                    amp_by_phase = np.array(amp_by_phase)\n",
    "                    if np.sum(amp_by_phase) > 0:\n",
    "                        amp_by_phase = amp_by_phase / np.sum(amp_by_phase)\n",
    "                        # Kullback-Leibler divergence from uniform distribution\n",
    "                        uniform = np.ones(n_bins) / n_bins\n",
    "                        kl_div = np.sum(amp_by_phase * np.log((amp_by_phase + 1e-10) / uniform))\n",
    "                        mi = kl_div / np.log(n_bins)  # Normalized MI\n",
    "                        channel_pac.append(mi)\n",
    "                    else:\n",
    "                        channel_pac.append(0.0)\n",
    "                \n",
    "                if channel_pac:\n",
    "                    pac_value = np.mean(channel_pac)\n",
    "                    features[f'{prefix}_pac_{phase_name}_{amp_name}'] = float(pac_value)\n",
    "                    pac_values.append(pac_value)\n",
    "                else:\n",
    "                    features[f'{prefix}_pac_{phase_name}_{amp_name}'] = 0.0\n",
    "        \n",
    "        # Overall PAC statistics\n",
    "        features[f'{prefix}_pac_mean'] = float(np.mean(pac_values))\n",
    "        features[f'{prefix}_pac_max'] = float(np.max(pac_values))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in PAC calculation: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c95e6",
   "metadata": {},
   "source": [
    "### Sample, Permutation, and Approximate Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec13c63",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_entropy_features(data, sfreq, prefix=\"seizures\"):\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        n_channels = data.shape[0]\n",
    "        \n",
    "        # Sample Entropy\n",
    "        sample_entropies = []\n",
    "        for ch in range(min(n_channels, 10)): \n",
    "            # Downsample for faster computation\n",
    "            downsampled = signal.resample(data[ch, :], min(1000, len(data[ch, :])))\n",
    "            # Simple sample entropy approximation\n",
    "            se = calculate_sample_entropy(downsampled, m=2, r=0.2*np.std(downsampled))\n",
    "            sample_entropies.append(se)\n",
    "        \n",
    "        features[f'{prefix}_sample_entropy_mean'] = float(np.mean(sample_entropies))\n",
    "        features[f'{prefix}_sample_entropy_std'] = float(np.std(sample_entropies))\n",
    "        \n",
    "        # Permutation Entropy\n",
    "        perm_entropies = []\n",
    "        for ch in range(min(n_channels, 10)):\n",
    "            pe = calculate_permutation_entropy(data[ch, :], order=3, delay=1)\n",
    "            perm_entropies.append(pe)\n",
    "        \n",
    "        features[f'{prefix}_permutation_entropy_mean'] = float(np.mean(perm_entropies))\n",
    "        features[f'{prefix}_permutation_entropy_std'] = float(np.std(perm_entropies))\n",
    "        \n",
    "        # Approximate Entropy (simplified)\n",
    "        approx_entropies = []\n",
    "        for ch in range(min(n_channels, 10)):\n",
    "            downsampled = signal.resample(data[ch, :], min(1000, len(data[ch, :])))\n",
    "            ae = calculate_approx_entropy(downsampled, m=2, r=0.2*np.std(downsampled))\n",
    "            approx_entropies.append(ae)\n",
    "        \n",
    "        features[f'{prefix}_approx_entropy_mean'] = float(np.mean(approx_entropies))\n",
    "        features[f'{prefix}_approx_entropy_std'] = float(np.std(approx_entropies))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in entropy calculation: {e}\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "def calculate_sample_entropy(signal_data, m=2, r=0.2):\n",
    "    \"\"\"Calculate sample entropy\"\"\"\n",
    "    N = len(signal_data)\n",
    "    \n",
    "    def _maxdist(x_i, x_j, m):\n",
    "        return max([abs(ua - va) for ua, va in zip(x_i, x_j)])\n",
    "    \n",
    "    def _phi(m):\n",
    "        patterns = np.array([signal_data[i:i+m] for i in range(N - m + 1)])\n",
    "        C = 0\n",
    "        for i in range(len(patterns)):\n",
    "            for j in range(i+1, len(patterns)):\n",
    "                if _maxdist(patterns[i], patterns[j], m) <= r:\n",
    "                    C += 1\n",
    "        return C\n",
    "    \n",
    "    try:\n",
    "        phi_m = _phi(m)\n",
    "        phi_m1 = _phi(m + 1)\n",
    "        \n",
    "        if phi_m == 0:\n",
    "            return 0\n",
    "        return -np.log(phi_m1 / phi_m) if phi_m1 > 0 else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def calculate_permutation_entropy(signal_data, order=3, delay=1):\n",
    "    \"\"\"Calculate permutation entropy\"\"\"\n",
    "    try:\n",
    "        n = len(signal_data)\n",
    "        permutations = []\n",
    "        \n",
    "        for i in range(n - delay * (order - 1)):\n",
    "            indices = [i + j * delay for j in range(order)]\n",
    "            sorted_indices = np.argsort(signal_data[indices])\n",
    "            permutations.append(tuple(sorted_indices))\n",
    "        \n",
    "        # Count occurrences\n",
    "        unique, counts = np.unique(permutations, axis=0, return_counts=True)\n",
    "        probs = counts / len(permutations)\n",
    "        \n",
    "        # Calculate entropy\n",
    "        return -np.sum(probs * np.log(probs + 1e-8))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def calculate_approx_entropy(signal_data, m=2, r=0.2):\n",
    "    \"\"\"Calculate approximate entropy\"\"\"\n",
    "    try:\n",
    "        N = len(signal_data)\n",
    "        \n",
    "        def _maxdist(x_i, x_j, m):\n",
    "            return max([abs(ua - va) for ua, va in zip(x_i, x_j)])\n",
    "        \n",
    "        def _phi(m):\n",
    "            patterns = np.array([signal_data[i:i+m] for i in range(N - m + 1)])\n",
    "            C = np.zeros(N - m + 1)\n",
    "            \n",
    "            for i in range(N - m + 1):\n",
    "                matches = 0\n",
    "                for j in range(N - m + 1):\n",
    "                    if _maxdist(patterns[i], patterns[j], m) <= r:\n",
    "                        matches += 1\n",
    "                C[i] = matches / (N - m + 1)\n",
    "            \n",
    "            return np.mean(np.log(C + 1e-8))\n",
    "        \n",
    "        return _phi(m) - _phi(m + 1)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4b881b",
   "metadata": {},
   "source": [
    "### Rhythmic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49105267",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_rhythmic_features(data, sfreq, prefix=\"seizure\"):\n",
    "    \"\"\"Extract rhythmic features for seizure patterns\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    def bandpass_filter(channel_data, lowcut, highcut, fs):\n",
    "        \"\"\"Apply bandpass filter\"\"\"\n",
    "        nyquist = fs / 2\n",
    "        low = lowcut / nyquist\n",
    "        high = highcut / nyquist\n",
    "        b, a = butter(4, [low, high], btype='band')\n",
    "        return filtfilt(b, a, channel_data)\n",
    "    \n",
    "    n_channels = data.shape[0]\n",
    "    \n",
    "    # Initialize arrays for each feature\n",
    "    theta_powers = []\n",
    "    delta_slow_powers = []\n",
    "    fast_powers = []\n",
    "    spike_rates = []\n",
    "    \n",
    "    for channel_idx in range(n_channels):\n",
    "        channel = data[channel_idx, :]\n",
    "        \n",
    "        # Theta rhythm (5-7 Hz)\n",
    "        theta_filtered = bandpass_filter(channel, 5, 7, sfreq)\n",
    "        theta_power = np.mean(theta_filtered**2)\n",
    "        theta_powers.append(theta_power)\n",
    "        \n",
    "        # Delta slow rhythm (2-5 Hz)\n",
    "        delta_slow_filtered = bandpass_filter(channel, 2, 5, sfreq)\n",
    "        delta_slow_power = np.mean(delta_slow_filtered**2)\n",
    "        delta_slow_powers.append(delta_slow_power)\n",
    "        \n",
    "        # Fast activity (15-25 Hz)\n",
    "        fast_filtered = bandpass_filter(channel, 15, 25, sfreq)\n",
    "        fast_power = np.mean(fast_filtered**2)\n",
    "        fast_powers.append(fast_power)\n",
    "        \n",
    "        # Spike detection\n",
    "        threshold = 3 * np.std(channel)\n",
    "        spikes = np.where(np.abs(channel) > threshold)[0]\n",
    "        spike_rate = len(spikes) / (len(channel) / sfreq)\n",
    "        spike_rates.append(spike_rate)\n",
    "    \n",
    "    print('Storing features...')\n",
    "    # Store features\n",
    "    features[f'{prefix}_rhythmic_theta_power_mean'] = float(np.mean(theta_powers) * 1e+8)\n",
    "    features[f'{prefix}_rhythmic_theta_power_std'] = float(np.std(theta_powers) * 1e+8)\n",
    "    features[f'{prefix}_rhythmic_delta_slow_power_mean'] = float(np.mean(delta_slow_powers) * 1e+8)\n",
    "    features[f'{prefix}_rhythmic_delta_slow_power_std'] = float(np.std(delta_slow_powers) * 1e+8)\n",
    "    features[f'{prefix}_rhythmic_fast_power_mean'] = float(np.mean(fast_powers) * 1e+8)\n",
    "    features[f'{prefix}_rhythmic_fast_power_std'] = float(np.std(fast_powers) * 1e+8)\n",
    "    features[f'{prefix}_rhythmic_spike_rate_mean'] = float(np.mean(spike_rates))\n",
    "    features[f'{prefix}_rhythmic_spike_rate_std'] = float(np.std(spike_rates))\n",
    "    \n",
    "    # Ratios\n",
    "    features[f'{prefix}_rhythmic_theta_delta_ratio'] = float(np.mean(theta_powers) / (np.mean(delta_slow_powers) + 1e-8))\n",
    "    features[f'{prefix}_rhythmic_fast_theta_ratio'] = float(np.mean(fast_powers) / (np.mean(theta_powers) + 1e-8))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0097497",
   "metadata": {},
   "source": [
    "## Process Single EDF File Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a972b",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_single_edf(\n",
    "                    edf_path, \n",
    "                    seizure_info, \n",
    "                    registration_start,\n",
    "                    registration_end,\n",
    "                    seizure_start,\n",
    "                    seizure_end,\n",
    "                    seizure_start_seconds,\n",
    "                    seizure_end_seconds,\n",
    "                    onset_sec,\n",
    "                    postictal_onset_sec,\n",
    "                    registration_end_seconds,\n",
    "                    postictal_end_sec):\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Load EDF file\n",
    "        print('Loading edf file...')\n",
    "        raw = mne.io.read_raw_edf(str(edf_path), preload=True, verbose=False)\n",
    "        \n",
    "        # Basic file info\n",
    "        features['file_path'] = str(edf_path)\n",
    "        features['num_channels'] = len(raw.ch_names)\n",
    "        features['sampling_rate'] = raw.info['sfreq']\n",
    "        features['duration_seconds'] = raw.n_times / raw.info['sfreq']\n",
    "\n",
    "        # Add seizure timing info\n",
    "        features['seizure_start_seconds'] = seizure_start_seconds\n",
    "        features['seizure_end_seconds'] = seizure_end_seconds\n",
    "        \n",
    "        # channels available\n",
    "        available_channels = raw.ch_names\n",
    "        print(f'Channels available: {available_channels}')\n",
    "        \n",
    "        # Apply bandpass filter\n",
    "        raw.filter(0.4, 50, fir_design='firwin', verbose=False)\n",
    "\n",
    "        # Get data\n",
    "        sfreq = raw.info['sfreq']\n",
    "        \n",
    "        # Create raw seizure section\n",
    "        seizure_raw = raw.copy().crop(tmin=onset_sec, tmax=postictal_onset_sec)\n",
    "        seizure_data = seizure_raw.get_data()\n",
    "\n",
    "        # Feature extraction functions\n",
    "        de_features = calculate_differential_entropy(seizure_data, sfreq)\n",
    "        features.update(de_features)\n",
    "\n",
    "        entropy_features = calculate_entropy_features(seizure_data, sfreq)\n",
    "        features.update(entropy_features)\n",
    "        \n",
    "        psd_features = extract_psd_features(seizure_raw)\n",
    "        features.update(psd_features)\n",
    "        \n",
    "        psd_regional_features = extract_psd_features_by_region(seizure_raw)\n",
    "        features.update(psd_regional_features)\n",
    "        \n",
    "        wt_features = calculate_wavelet_features(seizure_data, sfreq)\n",
    "        features.update(wt_features)\n",
    "        \n",
    "        time_features = calculate_time_domain_features(seizure_data, sfreq)\n",
    "        features.update(time_features)\n",
    "        \n",
    "        connectivity_features = calculate_connectivity_features(seizure_raw)\n",
    "        features.update(connectivity_features)\n",
    "        \n",
    "        pac_features = calculate_pac_features(seizure_data, sfreq)\n",
    "        features.update(pac_features)\n",
    "    \n",
    "        rhythmic_features = calculate_rhythmic_features(seizure_data, sfreq)\n",
    "        features.update(rhythmic_features)\n",
    "\n",
    "        # Propagation features\n",
    "        propagation_features = calculate_simple_propagation_features(seizure_raw, onset_sec, postictal_onset_sec)\n",
    "        features.update(propagation_features) \n",
    "\n",
    "        # Create raw seizure section\n",
    "        post_ictal_raw = raw.copy().crop(tmin=postictal_onset_sec, tmax=postictal_end_sec)\n",
    "        post_ictal_data = post_ictal_raw.get_data()\n",
    "\n",
    "        post_ictal_psd_features = extract_psd_features(post_ictal_raw, prefix=\"post_ictal\")\n",
    "        features.update(post_ictal_psd_features)\n",
    "        \n",
    "        post_ictal_psd_regional_features = extract_psd_features_by_region(post_ictal_raw, prefix=\"post_ictal\")\n",
    "        features.update(post_ictal_psd_regional_features)\n",
    "        \n",
    "        post_ictal_wt_features = calculate_wavelet_features(post_ictal_data, sfreq, prefix=\"post_ictal\")\n",
    "        features.update(post_ictal_wt_features)\n",
    "        \n",
    "        # Extract comprehensive features\n",
    "        #comprehensive_features = extract_comprehensive_features(raw, seizure_start, seizure_end)\n",
    "        #features.update(comprehensive_features)\n",
    "        \n",
    "        features['processing_success'] = True\n",
    "        features['error_message'] = ''\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {edf_path}: {e}\")\n",
    "        features['processing_success'] = False\n",
    "        features['error_message'] = str(e)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5645b995",
   "metadata": {},
   "source": [
    "## Build Comprehensive Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14cc12e",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_comprehensive_dataset(seizures_df, patients_df, non_seizures_df, \n",
    "                              data_root_paths=['data/siena_scalp', 'data/chb-mit']):\n",
    "    all_records = []\n",
    "    \n",
    "    # Convert Polars to pandas for iteration (if needed)\n",
    "    if isinstance(seizures_df, pl.DataFrame):\n",
    "        seizures_pd = seizures_df.to_pandas()\n",
    "        patients_pd = patients_df.to_pandas()\n",
    "        non_seizures_pd = non_seizures_df.to_pandas()\n",
    "    else:\n",
    "        seizures_pd = seizures_df\n",
    "        patients_pd = patients_df\n",
    "        non_seizures_pd = non_seizures_df\n",
    "    \n",
    "    print(f\"Processing {len(seizures_pd)} seizure records...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # options for testing\n",
    "    # for idx, seizure_row in tqdm(seizures_pd.iloc[:5].iterrows(), total=5):\n",
    "    # for idx, seizure_row in tqdm(seizures_pd.iterrows(), total=len(seizures_pd)): \n",
    "    \n",
    "    for idx, seizure_row in tqdm(seizures_pd.iterrows(), total=len(seizures_pd)): \n",
    "        record = {}\n",
    "        \n",
    "        # Add all seizure metadata\n",
    "        for col in seizure_row.index:\n",
    "            record[f'{col}'] = seizure_row[col]\n",
    "        \n",
    "        # Find corresponding patient and seizure info\n",
    "        patient_id = seizure_row['patient_id']\n",
    "        patient_info = patients_pd[patients_pd['patient_id'] == patient_id]\n",
    "        registration_start = seizure_row['registration_start_time']\n",
    "        registration_end = seizure_row['registration_end_time']\n",
    "        seizure_start = seizure_row['seizure_start_time']\n",
    "        seizure_end = seizure_row['seizure_end_time']\n",
    "\n",
    "        # Calculate registration and seizure total seconds\n",
    "        registration_start_seconds = registration_start.hour * 3600 + registration_start.minute * 60 + registration_start.second\n",
    "        registration_end_seconds = registration_end.hour * 3600 + registration_end.minute * 60 + registration_end.second\n",
    "        seizure_start_seconds = seizure_start.hour * 3600 + seizure_start.minute * 60 + seizure_start.second\n",
    "        seizure_end_seconds = seizure_end.hour * 3600 + seizure_end.minute * 60 + seizure_end.second\n",
    "        seizure_duration = (seizure_end_seconds - seizure_start_seconds)\n",
    "        edf_file_duration = (registration_end_seconds - registration_start_seconds)\n",
    "\n",
    "        if seizure_start_seconds < registration_start_seconds:\n",
    "            reg_adjusted = (86400 - registration_start_seconds)\n",
    "            onset_sec = (seizure_start_seconds + reg_adjusted)\n",
    "            postictal_onset_sec = (seizure_end_seconds + reg_adjusted)\n",
    "        else:\n",
    "            onset_sec = (seizure_start_seconds - registration_start_seconds)\n",
    "            postictal_onset_sec = (seizure_end_seconds - registration_start_seconds)\n",
    "\n",
    "        postictal_end_sec = (edf_file_duration - 1)\n",
    "\n",
    "        # Find EDF file\n",
    "        file_name = seizure_row['file_name']\n",
    "        edf_path = None\n",
    "        \n",
    "        print(f'Patient id: {patient_id}')\n",
    "        print(f'Edf file: {file_name}')\n",
    "        print(f'Registration start: {registration_start}')\n",
    "        print(f'Registration end: {registration_end}')\n",
    "        print(f'Registration start seconds: {registration_start_seconds}')\n",
    "        print(f'Registration end seconds: {registration_end_seconds}')\n",
    "        print(f'EDF file duration: {edf_file_duration}')\n",
    "        print(f'Seizure start: {seizure_start}')\n",
    "        print(f'Seizure end: {seizure_end}')\n",
    "        print(f'Seizure start seconds: {seizure_start_seconds}')\n",
    "        print(f'Seizure end seconds: {seizure_end_seconds}')\n",
    "        print(f'Seizure duration: {seizure_duration}')\n",
    "        print(f'Seizure onset seconds: {onset_sec}')\n",
    "        print(f'Post-ictal onset seconds: {postictal_onset_sec}')\n",
    "        \n",
    "        if not patient_info.empty:\n",
    "            for col in patient_info.columns:\n",
    "                if col != 'patient_id':  # Avoid duplication\n",
    "                    record[f'{col}'] = patient_info.iloc[0][col]\n",
    "        \n",
    "        for root_path in data_root_paths:\n",
    "            possible_paths = [\n",
    "                os.path.join(root_path, patient_id, file_name),\n",
    "                os.path.join(root_path, patient_id.lower(), file_name),\n",
    "                os.path.join(root_path, patient_id.upper(), file_name),\n",
    "            ]\n",
    "            \n",
    "            for path in possible_paths:\n",
    "                if os.path.exists(path):\n",
    "                    edf_path = path\n",
    "                    break\n",
    "            \n",
    "            if edf_path:\n",
    "                break\n",
    "        \n",
    "        if edf_path:\n",
    "            # Process EDF and extract features - pass the entire row as dict\n",
    "            edf_features = process_single_edf(\n",
    "                                    edf_path, \n",
    "                                    seizure_row.to_dict(),\n",
    "                                    registration_start,\n",
    "                                    registration_end,\n",
    "                                    seizure_start,\n",
    "                                    seizure_end,\n",
    "                                    seizure_start_seconds,\n",
    "                                    seizure_end_seconds,\n",
    "                                    onset_sec,\n",
    "                                    postictal_onset_sec,\n",
    "                                    registration_end_seconds,\n",
    "                                    postictal_end_sec\n",
    "                                    )\n",
    "            record.update(edf_features)\n",
    "        else:\n",
    "            record['processing_success'] = False\n",
    "            record['error_message'] = 'EDF file not found'\n",
    "        \n",
    "        all_records.append(record)\n",
    "    \n",
    "    # Create comprehensive dataframe using Polars\n",
    "    comprehensive_df = pl.DataFrame(all_records)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total records: {len(comprehensive_df)}\")\n",
    "    \n",
    "    if 'processing_success' in comprehensive_df.columns:\n",
    "        success_count = comprehensive_df.filter(pl.col('processing_success')).height\n",
    "        print(f\"Successfully processed: {success_count}\")\n",
    "        print(f\"Failed: {len(comprehensive_df) - success_count}\")\n",
    "    \n",
    "    return comprehensive_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91625555",
   "metadata": {},
   "source": [
    "## Fill in Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9472fa9d",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fill_missing_values_polars(df):\n",
    "    # Get column types\n",
    "    str_cols = [col for col in df.columns if df[col].dtype == pl.Utf8]\n",
    "    num_cols = [col for col in df.columns if df[col].dtype in [pl.Float32, pl.Float64, pl.Int8, pl.Int16, pl.Int32, pl.Int64]]\n",
    "    \n",
    "    # Fill string columns with \"N/A\"\n",
    "    for col in str_cols:\n",
    "        df = df.with_columns(pl.col(col).fill_null(\"N/A\"))\n",
    "    \n",
    "    # Fill numeric columns with 0\n",
    "    for col in num_cols:\n",
    "        df = df.with_columns(\n",
    "            pl.col(col).fill_null(0).fill_nan(0)\n",
    "        )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a783dd6",
   "metadata": {},
   "source": [
    "## Main Usage Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ba49f",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build comprehensive dataset\n",
    "comprehensive_df = build_comprehensive_dataset(\n",
    "    seizures_df, \n",
    "    patients_df,\n",
    "    non_seizures_df,\n",
    "    data_root_paths=[\n",
    "        'data/siena_scalp', \n",
    "        'data/chb-mit',\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c1b8d",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "print(\"\\nFilling missing values...\")\n",
    "comprehensive_df = fill_missing_values_polars(comprehensive_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c4ad6e-7275-4754-8714-7fd6974be21b",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "comprehensive_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc4a3ec",
   "metadata": {},
   "source": [
    "## Export Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1fb15",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save to Parquet\n",
    "print(\"\\nSaving results...\")\n",
    "save_to_parquet(comprehensive_df, 'comprehensive_eeg_features.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28605b3d",
   "metadata": {},
   "source": [
    "## Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab25b09",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load processed data set\n",
    "processed_df = pl.read_parquet(\"comprehensive_eeg_features.parquet\")\n",
    "\n",
    "print(f\"Shape: {processed_df.shape}\")\n",
    "print(f\"Columns: {processed_df.columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e460c12e",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get column names and dtypes\n",
    "for col, dtype in zip(processed_df.columns, processed_df.dtypes):\n",
    "    print(f\"{col}: {dtype}\")\n",
    "\n",
    "# Alternative: as a list of tuples\n",
    "column_info = [(col, dtype) for col, dtype in zip(processed_df.columns, processed_df.dtypes)]\n",
    "\n",
    "# Alternative: as a dictionary\n",
    "column_dtypes = dict(zip(processed_df.columns, processed_df.dtypes))\n",
    "\n",
    "# Display schema (formatted output)\n",
    "print(processed_df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0867d9",
   "metadata": {},
   "source": [
    "## Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01489a2",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def encode_categoricals(df, output_path='processed_data'):\n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Copy dataframe to avoid modifying original\n",
    "    encoded_df = df.clone()\n",
    "    \n",
    "    # Dictionary to store encoding mappings\n",
    "    encoding_mappings = {}\n",
    "    \n",
    "    # Process each column\n",
    "    for col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "        \n",
    "        # Check if column is string/object type\n",
    "        if dtype == pl.Utf8 or dtype == pl.Object:\n",
    "            # Get unique values and create mapping (starting from 1)\n",
    "            unique_vals = encoded_df[col].unique().drop_nulls().sort()\n",
    "            mapping = {val: i+1 for i, val in enumerate(unique_vals)}\n",
    "            encoding_mappings[col] = mapping\n",
    "            \n",
    "            # Apply the mapping directly\n",
    "            encoded_df = encoded_df.with_columns(\n",
    "                pl.col(col).replace(mapping).alias(col)\n",
    "            )\n",
    "            \n",
    "    # Save mappings to JSON\n",
    "    json_path = Path(output_path) / 'categorical_encodings.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(encoding_mappings, f, indent=2)\n",
    "    \n",
    "    return encoded_df, encoding_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34a27c",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoded_df, encoding_mappings = encode_categoricals(processed_df, 'processed_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf2acf",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6819b03",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the plot output path\n",
    "plot_output_path = Path('plot_outputs')\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "plot_output_path.mkdir(exist_ok=True)\n",
    "print(f\"Directory '{plot_output_path}' is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d5a35",
   "metadata": {},
   "source": [
    "## Plot Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ea82c",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_numerical_distributions(df, output_path=\"plot_outputs\"):\n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get numerical columns\n",
    "    numerical_cols = [col for col in df.columns \n",
    "                     if df[col].dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "                                          pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "                                          pl.Float32, pl.Float64]]\n",
    "    \n",
    "    print(f\"Found {len(numerical_cols)} numerical columns to plot\")\n",
    "    \n",
    "    # Plot each numerical column\n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        # Get the data for this column (remove nulls)\n",
    "        data = df[col].drop_nulls().to_numpy()\n",
    "        \n",
    "        if len(data) > 0:\n",
    "            # Create histogram\n",
    "            ax.hist(data, bins='auto', alpha=0.7, color='steelblue', edgecolor='black')\n",
    "            \n",
    "            # Add labels and title\n",
    "            ax.set_xlabel(col, fontsize=12)\n",
    "            ax.set_ylabel('Frequency', fontsize=12)\n",
    "            ax.set_title(f'Distribution of {col}', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # Add grid\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add statistics text box\n",
    "            stats_text = (f'Mean: {np.mean(data):.2e}\\n'\n",
    "                         f'Std: {np.std(data):.2e}\\n'\n",
    "                         f'Min: {np.min(data):.2e}\\n'\n",
    "                         f'Max: {np.max(data):.2e}')\n",
    "            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, \n",
    "                    verticalalignment='top', \n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "            \n",
    "            # Save the figure\n",
    "            plt.tight_layout()\n",
    "            save_path = Path(output_path) / f'{col}_distribution.png'\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Plotted {i + 1}/{len(numerical_cols)} columns\")\n",
    "        else:\n",
    "            print(f\"Skipped {col} - no non-null values\")\n",
    "            plt.close()\n",
    "    \n",
    "    print(f\"Completed! All {len(numerical_cols)} plots saved to '{output_path}' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a245ad",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_numerical_distributions(processed_df, \"plot_outputs/processed_data_distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a42e7d",
   "metadata": {},
   "source": [
    "## Plot Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517cfc3",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_categorical_distributions(df, output_path=\"plot_outputs\", max_categories=50):\n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get categorical, boolean, and string columns\n",
    "    categorical_cols = [col for col in df.columns \n",
    "                       if df[col].dtype in [pl.Utf8, pl.Boolean, pl.Categorical]]\n",
    "    \n",
    "    print(f\"Found {len(categorical_cols)} categorical/boolean/object columns to plot\")\n",
    "    \n",
    "    # Plot each categorical column\n",
    "    for i, col in enumerate(categorical_cols):\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Calculate value counts\n",
    "        value_counts = df[col].value_counts().sort(by='count', descending=True)\n",
    "        \n",
    "        # Handle null counts\n",
    "        null_count = df[col].null_count()\n",
    "        total_count = len(df)\n",
    "        \n",
    "        # Limit to max_categories if necessary\n",
    "        if len(value_counts) > max_categories:\n",
    "            value_counts = value_counts.head(max_categories)\n",
    "            truncated = True\n",
    "        else:\n",
    "            truncated = False\n",
    "        \n",
    "        if len(value_counts) > 0:\n",
    "            # Extract categories and counts\n",
    "            categories = value_counts[col].to_list()\n",
    "            counts = value_counts['count'].to_list()\n",
    "            \n",
    "            # Convert None to 'NULL' for display\n",
    "            categories = ['NULL' if cat is None else str(cat) for cat in categories]\n",
    "            \n",
    "            # Create bar plot\n",
    "            bars = ax.bar(range(len(categories)), counts, color='steelblue', \n",
    "                          alpha=0.7, edgecolor='black')\n",
    "            \n",
    "            # Customize x-axis\n",
    "            ax.set_xticks(range(len(categories)))\n",
    "            ax.set_xticklabels(categories)\n",
    "            \n",
    "            # Rotate labels if there are many categories\n",
    "            if len(categories) > 10:\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "            elif len(categories) > 5:\n",
    "                plt.xticks(rotation=30, ha='right')\n",
    "            \n",
    "            # Truncate long labels\n",
    "            current_labels = ax.get_xticklabels()\n",
    "            new_labels = []\n",
    "            for label in current_labels:\n",
    "                text = label.get_text()\n",
    "                if len(text) > 30:\n",
    "                    text = text[:27] + '...'\n",
    "                new_labels.append(text)\n",
    "            ax.set_xticklabels(new_labels)\n",
    "            \n",
    "            # Add value labels on top of bars\n",
    "            for bar, count in zip(bars, counts):\n",
    "                height = bar.get_height()\n",
    "                percentage = (count / total_count) * 100\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{count}\\n({percentage:.1f}%)',\n",
    "                       ha='center', va='bottom', fontsize=8)\n",
    "            \n",
    "            # Add labels and title\n",
    "            ax.set_xlabel(col, fontsize=12)\n",
    "            ax.set_ylabel('Frequency', fontsize=12)\n",
    "            title = f'Distribution of {col}'\n",
    "            if truncated:\n",
    "                title += f' (Top {max_categories} categories)'\n",
    "            ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # Add grid\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Add statistics text box\n",
    "            unique_count = df[col].n_unique()\n",
    "            stats_text = (f'Unique values: {unique_count}\\n'\n",
    "                         f'Total records: {total_count}\\n'\n",
    "                         f'Null values: {null_count} ({(null_count/total_count)*100:.1f}%)')\n",
    "            if truncated:\n",
    "                stats_text += f'\\n(Showing top {max_categories} only)'\n",
    "            \n",
    "            ax.text(0.98, 0.98, stats_text, transform=ax.transAxes, \n",
    "                    verticalalignment='top', horizontalalignment='right',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "            \n",
    "            # Adjust layout to prevent label cutoff\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save the figure\n",
    "            save_path = Path(output_path) / f'{col}_distribution.png'\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"Plotted {i + 1}/{len(categorical_cols)} columns\")\n",
    "        else:\n",
    "            print(f\"Skipped {col} - no data to plot\")\n",
    "            plt.close()\n",
    "    \n",
    "    print(f\"Completed! All {len(categorical_cols)} plots saved to '{output_path}' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a815",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_categorical_distributions(processed_df, \"plot_outputs/processed_data_distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53943fd7",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4e4f78",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(df, output_path):\n",
    "    # Feature lists - empty for manual configuration\n",
    "    features_to_remove = []\n",
    "    features_to_keep = []\n",
    "    key_features = []\n",
    "    target_variables = []\n",
    "    \n",
    "    # Combine features and targets\n",
    "    features_to_analyze = key_features + target_variables\n",
    "    \n",
    "    if len(features_to_analyze) > 1:\n",
    "        plt.figure(figsize=(24, 24))\n",
    "        \n",
    "        # Calculate correlation matrix directly in polars\n",
    "        correlation_matrix = df.select(features_to_analyze).corr()\n",
    "        \n",
    "        # Convert just the correlation matrix to numpy for plotting\n",
    "        corr_array = correlation_matrix.to_numpy()\n",
    "        \n",
    "        # Create mask to highlight target correlations\n",
    "        num_targets = len(target_variables)\n",
    "        mask = np.zeros_like(corr_array)\n",
    "        if num_targets > 0:\n",
    "            mask[:-num_targets, :-num_targets] = True\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(corr_array, \n",
    "                   annot=True, \n",
    "                   cmap='coolwarm', \n",
    "                   center=0,\n",
    "                   fmt='.2f',\n",
    "                   mask=mask,\n",
    "                   vmin=-1, \n",
    "                   vmax=1,\n",
    "                   xticklabels=features_to_analyze,\n",
    "                   yticklabels=features_to_analyze)\n",
    "        \n",
    "        plt.title('Feature-Target Correlations')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_path}/correlation_matrix.png', bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        # Print sorted correlations with targets\n",
    "        if len(target_variables) > 0:\n",
    "            print(\"\\nCorrelations with targets (sorted):\")\n",
    "            for i, target in enumerate(target_variables):\n",
    "                target_idx = features_to_analyze.index(target)\n",
    "                correlations = [(features_to_analyze[j], corr_array[target_idx, j]) \n",
    "                               for j in range(len(features_to_analyze)) if j != target_idx]\n",
    "                correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "                \n",
    "                print(f\"\\n{target} correlations:\")\n",
    "                for feat, corr in correlations:\n",
    "                    print(f\"  {feat}: {corr:.3f}\")\n",
    "\n",
    "    return features_to_keep, features_to_remove, key_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e8859a",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "features_to_keep, features_to_remove, key_features = plot_correlation_matrix(processed_df, plot_output_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8168099,
     "sourceId": 12997471,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
