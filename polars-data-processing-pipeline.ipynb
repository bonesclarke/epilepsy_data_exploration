{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65183a8e",
   "metadata": {},
   "source": [
    "# Epilepsy Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043b8dc",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78b7c0",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Epilepsy Data Processing Pipeline\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pywt\n",
    "import warnings\n",
    "import uuid\n",
    "import base64\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MNE Libraries\n",
    "import mne\n",
    "from mne import Epochs, pick_types, events_from_annotations\n",
    "from mne.channels import make_standard_montage\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.datasets import eegbci\n",
    "from mne.decoding import CSP\n",
    "from mne.preprocessing import ICA, create_eog_epochs, create_ecg_epochs\n",
    "from mne_connectivity import spectral_connectivity_epochs\n",
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "# Scipy and Scikit-learn\n",
    "from scipy import signal, stats\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b0576",
   "metadata": {},
   "source": [
    "## Load Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ad4e1",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_metadata_from_json(data_dir='data'):\n",
    "    \"\"\"Load patient and seizure metadata from JSON files using Polars\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    all_seizures = []\n",
    "    all_patients = []\n",
    "    all_non_seizures = []\n",
    "    \n",
    "    json_files = sorted(list(data_dir.glob('**/*.json')))\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract patient-level information\n",
    "            patient_info = {\n",
    "                'patient_id': data['patient_id'],\n",
    "                'age': data['age'],\n",
    "                'gender': data['gender'],\n",
    "                'sampling_rate_hz': data['sampling_rate_hz'],\n",
    "                'num_channels': len(data['channels']),\n",
    "                'json_file_path': str(json_file),\n",
    "                'eeg_channel': data['eeg_channel'],\n",
    "                'seizure_type': data['seizure_type'],\n",
    "                'localization': data['localization'],\n",
    "                'lateralization': data['lateralization'],\n",
    "            }\n",
    "            \n",
    "            if 'file_name' in data:\n",
    "                patient_info['file_name'] = data['file_name']\n",
    "                patient_info['registration_start_time'] = data.get('registration_start_time')\n",
    "                patient_info['registration_end_time'] = data.get('registration_end_time')\n",
    "            \n",
    "            all_patients.append(patient_info)\n",
    "            \n",
    "            # Process each seizure\n",
    "            for seizure in data['seizures']:\n",
    "                seizure_record = {\n",
    "                    'seizure_index': str(uuid.uuid4())[:8],\n",
    "                    'patient_id': data['patient_id'],\n",
    "                    'seizure_bool': True,\n",
    "                    'sampling_rate_hz': data['sampling_rate_hz'],\n",
    "                    'seizure_number': seizure['seizure_number']\n",
    "                }\n",
    "                \n",
    "                for key, value in seizure.items():\n",
    "                    seizure_record[key] = value\n",
    "                \n",
    "                if 'file_name' in patient_info and 'file_name' not in seizure:\n",
    "                    seizure_record['file_name'] = patient_info['file_name']\n",
    "                    if 'registration_start_time' in patient_info:\n",
    "                        seizure_record['registration_start_time'] = patient_info['registration_start_time']\n",
    "                        seizure_record['registration_end_time'] = patient_info['registration_end_time']\n",
    "                \n",
    "                all_seizures.append(seizure_record)\n",
    "                \n",
    "            # Check if non_seizures exists before processing\n",
    "            if 'non-seizures' in data:\n",
    "                for non_seizure in data['non-seizures']:\n",
    "                    non_seizure_record = {\n",
    "                        'patient_id': data['patient_id'],\n",
    "                        'seizure_bool': False,\n",
    "                        'file_name': non_seizure['file_name'],\n",
    "                        'registration_start_time': non_seizure['registration_start_time'],\n",
    "                        'registration_end_time': non_seizure['registration_end_time']\n",
    "                    }\n",
    "                    \n",
    "                    all_non_seizures.append(non_seizure_record)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to Polars DataFrames\n",
    "    seizures_df = pl.DataFrame(all_seizures)\n",
    "    patients_df = pl.DataFrame(all_patients)\n",
    "    non_seizures_df = pl.DataFrame(all_non_seizures)\n",
    "    \n",
    "    return seizures_df, patients_df, non_seizures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb5731",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "print(\"Loading metadata from JSON files...\")\n",
    "\n",
    "# change to other folder for full set\n",
    "seizures_df, patients_df, non_seizures_df = load_metadata_from_json(\n",
    "    'data'\n",
    "    )\n",
    "print(f\"Loaded {len(patients_df)} patients and {len(seizures_df)} seizures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ba1b6",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fix_time_columns(df, time_cols):\n",
    "    for col in time_cols:\n",
    "        if col in df.columns:\n",
    "            # First replace \"N/A\" with null\n",
    "            df = df.with_columns(\n",
    "                pl.when(pl.col(col) == \"N/A\")\n",
    "                .then(None)\n",
    "                .otherwise(pl.col(col))\n",
    "                .alias(col)\n",
    "            )\n",
    "            \n",
    "            # Replace literal '.' with ':' for non-null values\n",
    "            df = df.with_columns(\n",
    "                pl.when(pl.col(col).is_not_null())\n",
    "                .then(pl.col(col).str.replace_all(\".\", \":\", literal=True))\n",
    "                .otherwise(None)\n",
    "                .alias(col)\n",
    "            )\n",
    "            \n",
    "            # Split time string and wrap hours > 24 for non-null values\n",
    "            df = df.with_columns(\n",
    "                pl.when(pl.col(col).is_not_null())\n",
    "                .then(pl.col(col).str.split(\":\").list.to_struct()\n",
    "                      .struct.rename_fields([\"hour\", \"minute\", \"second\"]))\n",
    "                .otherwise(None)\n",
    "                .alias(col + \"_split\")\n",
    "            )\n",
    "            \n",
    "            # Reconstruct time with wrapped hours for non-null values\n",
    "            df = df.with_columns(\n",
    "                pl.when(pl.col(col + \"_split\").is_not_null())\n",
    "                .then(\n",
    "                    pl.concat_str(\n",
    "                        [\n",
    "                            (pl.col(col + \"_split\").struct.field(\"hour\").cast(pl.Int32) % 24).cast(pl.Utf8).str.zfill(2),\n",
    "                            pl.col(col + \"_split\").struct.field(\"minute\").cast(pl.Utf8).str.zfill(2),\n",
    "                            pl.col(col + \"_split\").struct.field(\"second\").cast(pl.Utf8).str.zfill(2)\n",
    "                        ],\n",
    "                        separator=\":\"\n",
    "                    )\n",
    "                )\n",
    "                .otherwise(None)\n",
    "                .alias(col)\n",
    "            ).drop(col + \"_split\")\n",
    "            \n",
    "            # Convert to time dtype for non-null values\n",
    "            df = df.with_columns(\n",
    "                pl.when(pl.col(col).is_not_null())\n",
    "                .then(pl.col(col).str.to_time(\"%H:%M:%S\"))\n",
    "                .otherwise(None)\n",
    "                .alias(col)\n",
    "            )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfafa88b",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "time_cols = [\n",
    "    'registration_start_time',\n",
    "    'registration_end_time',\n",
    "    'seizure_start_time',\n",
    "    'seizure_end_time'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5cfb0",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seizures_df = fix_time_columns(seizures_df, time_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efafc6e3-beef-478f-8e9b-ed738e40b92f",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seizures_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbd515",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_seizures_df = fix_time_columns(non_seizures_df, time_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd397b94",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_to_csv(df, output_path='comprehensive_df.csv'):\n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        df_pd = df.to_pandas()\n",
    "    else:\n",
    "        df_pd = df\n",
    "    \n",
    "    df_pd.to_csv(output_path, index=False)\n",
    "    print(f\"Dataset saved to: {output_path}\")\n",
    "    print(f\"File size: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "def save_to_parquet(df, output_path='comprehensive_eeg_features.parquet'):\n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        df.write_parquet(output_path)\n",
    "    else:\n",
    "        df.to_parquet(output_path, index=False, engine='pyarrow')\n",
    "    \n",
    "    print(f\"Dataset saved to: {output_path}\")\n",
    "    print(f\"File size: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e55050",
   "metadata": {},
   "source": [
    "## Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059a592e",
   "metadata": {},
   "source": [
    "### PSD Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e890490",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_psd_features(raw, fmin=0.4, fmax=50, prefix=\"seizure\"):\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Get data\n",
    "        data = raw.get_data()\n",
    "        sfreq = raw.info['sfreq']\n",
    "        \n",
    "        # Calculate PSD using multitaper (more robust than welch for non-stationary signals)\n",
    "        psds, freqs = psd_array_multitaper(\n",
    "            data, sfreq, fmin=fmin, fmax=fmax, \n",
    "            adaptive=True, normalization='full', verbose=False\n",
    "        )\n",
    "        \n",
    "        # Extended frequency bands\n",
    "        bands = {\n",
    "            'delta': (0.4, 4),\n",
    "            'theta': (4, 7),\n",
    "            'alpha': (7, 12),\n",
    "            'low_beta': (12, 20),\n",
    "            'high_beta': (20, 30),\n",
    "            'gamma': (30, 50),\n",
    "            'high_gamma': (50, 60)\n",
    "        }\n",
    "        \n",
    "        band_powers = {}\n",
    "        \n",
    "        for band_name, (low_freq, high_freq) in bands.items():\n",
    "            freq_mask = (freqs >= low_freq) & (freqs < high_freq)\n",
    "            if freq_mask.any():\n",
    "                band_power = np.mean(psds[:, freq_mask], axis=1)\n",
    "                band_powers[band_name] = band_power\n",
    "                \n",
    "                # Store band power statistics\n",
    "                features[f'{prefix}_psd_{band_name}_mean'] = float(np.mean(band_power) * 1e+8)\n",
    "                features[f'{prefix}_psd_{band_name}_std'] = float(np.std(band_power) * 1e+8)\n",
    "                features[f'{prefix}_psd_{band_name}_cv'] = float(np.std(band_power) / (np.mean(band_power) + 1e-10))  # Coefficient of variation\n",
    "        \n",
    "        # Calculate band power ratios (important for seizure detection)\n",
    "        if 'theta' in band_powers and 'alpha' in band_powers:\n",
    "            features[f'{prefix}_psd_theta_alpha_ratio'] = float(np.mean(band_powers['theta']) / (np.mean(band_powers['alpha']) + 1e-8))\n",
    "        \n",
    "        if 'delta' in band_powers and 'alpha' in band_powers:\n",
    "            features[f'{prefix}_psd_delta_alpha_ratio'] = float(np.mean(band_powers['delta']) / (np.mean(band_powers['alpha']) + 1e-8))\n",
    "        \n",
    "        if 'low_beta' in band_powers and 'high_beta' in band_powers:\n",
    "            features[f'{prefix}_psd_beta_ratio'] = float(np.mean(band_powers['high_beta']) / (np.mean(band_powers['low_beta']) + 1e-8))\n",
    "        \n",
    "        # Spectral edge frequencies (SEF50, SEF90, SEF95)\n",
    "        mean_psd = np.mean(psds, axis=0)\n",
    "        cumsum_psd = np.cumsum(mean_psd)\n",
    "        cumsum_psd = cumsum_psd / cumsum_psd[-1]\n",
    "        \n",
    "        for percentile in [50, 75, 90, 95]:\n",
    "            edge_idx = np.where(cumsum_psd >= percentile/100)[0]\n",
    "            if len(edge_idx) > 0:\n",
    "                features[f'{prefix}_psd_sef{percentile}'] = float(freqs[edge_idx[0]])\n",
    "            else:\n",
    "                features[f'{prefix}_psd_sef{percentile}'] = float(freqs[-1])\n",
    "        \n",
    "        # Spectral centroid and spread\n",
    "        freq_weights = freqs * mean_psd\n",
    "        spectral_centroid = np.sum(freq_weights) / (np.sum(mean_psd) + 1e-8)\n",
    "        features[f'{prefix}_psd_spectral_centroid'] = float(spectral_centroid)\n",
    "        \n",
    "        # Spectral spread (standard deviation around centroid)\n",
    "        spectral_spread = np.sqrt(np.sum(((freqs - spectral_centroid) ** 2) * mean_psd) / (np.sum(mean_psd) + 1e-8))\n",
    "        features[f'{prefix}_psd_spectral_spread'] = float(spectral_spread)\n",
    "        \n",
    "        # Spectral skewness and kurtosis\n",
    "        if spectral_spread > 0:\n",
    "            spectral_skewness = np.sum(((freqs - spectral_centroid) ** 3) * mean_psd) / ((spectral_spread ** 3) * np.sum(mean_psd) + 1e-8)\n",
    "            spectral_kurtosis = np.sum(((freqs - spectral_centroid) ** 4) * mean_psd) / ((spectral_spread ** 4) * np.sum(mean_psd) + 1e-8)\n",
    "            features[f'{prefix}_psd_spectral_skewness'] = float(spectral_skewness)\n",
    "            features[f'{prefix}_psd_spectral_kurtosis'] = float(spectral_kurtosis)\n",
    "        else:\n",
    "            features[f'{prefix}_psd_spectral_skewness'] = 0.0\n",
    "            features[f'{prefix}_psd_spectral_kurtosis'] = 0.0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in enhanced PSD calculation: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7eace",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_psd_features_by_region(raw, fmin=0.4, fmax=60, prefix=\"seizure\"):\n",
    "    # Define channel groups for standard monopolar montage\n",
    "    monopolar_channel_groups = {\n",
    "        'frontal': ['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'Fz'],\n",
    "        'central': ['C3', 'C4', 'Cz'],\n",
    "        'parietal': ['P3', 'P4', 'Pz'],\n",
    "        'occipital': ['O1', 'O2'],\n",
    "        'temporal': ['T3', 'T4', 'T5', 'T6'],\n",
    "        'left': ['Fp1', 'F3', 'F7', 'C3', 'P3', 'O1', 'T3', 'T5'],\n",
    "        'right': ['Fp2', 'F4', 'F8', 'C4', 'P4', 'O2', 'T4', 'T6']\n",
    "    }\n",
    "    \n",
    "    # Define channel groups for bipolar montage\n",
    "    bipolar_channel_groups = {\n",
    "        'frontal': ['FP1-F7', 'FP1-F3', 'FP2-F4', 'FP2-F8', 'F7-T7', 'F3-C3', 'F4-C4', 'F8-T8', 'FZ-CZ'],\n",
    "        'temporal': ['F7-T7', 'F8-T8', 'T7-P7', 'T8-P8-0', 'T8-P8-1', 'P7-T7', 'T7-FT9', 'FT10-T8'],\n",
    "        'frontotemporal': ['T7-FT9', 'FT9-FT10', 'FT10-T8'],\n",
    "        'parietal': ['T7-P7', 'C3-P3', 'C4-P4', 'T8-P8-0', 'T8-P8-1', 'P7-O1', 'P3-O1', 'P4-O2', 'P8-O2', 'CZ-PZ', 'P7-T7'],\n",
    "        'occipital': ['P7-O1', 'P3-O1', 'P4-O2', 'P8-O2'],\n",
    "        'central': ['F3-C3', 'C3-P3', 'F4-C4', 'C4-P4', 'FZ-CZ', 'CZ-PZ'],\n",
    "        'left': ['FP1-F7', 'F7-T7', 'T7-P7', 'P7-O1', 'FP1-F3', 'F3-C3', 'C3-P3', 'P3-O1', 'P7-T7', 'T7-FT9'],\n",
    "        'right': ['FP2-F4', 'F4-C4', 'C4-P4', 'P4-O2', 'FP2-F8', 'F8-T8', 'T8-P8-0', 'P8-O2', 'FT10-T8', 'T8-P8-1'],\n",
    "        'midline': ['FZ-CZ', 'CZ-PZ'],\n",
    "        'cross_hemisphere': ['FT9-FT10'],\n",
    "        'left_lateral_chain': ['FP1-F7', 'F7-T7', 'T7-P7', 'P7-O1'],\n",
    "        'left_parasagittal_chain': ['FP1-F3', 'F3-C3', 'C3-P3', 'P3-O1'],\n",
    "        'right_parasagittal_chain': ['FP2-F4', 'F4-C4', 'C4-P4', 'P4-O2'],\n",
    "        'right_lateral_chain': ['FP2-F8', 'F8-T8', 'T8-P8-0', 'P8-O2'],\n",
    "        'midline_chain': ['FZ-CZ', 'CZ-PZ']\n",
    "    }\n",
    "    \n",
    "    available_channels = raw.ch_names\n",
    "    \n",
    "    # Detect channel type based on presence of '-' in channel names\n",
    "    def detect_channel_type(channels):\n",
    "        bipolar_count = sum(1 for ch in channels if '-' in ch and not ch.startswith('EEG'))\n",
    "        monopolar_count = sum(1 for ch in channels if '-' not in ch or ch.startswith('EEG'))\n",
    "        \n",
    "        # If majority of channels contain '-', it's likely bipolar montage\n",
    "        if bipolar_count > monopolar_count * 0.5:\n",
    "            return 'bipolar'\n",
    "        else:\n",
    "            return 'monopolar'\n",
    "    \n",
    "    # Determine which channel groups to use\n",
    "    channel_type = detect_channel_type(available_channels)\n",
    "    \n",
    "    if channel_type == 'bipolar':\n",
    "        channel_groups = bipolar_channel_groups\n",
    "        \n",
    "        # Function to find matching bipolar channels\n",
    "        def find_matching_channels(available_channels, channel_list):\n",
    "            region_channels = []\n",
    "            for ch in available_channels:\n",
    "                ch_clean = ch.upper().replace('EEG', '').replace(' ', '').strip()\n",
    "                for target_ch in channel_list:\n",
    "                    target_clean = target_ch.upper()\n",
    "                    # Check for exact match after normalization\n",
    "                    if ch_clean == target_clean:\n",
    "                        region_channels.append(ch)\n",
    "                        break\n",
    "                    # Check for P8 variations (P8-O2 vs T8-P8-0/T8-P8-1)\n",
    "                    elif 'P8' in target_clean and 'P8' in ch_clean:\n",
    "                        # Handle the special case of P8 channels\n",
    "                        if (target_clean in ch_clean) or (ch_clean in target_clean):\n",
    "                            region_channels.append(ch)\n",
    "                            break\n",
    "            return region_channels\n",
    "            \n",
    "    else:\n",
    "        channel_groups = monopolar_channel_groups\n",
    "        \n",
    "        # Function to find matching monopolar channels (your original logic)\n",
    "        def find_matching_channels(available_channels, channel_list):\n",
    "            region_channels = []\n",
    "            for ch in available_channels:\n",
    "                ch_clean = ch.upper().replace('EEG', '').replace('-', '').replace(' ', '').strip()\n",
    "                for target_ch in channel_list:\n",
    "                    if target_ch.upper() in ch_clean or ch_clean in target_ch.upper():\n",
    "                        region_channels.append(ch)\n",
    "                        break\n",
    "            return region_channels\n",
    "    \n",
    "    # Process each region\n",
    "    regional_features = {}\n",
    "    \n",
    "    for region, channel_list in channel_groups.items():\n",
    "        # Find matching channels for this region\n",
    "        region_channels = find_matching_channels(available_channels, channel_list)\n",
    "        \n",
    "        if region_channels:\n",
    "            try:\n",
    "                # Pick channels for this region\n",
    "                raw_region = raw.copy().pick(region_channels)\n",
    "                \n",
    "                # Extract features for this region\n",
    "                region_psd_features = extract_psd_features(raw_region, fmin, fmax, prefix)\n",
    "                \n",
    "                # Add region prefix to feature names\n",
    "                for feature_name, value in region_psd_features.items():\n",
    "                    regional_features[f'{region}_{feature_name}'] = value\n",
    "                    \n",
    "                #print(f\"Processed {region} region with {len(region_channels)} channels\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Could not process {region} region: {e}\")\n",
    "        else:\n",
    "            print(f\"No channels found for {region} region\")\n",
    "    \n",
    "    return regional_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b817e",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def parse_seizure_time(time_str):\n",
    "    if pd.isna(time_str) or time_str == '' or time_str is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        time_str = str(time_str).strip()\n",
    "        \n",
    "        # Handle HH:MM:SS or MM:SS format\n",
    "        if '.' in time_str:\n",
    "            parts = time_str.split('.')\n",
    "            if len(parts) == 3:\n",
    "                hours, minutes, seconds = map(float, parts)\n",
    "                return hours * 3600 + minutes * 60 + seconds\n",
    "            elif len(parts) == 2:\n",
    "                minutes, seconds = map(float, parts)\n",
    "                return minutes * 60 + seconds\n",
    "        \n",
    "        # Handle pure seconds\n",
    "        return float(time_str)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse time '{time_str}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b7b02",
   "metadata": {},
   "source": [
    "### Differential Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574fb55",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_differential_entropy(data, sfreq, bands=None, prefix=\"seizure\"):\n",
    "    if bands is None:\n",
    "        bands = {\n",
    "            'delta': (0.4, 4),\n",
    "            'theta': (4, 7),\n",
    "            'alpha': (7, 12),\n",
    "            'low_beta': (12, 20),\n",
    "            'high_beta': (20, 30),\n",
    "            'gamma': (30, 50),\n",
    "            'high_gamma': (50, 60)\n",
    "        }\n",
    "    \n",
    "    features = {}\n",
    "    n_channels = data.shape[0]\n",
    "    \n",
    "    for band_name, (low_freq, high_freq) in bands.items():\n",
    "        # Bandpass filter for each band\n",
    "        try:\n",
    "            filtered_data = mne.filter.filter_data(\n",
    "                data, sfreq, l_freq=low_freq, h_freq=high_freq, \n",
    "                verbose=False, method='fir', fir_design='firwin'\n",
    "            )\n",
    "            \n",
    "            # Calculate variance for each channel\n",
    "            variances = np.var(filtered_data, axis=1)\n",
    "            \n",
    "            # Calculate DE: 0.5 * log(2 * pi * e * variance)\n",
    "            # Adding small epsilon to avoid log(0)\n",
    "            de_values = 0.5 * np.log(2 * np.pi * np.e * (variances + 1e-8))\n",
    "            \n",
    "            # Store statistics\n",
    "            features[f'{prefix}_de_{band_name}_mean'] = float(np.mean(de_values))\n",
    "            features[f'{prefix}_de_{band_name}_std'] = float(np.std(de_values))\n",
    "            features[f'{prefix}_de_{band_name}_median'] = float(np.median(de_values))\n",
    "            features[f'{prefix}_de_{band_name}_max'] = float(np.max(de_values))\n",
    "            features[f'{prefix}_de_{band_name}_min'] = float(np.min(de_values))\n",
    "            \n",
    "            # Asymmetry features (frontal asymmetry is important for emotion/seizure)\n",
    "            if n_channels >= 2:\n",
    "                # Calculate hemispheric asymmetry\n",
    "                left_channels = de_values[:n_channels//2]\n",
    "                right_channels = de_values[n_channels//2:]\n",
    "                min_len = min(len(left_channels), len(right_channels))\n",
    "                if min_len > 0:\n",
    "                    asymmetry = left_channels[:min_len] - right_channels[:min_len]\n",
    "                    features[f'{prefix}_de_{band_name}_asymmetry_mean'] = float(np.mean(asymmetry))\n",
    "                    features[f'{prefix}_de_{band_name}_asymmetry_std'] = float(np.std(asymmetry))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating DE for {band_name}: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f767c41b",
   "metadata": {},
   "source": [
    "### Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe2c52",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# simple propagation\n",
    "def calculate_simple_propagation_features(data, sfreq, prefix=\"seizure\"):\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        n_channels, n_samples = data.shape\n",
    "        \n",
    "        # Calculate onset time for each channel\n",
    "        onset_times = []\n",
    "        \n",
    "        for ch_idx in range(n_channels):\n",
    "            channel_data = data[ch_idx, :]\n",
    "            \n",
    "            # Calculate envelope\n",
    "            analytic_signal = signal.hilbert(channel_data)\n",
    "            envelope = np.abs(analytic_signal)\n",
    "            \n",
    "            # Simple smoothing\n",
    "            window_samples = max(3, int(0.1 * sfreq))\n",
    "            if window_samples % 2 == 0:\n",
    "                window_samples += 1\n",
    "            envelope_smooth = signal.savgol_filter(\n",
    "                envelope, \n",
    "                window_samples, \n",
    "                min(1, window_samples-1)\n",
    "            )\n",
    "            \n",
    "            # Find onset as first time envelope exceeds threshold\n",
    "            baseline = np.median(envelope_smooth[:int(0.5 * sfreq)])  # Use first 0.5s as baseline\n",
    "            threshold = baseline + 2 * np.std(envelope_smooth[:int(0.5 * sfreq)])\n",
    "            \n",
    "            # Find first crossing of threshold\n",
    "            above_threshold = envelope_smooth > threshold\n",
    "            if np.any(above_threshold):\n",
    "                onset_idx = np.argmax(above_threshold)\n",
    "                onset_times.append(onset_idx / sfreq)\n",
    "        \n",
    "        print(f'Detected {len(onset_times)} onset times')\n",
    "        \n",
    "        if len(onset_times) > 0:\n",
    "            onset_times = np.array(onset_times)\n",
    "            \n",
    "            # Calculate propagation metrics\n",
    "            sorted_onsets = np.sort(onset_times)\n",
    "            delays = np.diff(sorted_onsets)\n",
    "            \n",
    "            # Propagation speed estimates\n",
    "            avg_distance_mm = 10  # Distance between electrodes in mm\n",
    "            speeds = []\n",
    "            \n",
    "            positive_delays = delays[delays > 0]\n",
    "            if len(positive_delays) > 0:\n",
    "                speeds = avg_distance_mm / positive_delays\n",
    "                \n",
    "                features[f'{prefix}_mean_propagation_speed'] = float(np.mean(speeds))\n",
    "                features[f'{prefix}_median_propagation_speed'] = float(np.median(speeds))\n",
    "                features[f'{prefix}_std_propagation_speed'] = float(np.std(speeds))\n",
    "                features[f'{prefix}_max_propagation_speed'] = float(np.max(speeds))\n",
    "                features[f'{prefix}_min_propagation_speed'] = float(np.min(speeds))\n",
    "                features[f'{prefix}_num_propagation_events'] = len(speeds)\n",
    "            \n",
    "            # Onset timing features\n",
    "            features[f'{prefix}_earliest_onset'] = float(np.min(onset_times))\n",
    "            features[f'{prefix}_latest_onset'] = float(np.max(onset_times))\n",
    "            features[f'{prefix}_onset_spread'] = float(np.max(onset_times) - np.min(onset_times))\n",
    "            \n",
    "            if len(delays) > 0:\n",
    "                features[f'{prefix}_mean_onset_delay'] = float(np.mean(delays))\n",
    "                features[f'{prefix}_max_onset_delay'] = float(np.max(delays))\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in propagation calculation: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f03e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak based propagation\n",
    "def calculate_propagation_features_peak_based(data, sfreq, prefix=\"seizure\"):\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        n_channels, n_samples = data.shape\n",
    "        \n",
    "        onset_times = []\n",
    "        peak_times = []\n",
    "        \n",
    "        for ch_idx in range(n_channels):\n",
    "            channel_data = data[ch_idx, :]\n",
    "            \n",
    "            # Calculate envelope\n",
    "            analytic_signal = signal.hilbert(channel_data)\n",
    "            envelope = np.abs(analytic_signal)\n",
    "            \n",
    "            # Smooth envelope\n",
    "            window_samples = max(3, int(0.05 * sfreq))\n",
    "            if window_samples % 2 == 0:\n",
    "                window_samples += 1\n",
    "            envelope_smooth = signal.savgol_filter(\n",
    "                envelope, \n",
    "                window_samples, \n",
    "                min(1, window_samples-1)\n",
    "            )\n",
    "            \n",
    "            # Find the time of maximum activity (peak)\n",
    "            peak_idx = np.argmax(envelope_smooth)\n",
    "            peak_times.append(peak_idx / sfreq)\n",
    "            \n",
    "            # Find onset using derivative\n",
    "            envelope_diff = np.diff(envelope_smooth)\n",
    "            threshold = np.std(envelope_diff[:int(0.5 * sfreq)]) * 2\n",
    "            \n",
    "            significant_rise = envelope_diff > threshold\n",
    "            if np.any(significant_rise):\n",
    "                onset_idx = np.argmax(significant_rise)\n",
    "                onset_times.append(onset_idx / sfreq)\n",
    "        \n",
    "        print(f'Found {len(onset_times)} onsets and {len(peak_times)} peaks')\n",
    "        \n",
    "        # Calculate features from onset times\n",
    "        if len(onset_times) > 0:\n",
    "            onset_array = np.array(onset_times)\n",
    "            sorted_onsets = np.sort(onset_array)\n",
    "            delays = np.diff(sorted_onsets)\n",
    "            \n",
    "            # Speed calculations\n",
    "            avg_distance_mm = 10\n",
    "            positive_delays = delays[delays > 0]\n",
    "            \n",
    "            if len(positive_delays) > 0:\n",
    "                speeds = avg_distance_mm / positive_delays\n",
    "                \n",
    "                features[f'{prefix}_mean_propagation_speed'] = float(np.mean(speeds))\n",
    "                features[f'{prefix}_median_propagation_speed'] = float(np.median(speeds))\n",
    "                features[f'{prefix}_std_propagation_speed'] = float(np.std(speeds))\n",
    "                features[f'{prefix}_max_propagation_speed'] = float(np.max(speeds))\n",
    "                features[f'{prefix}_min_propagation_speed'] = float(np.min(speeds))\n",
    "            \n",
    "            features[f'{prefix}_onset_spread'] = float(np.max(onset_array) - np.min(onset_array))\n",
    "            features[f'{prefix}_mean_onset_time'] = float(np.mean(onset_array))\n",
    "            \n",
    "        # Calculate features from peak times\n",
    "        if len(peak_times) > 0:\n",
    "            peak_array = np.array(peak_times)\n",
    "            features[f'{prefix}_peak_spread'] = float(np.max(peak_array) - np.min(peak_array))\n",
    "            features[f'{prefix}_mean_peak_time'] = float(np.mean(peak_array))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b9ff2d",
   "metadata": {},
   "source": [
    "### Wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e090633",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_wavelet_features(data, sfreq, wavelet='db4', levels=5, prefix=\"seizure\"):\n",
    "    features = {}\n",
    "    n_channels = data.shape[0]\n",
    "    \n",
    "    # Store features for each decomposition level\n",
    "    all_channel_features = []\n",
    "    \n",
    "    for ch_idx in range(n_channels):\n",
    "        channel_data = data[ch_idx, :]\n",
    "        \n",
    "        try:\n",
    "            # Perform wavelet decomposition\n",
    "            coeffs = pywt.wavedec(channel_data, wavelet, level=levels)\n",
    "            \n",
    "            # Calculate features for each level\n",
    "            channel_features = []\n",
    "            for level, coeff in enumerate(coeffs):\n",
    "                if len(coeff) > 0:\n",
    "                    # Energy of coefficients\n",
    "                    energy = np.sum(coeff ** 2)\n",
    "                    # Entropy\n",
    "                    entropy = stats.entropy(np.abs(coeff) + 1e-8)\n",
    "                    # Statistical features\n",
    "                    mean_coeff = np.mean(np.abs(coeff))\n",
    "                    std_coeff = np.std(coeff)\n",
    "                    max_coeff = np.max(np.abs(coeff))\n",
    "                    \n",
    "                    channel_features.extend([energy, entropy, mean_coeff, std_coeff, max_coeff])\n",
    "                else:\n",
    "                    channel_features.extend([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "            \n",
    "            all_channel_features.append(channel_features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in wavelet transform for channel {ch_idx}: {e}\")\n",
    "            # Add zeros for failed channel\n",
    "            all_channel_features.append([0.0] * (5 * (levels + 1)))\n",
    "    \n",
    "    # Aggregate across channels\n",
    "    all_channel_features = np.array(all_channel_features)\n",
    "    \n",
    "    # Store aggregated features\n",
    "    feature_names = ['energy', 'entropy', 'mean', 'std', 'max']\n",
    "    for level in range(levels + 1):\n",
    "        for feat_idx, feat_name in enumerate(feature_names):\n",
    "            feat_values = all_channel_features[:, level * 5 + feat_idx]\n",
    "            features[f'{prefix}_wt_level{level}_{feat_name}_mean'] = round(float(np.mean(feat_values)), 12)\n",
    "            features[f'{prefix}_wt_level{level}_{feat_name}_std'] = round(float(np.std(feat_values)), 12)\n",
    "    \n",
    "    # Calculate wavelet packet energy distribution\n",
    "    try:\n",
    "        # Use first channel for packet decomposition (computationally expensive for all)\n",
    "        wp = pywt.WaveletPacket(data[0, :], wavelet, maxlevel=3)\n",
    "        packet_energies = []\n",
    "        for node in wp.get_level(3):\n",
    "            packet_energies.append(np.sum(node.data ** 2))\n",
    "        \n",
    "        if packet_energies:\n",
    "            total_energy = sum(packet_energies)\n",
    "            if total_energy > 0:\n",
    "                normalized_energies = np.array([e/total_energy for e in packet_energies])\n",
    "                features[f'{prefix}_wt_packet_entropy'] = float(stats.entropy(normalized_energies + 1e-8))\n",
    "            else:\n",
    "                features[f'{prefix}_wt_packet_entropy'] = 0.0\n",
    "        else:\n",
    "            features[f'{prefix}_wt_packet_entropy'] = 0.0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in wavelet packet decomposition: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc3c8c7",
   "metadata": {},
   "source": [
    "### Time Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc39684b",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_time_domain_features(data, sfreq, prefix=\"seizure\"):\n",
    "    features = {}\n",
    "    n_channels, n_samples = data.shape\n",
    "    \n",
    "    try:\n",
    "        # Statistical features for each channel\n",
    "        channel_features = {\n",
    "            'mean': np.mean(data, axis=1),\n",
    "            'std': np.std(data, axis=1),\n",
    "            'var': np.var(data, axis=1),\n",
    "            'skewness': stats.skew(data, axis=1),\n",
    "            'kurtosis': stats.kurtosis(data, axis=1),\n",
    "            'rms': np.sqrt(np.mean(data ** 2, axis=1)),\n",
    "            'peak_to_peak': np.ptp(data, axis=1),\n",
    "            'zero_crossings': np.sum(np.diff(np.sign(data), axis=1) != 0, axis=1) / (n_samples / sfreq),  # Rate per second\n",
    "        }\n",
    "        \n",
    "        # Hjorth parameters\n",
    "        # Activity (variance of signal)\n",
    "        activity = (np.var(data, axis=1) * 1e+8)\n",
    "        \n",
    "        # Mobility (standard deviation of first derivative / standard deviation of signal)\n",
    "        first_deriv = np.diff(data, axis=1)\n",
    "        mobility = np.std(first_deriv, axis=1) / (np.std(data, axis=1) + 1e-8)\n",
    "        \n",
    "        # Complexity (mobility of first derivative / mobility of signal)\n",
    "        second_deriv = np.diff(first_deriv, axis=1)\n",
    "        mobility_deriv = np.std(second_deriv, axis=1) / (np.std(first_deriv, axis=1) + 1e-8)\n",
    "        complexity = mobility_deriv / (mobility + 1e-8)\n",
    "        \n",
    "        channel_features['hjorth_activity'] = activity\n",
    "        channel_features['hjorth_mobility'] = mobility\n",
    "        channel_features['hjorth_complexity'] = complexity\n",
    "        \n",
    "        # Line length (sum of absolute differences)\n",
    "        line_length = np.sum(np.abs(np.diff(data, axis=1)), axis=1) / n_samples\n",
    "        channel_features['line_length'] = line_length\n",
    "        \n",
    "        # Non-linear energy\n",
    "        nonlinear_energy = []\n",
    "        for ch in range(n_channels):\n",
    "            if n_samples >= 3:\n",
    "                nle = np.mean(data[ch, 1:-1]**2 - data[ch, :-2] * data[ch, 2:])\n",
    "                nonlinear_energy.append(nle)\n",
    "            else:\n",
    "                nonlinear_energy.append(0.0)\n",
    "        channel_features['nonlinear_energy'] = np.array(nonlinear_energy)\n",
    "        \n",
    "        # Aggregate features across channels\n",
    "        for feat_name, feat_values in channel_features.items():\n",
    "            features[f'{prefix}_time_{feat_name}_mean'] = round(float(np.mean(feat_values)), 12)\n",
    "            features[f'{prefix}_time_{feat_name}_std'] = round(float(np.std(feat_values)), 12)\n",
    "            features[f'{prefix}_time_{feat_name}_max'] = round(float(np.max(feat_values)), 12)\n",
    "            features[f'{prefix}_time_{feat_name}_min'] = round(float(np.min(feat_values)), 12)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in time-domain feature calculation: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f5dd3",
   "metadata": {},
   "source": [
    "### Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8dca95",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_connectivity_features(raw, fmin=0.4, fmax=50, prefix=\"seizure\"):\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        data = raw.get_data()\n",
    "        n_channels, n_samples = data.shape\n",
    "        sfreq = raw.info['sfreq']\n",
    "        \n",
    "        # Only calculate if we have multiple channels\n",
    "        if n_channels < 2:\n",
    "            raise ValueError(\"Need at least 2 channels for connectivity\")\n",
    "        \n",
    "        # Reshape data for connectivity calculation (n_epochs, n_channels, n_times)\n",
    "        # Create pseudo-epochs by segmenting the data\n",
    "        epoch_length = int(2 * sfreq)  # 2-second epochs\n",
    "        n_epochs = n_samples // epoch_length\n",
    "        \n",
    "        if n_epochs > 0:\n",
    "            epochs_data = []\n",
    "            for i in range(n_epochs):\n",
    "                start = i * epoch_length\n",
    "                end = start + epoch_length\n",
    "                epochs_data.append(data[:, start:end])\n",
    "            epochs_data = np.array(epochs_data)\n",
    "            \n",
    "            # Calculate spectral connectivity\n",
    "            # Using coherence as the connectivity measure\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_data, method='coh', mode='multitaper',\n",
    "                sfreq=sfreq, fmin=fmin, fmax=fmax,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Get connectivity matrix (n_channels x n_channels x n_freqs)\n",
    "            con_matrix = con.get_data(output='dense')\n",
    "            \n",
    "            # Average across frequencies\n",
    "            mean_connectivity = np.mean(con_matrix, axis=2)\n",
    "            \n",
    "            # Extract upper triangle (excluding diagonal)\n",
    "            upper_tri = np.triu_indices_from(mean_connectivity, k=1)\n",
    "            connectivity_values = mean_connectivity[upper_tri]\n",
    "            \n",
    "            # Store connectivity statistics - these aren't working\n",
    "            #features[f'{prefix}_connectivity_mean'] = np.mean(connectivity_values)\n",
    "            #features[f'{prefix}_connectivity_std'] = np.std(connectivity_values)\n",
    "            #features[f'{prefix}_connectivity_max'] = np.max(connectivity_values)\n",
    "            #features[f'{prefix}_connectivity_min'] = np.min(connectivity_values)\n",
    "            \n",
    "            # Global efficiency (mean of connectivity)\n",
    "            #features[f'{prefix}_global_efficiency'] = np.mean(connectivity_values)\n",
    "            \n",
    "            # Node strength (sum of connections for each node)\n",
    "            node_strengths = np.sum(mean_connectivity, axis=0) - 1  # Subtract diagonal\n",
    "            features[f'{prefix}_node_strength_mean'] = float(np.mean(node_strengths))\n",
    "            features[f'{prefix}_node_strength_std'] = float(np.std(node_strengths))\n",
    "            features[f'{prefix}_node_strength_max'] = float(np.max(node_strengths))\n",
    "            \n",
    "            # Clustering coefficient (simplified version) - doesn't work\n",
    "            #clustering_coeffs = []\n",
    "            #for i in range(n_channels):\n",
    "            #    neighbors = mean_connectivity[i, :] > 0.01  # Threshold for connection\n",
    "            #    n_neighbors = np.sum(neighbors) - 1  # Exclude self\n",
    "            #    if n_neighbors > 1:\n",
    "            #        # Count connections between neighbors\n",
    "            #        neighbor_indices = np.where(neighbors)[0]\n",
    "            #        neighbor_connections = 0\n",
    "            #        for j in range(len(neighbor_indices)):\n",
    "            #            for k in range(j+1, len(neighbor_indices)):\n",
    "            #                if mean_connectivity[neighbor_indices[j], neighbor_indices[k]] > 0.3:\n",
    "            #                    neighbor_connections += 1\n",
    "            #        max_connections = n_neighbors * (n_neighbors - 1) / 2\n",
    "            #        if max_connections > 0:\n",
    "            #            clustering = neighbor_connections / max_connections\n",
    "            #            clustering_coeffs.append(clustering)\n",
    "            \n",
    "            #features[f'{prefix}_clustering_coefficient'] = float(np.mean(clustering_coeffs))\n",
    "  \n",
    "        else:\n",
    "            raise ValueError(\"Not enough data for connectivity analysis\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in connectivity calculation: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5c200",
   "metadata": {},
   "source": [
    "### PAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491ad9f",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_pac_features(data, sfreq, prefix=\"seizure\"):\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        n_channels = data.shape[0]\n",
    "        \n",
    "        # Define phase and amplitude frequency bands\n",
    "        phase_bands = {\n",
    "            'theta': (4, 7),\n",
    "            'alpha': (7, 12)\n",
    "        }\n",
    "        \n",
    "        amplitude_bands = {\n",
    "            'gamma': (30, 50),\n",
    "            'high_gamma': (50, 100)\n",
    "        }\n",
    "        \n",
    "        pac_values = []\n",
    "        \n",
    "        for phase_name, (phase_low, phase_high) in phase_bands.items():\n",
    "            for amp_name, (amp_low, amp_high) in amplitude_bands.items():\n",
    "                \n",
    "                channel_pac = []\n",
    "                for ch in range(min(n_channels, 10)):  # Limit to first 10 channels for speed\n",
    "                    # Extract phase\n",
    "                    phase_filtered = mne.filter.filter_data(\n",
    "                        data[ch:ch+1, :], sfreq, \n",
    "                        l_freq=phase_low, h_freq=phase_high,\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    phase = np.angle(signal.hilbert(phase_filtered[0]))\n",
    "                    \n",
    "                    # Extract amplitude\n",
    "                    amp_filtered = mne.filter.filter_data(\n",
    "                        data[ch:ch+1, :], sfreq,\n",
    "                        l_freq=amp_low, h_freq=amp_high,\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    amplitude = np.abs(signal.hilbert(amp_filtered[0]))\n",
    "                    \n",
    "                    # Calculate PAC using Modulation Index\n",
    "                    n_bins = 18\n",
    "                    phase_bins = np.linspace(-np.pi, np.pi, n_bins + 1)\n",
    "                    amp_by_phase = []\n",
    "                    \n",
    "                    for i in range(n_bins):\n",
    "                        mask = (phase >= phase_bins[i]) & (phase < phase_bins[i+1])\n",
    "                        if np.sum(mask) > 0:\n",
    "                            amp_by_phase.append(np.mean(amplitude[mask]))\n",
    "                        else:\n",
    "                            amp_by_phase.append(0)\n",
    "                    \n",
    "                    # Normalize and calculate entropy\n",
    "                    amp_by_phase = np.array(amp_by_phase)\n",
    "                    if np.sum(amp_by_phase) > 0:\n",
    "                        amp_by_phase = amp_by_phase / np.sum(amp_by_phase)\n",
    "                        # Kullback-Leibler divergence from uniform distribution\n",
    "                        uniform = np.ones(n_bins) / n_bins\n",
    "                        kl_div = np.sum(amp_by_phase * np.log((amp_by_phase + 1e-10) / uniform))\n",
    "                        mi = kl_div / np.log(n_bins)  # Normalized MI\n",
    "                        channel_pac.append(mi)\n",
    "                    else:\n",
    "                        channel_pac.append(0.0)\n",
    "                \n",
    "                if channel_pac:\n",
    "                    pac_value = np.mean(channel_pac)\n",
    "                    features[f'{prefix}_pac_{phase_name}_{amp_name}'] = float(pac_value)\n",
    "                    pac_values.append(pac_value)\n",
    "                else:\n",
    "                    features[f'{prefix}_pac_{phase_name}_{amp_name}'] = 0.0\n",
    "        \n",
    "        # Overall PAC statistics\n",
    "        features[f'{prefix}_pac_mean'] = float(np.mean(pac_values))\n",
    "        features[f'{prefix}_pac_max'] = float(np.max(pac_values))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in PAC calculation: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c95e6",
   "metadata": {},
   "source": [
    "### Sample, Permutation, and Approximate Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec13c63",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_entropy_features(data, sfreq, prefix=\"seizures\"):\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        n_channels = data.shape[0]\n",
    "        \n",
    "        # Sample Entropy\n",
    "        sample_entropies = []\n",
    "        for ch in range(min(n_channels, 10)): \n",
    "            # Downsample for faster computation\n",
    "            downsampled = signal.resample(data[ch, :], min(1000, len(data[ch, :])))\n",
    "            # Simple sample entropy approximation\n",
    "            se = calculate_sample_entropy(downsampled, m=2, r=0.2*np.std(downsampled))\n",
    "            sample_entropies.append(se)\n",
    "        \n",
    "        features[f'{prefix}_sample_entropy_mean'] = float(np.mean(sample_entropies))\n",
    "        features[f'{prefix}_sample_entropy_std'] = float(np.std(sample_entropies))\n",
    "        \n",
    "        # Permutation Entropy\n",
    "        perm_entropies = []\n",
    "        for ch in range(min(n_channels, 10)):\n",
    "            pe = calculate_permutation_entropy(data[ch, :], order=3, delay=1)\n",
    "            perm_entropies.append(pe)\n",
    "        \n",
    "        features[f'{prefix}_permutation_entropy_mean'] = float(np.mean(perm_entropies))\n",
    "        features[f'{prefix}_permutation_entropy_std'] = float(np.std(perm_entropies))\n",
    "        \n",
    "        # Approximate Entropy (simplified)\n",
    "        approx_entropies = []\n",
    "        for ch in range(min(n_channels, 10)):\n",
    "            downsampled = signal.resample(data[ch, :], min(1000, len(data[ch, :])))\n",
    "            ae = calculate_approx_entropy(downsampled, m=2, r=0.2*np.std(downsampled))\n",
    "            approx_entropies.append(ae)\n",
    "        \n",
    "        features[f'{prefix}_approx_entropy_mean'] = float(np.mean(approx_entropies))\n",
    "        features[f'{prefix}_approx_entropy_std'] = float(np.std(approx_entropies))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in entropy calculation: {e}\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "def calculate_sample_entropy(signal_data, m=2, r=0.2):\n",
    "    \"\"\"Calculate sample entropy\"\"\"\n",
    "    N = len(signal_data)\n",
    "    \n",
    "    def _maxdist(x_i, x_j, m):\n",
    "        return max([abs(ua - va) for ua, va in zip(x_i, x_j)])\n",
    "    \n",
    "    def _phi(m):\n",
    "        patterns = np.array([signal_data[i:i+m] for i in range(N - m + 1)])\n",
    "        C = 0\n",
    "        for i in range(len(patterns)):\n",
    "            for j in range(i+1, len(patterns)):\n",
    "                if _maxdist(patterns[i], patterns[j], m) <= r:\n",
    "                    C += 1\n",
    "        return C\n",
    "    \n",
    "    try:\n",
    "        phi_m = _phi(m)\n",
    "        phi_m1 = _phi(m + 1)\n",
    "        \n",
    "        if phi_m == 0:\n",
    "            return 0\n",
    "        return -np.log(phi_m1 / phi_m) if phi_m1 > 0 else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def calculate_permutation_entropy(signal_data, order=3, delay=1):\n",
    "    \"\"\"Calculate permutation entropy\"\"\"\n",
    "    try:\n",
    "        n = len(signal_data)\n",
    "        permutations = []\n",
    "        \n",
    "        for i in range(n - delay * (order - 1)):\n",
    "            indices = [i + j * delay for j in range(order)]\n",
    "            sorted_indices = np.argsort(signal_data[indices])\n",
    "            permutations.append(tuple(sorted_indices))\n",
    "        \n",
    "        # Count occurrences\n",
    "        unique, counts = np.unique(permutations, axis=0, return_counts=True)\n",
    "        probs = counts / len(permutations)\n",
    "        \n",
    "        # Calculate entropy\n",
    "        return -np.sum(probs * np.log(probs + 1e-8))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def calculate_approx_entropy(signal_data, m=2, r=0.2):\n",
    "    \"\"\"Calculate approximate entropy\"\"\"\n",
    "    try:\n",
    "        N = len(signal_data)\n",
    "        \n",
    "        def _maxdist(x_i, x_j, m):\n",
    "            return max([abs(ua - va) for ua, va in zip(x_i, x_j)])\n",
    "        \n",
    "        def _phi(m):\n",
    "            patterns = np.array([signal_data[i:i+m] for i in range(N - m + 1)])\n",
    "            C = np.zeros(N - m + 1)\n",
    "            \n",
    "            for i in range(N - m + 1):\n",
    "                matches = 0\n",
    "                for j in range(N - m + 1):\n",
    "                    if _maxdist(patterns[i], patterns[j], m) <= r:\n",
    "                        matches += 1\n",
    "                C[i] = matches / (N - m + 1)\n",
    "            \n",
    "            return np.mean(np.log(C + 1e-8))\n",
    "        \n",
    "        return _phi(m) - _phi(m + 1)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4b881b",
   "metadata": {},
   "source": [
    "### Rhythmic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49105267",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_rhythmic_features(data, sfreq, prefix=\"seizure\"):\n",
    "    features = {}\n",
    "    \n",
    "    def bandpass_filter(channel_data, lowcut, highcut, fs):\n",
    "        nyquist = fs / 2\n",
    "        low = lowcut / nyquist\n",
    "        high = highcut / nyquist\n",
    "        b, a = butter(4, [low, high], btype='band')\n",
    "        return filtfilt(b, a, channel_data)\n",
    "    \n",
    "    n_channels = data.shape[0]\n",
    "    \n",
    "    # Initialize arrays for each feature\n",
    "    theta_powers = []\n",
    "    delta_slow_powers = []\n",
    "    fast_powers = []\n",
    "    spike_rates = []\n",
    "    \n",
    "    for channel_idx in range(n_channels):\n",
    "        channel = data[channel_idx, :]\n",
    "        \n",
    "        # Theta rhythm (5-7 Hz)\n",
    "        theta_filtered = bandpass_filter(channel, 5, 7, sfreq)\n",
    "        theta_power = np.mean(theta_filtered**2)\n",
    "        theta_powers.append(theta_power)\n",
    "        \n",
    "        # Delta slow rhythm (2-5 Hz)\n",
    "        delta_slow_filtered = bandpass_filter(channel, 2, 5, sfreq)\n",
    "        delta_slow_power = np.mean(delta_slow_filtered**2)\n",
    "        delta_slow_powers.append(delta_slow_power)\n",
    "        \n",
    "        # Fast activity (15-25 Hz)\n",
    "        fast_filtered = bandpass_filter(channel, 15, 25, sfreq)\n",
    "        fast_power = np.mean(fast_filtered**2)\n",
    "        fast_powers.append(fast_power)\n",
    "        \n",
    "        # Spike detection\n",
    "        threshold = 3 * np.std(channel)\n",
    "        spikes = np.where(np.abs(channel) > threshold)[0]\n",
    "        spike_rate = len(spikes) / (len(channel) / sfreq)\n",
    "        spike_rates.append(spike_rate)\n",
    "    \n",
    "    # Store features\n",
    "    features[f'{prefix}_rhythmic_theta_power_mean'] = float(np.mean(theta_powers) * 1e+8)\n",
    "    features[f'{prefix}_rhythmic_theta_power_std'] = float(np.std(theta_powers) * 1e+8)\n",
    "    features[f'{prefix}_rhythmic_delta_slow_power_mean'] = float(np.mean(delta_slow_powers) * 1e+8)\n",
    "    features[f'{prefix}_rhythmic_delta_slow_power_std'] = float(np.std(delta_slow_powers) * 1e+8)\n",
    "    features[f'{prefix}_rhythmic_fast_power_mean'] = float(np.mean(fast_powers) * 1e+8)\n",
    "    features[f'{prefix}_rhythmic_fast_power_std'] = float(np.std(fast_powers) * 1e+8)\n",
    "    features[f'{prefix}_rhythmic_spike_rate_mean'] = float(np.mean(spike_rates))\n",
    "    features[f'{prefix}_rhythmic_spike_rate_std'] = float(np.std(spike_rates))\n",
    "    \n",
    "    # Ratios\n",
    "    features[f'{prefix}_rhythmic_theta_delta_ratio'] = float(np.mean(theta_powers) / (np.mean(delta_slow_powers) + 1e-8))\n",
    "    features[f'{prefix}_rhythmic_fast_theta_ratio'] = float(np.mean(fast_powers) / (np.mean(theta_powers) + 1e-8))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0097497",
   "metadata": {},
   "source": [
    "## Process Single EDF File Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a972b",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_single_edf(\n",
    "                    edf_path, \n",
    "                    seizure_info, \n",
    "                    registration_start,\n",
    "                    registration_end,\n",
    "                    seizure_start,\n",
    "                    seizure_end,\n",
    "                    seizure_start_seconds,\n",
    "                    seizure_end_seconds,\n",
    "                    onset_sec,\n",
    "                    postictal_onset_sec,\n",
    "                    registration_end_seconds,\n",
    "                    postictal_end_sec):\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Load EDF file\n",
    "        raw = mne.io.read_raw_edf(str(edf_path), preload=True, verbose=False)\n",
    "        \n",
    "        # Basic file info\n",
    "        features['file_path'] = str(edf_path)\n",
    "        features['num_channels'] = len(raw.ch_names)\n",
    "        features['sampling_rate'] = raw.info['sfreq']\n",
    "        features['duration_seconds'] = raw.n_times / raw.info['sfreq']\n",
    "\n",
    "        # Add seizure timing info\n",
    "        features['seizure_start_seconds'] = seizure_start_seconds\n",
    "        features['seizure_end_seconds'] = seizure_end_seconds\n",
    "        \n",
    "        # channels available\n",
    "        available_channels = raw.ch_names\n",
    "        \n",
    "        # Apply bandpass filter\n",
    "        raw.filter(0.4, 50, fir_design='firwin', verbose=False)\n",
    "\n",
    "        # Get data\n",
    "        sfreq = raw.info['sfreq']\n",
    "        \n",
    "        # Create raw seizure section\n",
    "        seizure_raw = raw.copy().crop(tmin=onset_sec, tmax=postictal_onset_sec)\n",
    "        seizure_data = seizure_raw.get_data()\n",
    "\n",
    "        # Feature extraction functions\n",
    "        de_features = calculate_differential_entropy(seizure_data, sfreq)\n",
    "        features.update(de_features)\n",
    "\n",
    "        entropy_features = calculate_entropy_features(seizure_data, sfreq)\n",
    "        features.update(entropy_features)\n",
    "        \n",
    "        psd_features = extract_psd_features(seizure_raw)\n",
    "        features.update(psd_features)\n",
    "        \n",
    "        psd_regional_features = extract_psd_features_by_region(seizure_raw)\n",
    "        features.update(psd_regional_features)\n",
    "        \n",
    "        wt_features = calculate_wavelet_features(seizure_data, sfreq)\n",
    "        features.update(wt_features)\n",
    "        \n",
    "        time_features = calculate_time_domain_features(seizure_data, sfreq)\n",
    "        features.update(time_features)\n",
    "        \n",
    "        connectivity_features = calculate_connectivity_features(seizure_raw)\n",
    "        features.update(connectivity_features)\n",
    "        \n",
    "        pac_features = calculate_pac_features(seizure_data, sfreq)\n",
    "        features.update(pac_features)\n",
    "    \n",
    "        rhythmic_features = calculate_rhythmic_features(seizure_data, sfreq)\n",
    "        features.update(rhythmic_features)\n",
    "\n",
    "        # Propagation features\n",
    "        propagation_features = calculate_simple_propagation_features(seizure_data, sfreq)\n",
    "        features.update(propagation_features)\n",
    "        \n",
    "        peak_propagation_features = calculate_propagation_features_peak_based(seizure_data, sfreq) \n",
    "        features.update(peak_propagation_features)\n",
    "\n",
    "        # Create raw post-ictal section\n",
    "        post_ictal_raw = raw.copy().crop(tmin=postictal_onset_sec, tmax=postictal_end_sec)\n",
    "        post_ictal_data = post_ictal_raw.get_data()\n",
    "\n",
    "        post_ictal_psd_features = extract_psd_features(post_ictal_raw, prefix=\"post_ictal\")\n",
    "        features.update(post_ictal_psd_features)\n",
    "        \n",
    "        post_ictal_psd_regional_features = extract_psd_features_by_region(post_ictal_raw, prefix=\"post_ictal\")\n",
    "        features.update(post_ictal_psd_regional_features)\n",
    "        \n",
    "        post_ictal_wt_features = calculate_wavelet_features(post_ictal_data, sfreq, prefix=\"post_ictal\")\n",
    "        features.update(post_ictal_wt_features)\n",
    "        \n",
    "        features['processing_success'] = True\n",
    "        features['error_message'] = ''\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {edf_path}: {e}\")\n",
    "        features['processing_success'] = False\n",
    "        features['error_message'] = str(e)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5645b995",
   "metadata": {},
   "source": [
    "## Build Comprehensive Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14cc12e",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_comprehensive_dataset(seizures_df, patients_df, non_seizures_df, \n",
    "                              data_root_paths=['data/siena_scalp', 'data/chb-mit']):\n",
    "    all_records = []\n",
    "    \n",
    "    # Convert Polars to pandas for iteration (if needed)\n",
    "    if isinstance(seizures_df, pl.DataFrame):\n",
    "        seizures_pd = seizures_df.to_pandas()\n",
    "        patients_pd = patients_df.to_pandas()\n",
    "        non_seizures_pd = non_seizures_df.to_pandas()\n",
    "    else:\n",
    "        seizures_pd = seizures_df\n",
    "        patients_pd = patients_df\n",
    "        non_seizures_pd = non_seizures_df\n",
    "    \n",
    "    print(f\"Processing {len(seizures_pd)} seizure records...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # options for testing\n",
    "    # for idx, seizure_row in tqdm(seizures_pd.iloc[:5].iterrows(), total=5):\n",
    "    # for idx, seizure_row in tqdm(seizures_pd.iterrows(), total=len(seizures_pd)): \n",
    "    \n",
    "    for idx, seizure_row in tqdm(seizures_pd.iterrows(), total=len(seizures_pd)): \n",
    "        record = {}\n",
    "        \n",
    "        # Add all seizure metadata\n",
    "        for col in seizure_row.index:\n",
    "            record[f'{col}'] = seizure_row[col]\n",
    "        \n",
    "        # Find corresponding patient and seizure info\n",
    "        patient_id = seizure_row['patient_id']\n",
    "        patient_info = patients_pd[patients_pd['patient_id'] == patient_id]\n",
    "        registration_start = seizure_row['registration_start_time']\n",
    "        registration_end = seizure_row['registration_end_time']\n",
    "        seizure_start = seizure_row['seizure_start_time']\n",
    "        seizure_end = seizure_row['seizure_end_time']\n",
    "\n",
    "        # Calculate registration and seizure total seconds\n",
    "        registration_start_seconds = registration_start.hour * 3600 + registration_start.minute * 60 + registration_start.second\n",
    "        registration_end_seconds = registration_end.hour * 3600 + registration_end.minute * 60 + registration_end.second\n",
    "        seizure_start_seconds = seizure_start.hour * 3600 + seizure_start.minute * 60 + seizure_start.second\n",
    "        seizure_end_seconds = seizure_end.hour * 3600 + seizure_end.minute * 60 + seizure_end.second\n",
    "        seizure_duration = (seizure_end_seconds - seizure_start_seconds)\n",
    "\n",
    "        if seizure_start_seconds < registration_start_seconds:\n",
    "            reg_adjusted = (86400 - registration_start_seconds)\n",
    "            onset_sec = (seizure_start_seconds + reg_adjusted)\n",
    "            postictal_onset_sec = (seizure_end_seconds + reg_adjusted)\n",
    "        else:\n",
    "            onset_sec = (seizure_start_seconds - registration_start_seconds)\n",
    "            postictal_onset_sec = (seizure_end_seconds - registration_start_seconds)\n",
    "        \n",
    "        if registration_start_seconds > registration_end_seconds:\n",
    "            edf_file_duration = ((registration_end_seconds + 86400) - registration_start_seconds)\n",
    "        else:\n",
    "            edf_file_duration = (registration_end_seconds - registration_start_seconds)\n",
    "            \n",
    "        postictal_end_sec = (edf_file_duration - 1)\n",
    "\n",
    "        # Find EDF file\n",
    "        file_name = seizure_row['file_name']\n",
    "        edf_path = None\n",
    "        \n",
    "        print(f'Patient id: {patient_id}')\n",
    "        print(f'Edf file: {file_name}')\n",
    "        print(f'Registration start: {registration_start}')\n",
    "        print(f'Registration end: {registration_end}')\n",
    "        print(f'Registration start seconds: {registration_start_seconds}')\n",
    "        print(f'Registration end seconds: {registration_end_seconds}')\n",
    "        print(f'EDF file duration: {edf_file_duration}')\n",
    "        print(f'Seizure start: {seizure_start}')\n",
    "        print(f'Seizure end: {seizure_end}')\n",
    "        print(f'Seizure start seconds: {seizure_start_seconds}')\n",
    "        print(f'Seizure end seconds: {seizure_end_seconds}')\n",
    "        print(f'Seizure duration: {seizure_duration}')\n",
    "        print(f'Seizure onset seconds: {onset_sec}')\n",
    "        print(f'Post-ictal onset seconds: {postictal_onset_sec}')\n",
    "        print(f'Post-ictal end seconds: {postictal_end_sec}')\n",
    "        \n",
    "        if not patient_info.empty:\n",
    "            for col in patient_info.columns:\n",
    "                if col != 'patient_id':  # Avoid duplication\n",
    "                    record[f'{col}'] = patient_info.iloc[0][col]\n",
    "        \n",
    "        for root_path in data_root_paths:\n",
    "            possible_paths = [\n",
    "                os.path.join(root_path, patient_id, file_name),\n",
    "                os.path.join(root_path, patient_id.lower(), file_name),\n",
    "                os.path.join(root_path, patient_id.upper(), file_name),\n",
    "            ]\n",
    "            \n",
    "            for path in possible_paths:\n",
    "                if os.path.exists(path):\n",
    "                    edf_path = path\n",
    "                    break\n",
    "            \n",
    "            if edf_path:\n",
    "                break\n",
    "        \n",
    "        if edf_path:\n",
    "            # Process EDF and extract features - pass the entire row as dict\n",
    "            edf_features = process_single_edf(\n",
    "                                    edf_path, \n",
    "                                    seizure_row.to_dict(),\n",
    "                                    registration_start,\n",
    "                                    registration_end,\n",
    "                                    seizure_start,\n",
    "                                    seizure_end,\n",
    "                                    seizure_start_seconds,\n",
    "                                    seizure_end_seconds,\n",
    "                                    onset_sec,\n",
    "                                    postictal_onset_sec,\n",
    "                                    registration_end_seconds,\n",
    "                                    postictal_end_sec\n",
    "                                    )\n",
    "            record.update(edf_features)\n",
    "        else:\n",
    "            record['processing_success'] = False\n",
    "            record['error_message'] = 'EDF file not found'\n",
    "        \n",
    "        all_records.append(record)\n",
    "    \n",
    "    # Create comprehensive dataframe using Polars\n",
    "    comprehensive_df = pl.DataFrame(all_records)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total records: {len(comprehensive_df)}\")\n",
    "    \n",
    "    if 'processing_success' in comprehensive_df.columns:\n",
    "        success_count = comprehensive_df.filter(pl.col('processing_success')).height\n",
    "        print(f\"Successfully processed: {success_count}\")\n",
    "        print(f\"Failed: {len(comprehensive_df) - success_count}\")\n",
    "    \n",
    "    return comprehensive_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91625555",
   "metadata": {},
   "source": [
    "## Fill in Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9472fa9d",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fill_missing_values_polars(df):\n",
    "    # Get column types\n",
    "    str_cols = [col for col in df.columns if df[col].dtype == pl.Utf8]\n",
    "    num_cols = [col for col in df.columns if df[col].dtype in [pl.Float32, pl.Float64, pl.Int8, pl.Int16, pl.Int32, pl.Int64]]\n",
    "    \n",
    "    # Fill string columns with \"N/A\"\n",
    "    for col in str_cols:\n",
    "        df = df.with_columns(pl.col(col).fill_null(\"N/A\"))\n",
    "    \n",
    "    # Fill numeric columns with 0\n",
    "    for col in num_cols:\n",
    "        df = df.with_columns(\n",
    "            pl.col(col).fill_null(0).fill_nan(0)\n",
    "        )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a783dd6",
   "metadata": {},
   "source": [
    "## Main Usage Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ba49f",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build comprehensive dataset\n",
    "comprehensive_df = build_comprehensive_dataset(\n",
    "    seizures_df, \n",
    "    patients_df,\n",
    "    non_seizures_df,\n",
    "    data_root_paths=[\n",
    "        'data/siena_scalp', \n",
    "        'data/chb-mit',\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c1b8d",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "print(\"\\nFilling missing values...\")\n",
    "comprehensive_df = fill_missing_values_polars(comprehensive_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c4ad6e-7275-4754-8714-7fd6974be21b",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "comprehensive_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc4a3ec",
   "metadata": {},
   "source": [
    "## Export Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1fb15",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save to Parquet\n",
    "print(\"\\nSaving results...\")\n",
    "save_to_parquet(comprehensive_df, 'processed_data/comprehensive_eeg_features.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a5c025",
   "metadata": {},
   "source": [
    "## Add LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c11e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=6, dropout=0.2):\n",
    "        super(BiLSTMFeatureExtractor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_features)\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        # lstm_out shape: (batch_size, sequence_length, hidden_size * 2) for bidirectional\n",
    "        # hidden shape: (num_layers * 2, batch_size, hidden_size) for bidirectional\n",
    "        return lstm_out, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf15f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lstm_features(seizure_data, sfreq, window_size=1.0, batch_size=64, num_layers=6):\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        n_channels, n_samples = seizure_data.shape\n",
    "        window_samples = int(window_size * sfreq)\n",
    "        \n",
    "        # Calculate number of windows\n",
    "        n_windows = n_samples // window_samples\n",
    "        \n",
    "        if n_windows < 1:\n",
    "            print(\"Seizure too short for 1-second windows\")\n",
    "            return features\n",
    "        \n",
    "        # Reshape data into windows\n",
    "        # Shape will be (n_windows, n_channels, window_samples)\n",
    "        windowed_data = []\n",
    "        for i in range(n_windows):\n",
    "            start_idx = i * window_samples\n",
    "            end_idx = start_idx + window_samples\n",
    "            window = seizure_data[:, start_idx:end_idx]\n",
    "            windowed_data.append(window)\n",
    "        \n",
    "        windowed_data = np.array(windowed_data)\n",
    "        \n",
    "        # For LSTM input, we want (batch_size, sequence_length, features)\n",
    "        # Treat each window as a sequence element, channels as features\n",
    "        # Average or aggregate samples within each window\n",
    "        lstm_input = []\n",
    "        for window in windowed_data:\n",
    "            # Option 1: Use mean of samples in window for each channel\n",
    "            window_features = np.mean(window, axis=1)\n",
    "            # Option 2: Use all samples (flattened) - uncomment if preferred\n",
    "            # window_features = window.flatten()\n",
    "            lstm_input.append(window_features)\n",
    "        \n",
    "        lstm_input = np.array(lstm_input)\n",
    "        \n",
    "        # Create sequences for LSTM\n",
    "        # Each sequence will be multiple consecutive windows\n",
    "        sequence_length = min(8, n_windows)  # Use 10 seconds or available windows\n",
    "        sequences = []\n",
    "        \n",
    "        for i in range(n_windows - sequence_length + 1):\n",
    "            seq = lstm_input[i:i+sequence_length]\n",
    "            sequences.append(seq)\n",
    "        \n",
    "        if len(sequences) == 0:\n",
    "            print(\"Not enough windows to create sequences\")\n",
    "            return features\n",
    "        \n",
    "        sequences = np.array(sequences)\n",
    "        \n",
    "        # Initialize LSTM model\n",
    "        input_size = n_channels  # Number of EEG channels\n",
    "        hidden_size = 128  # Hidden layer size\n",
    "        lstm_model = BiLSTMFeatureExtractor(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Convert to PyTorch tensor\n",
    "        sequences_tensor = torch.FloatTensor(sequences)\n",
    "        \n",
    "        # Process in batches\n",
    "        lstm_model.eval()\n",
    "        all_outputs = []\n",
    "        all_hidden = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(sequences_tensor), batch_size):\n",
    "                batch = sequences_tensor[i:i+batch_size]\n",
    "                lstm_out, hidden, cell = lstm_model(batch)\n",
    "                \n",
    "                # Collect outputs\n",
    "                all_outputs.append(lstm_out.numpy())\n",
    "                all_hidden.append(hidden.numpy())\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        if all_outputs:\n",
    "            all_outputs = np.concatenate(all_outputs, axis=0)\n",
    "            all_hidden = np.concatenate(all_hidden, axis=1)\n",
    "            \n",
    "            # Extract features from LSTM outputs\n",
    "            # Mean and std of final outputs\n",
    "            features['lstm_output_mean'] = np.mean(all_outputs[:, -1, :])\n",
    "            features['lstm_output_std'] = np.std(all_outputs[:, -1, :])\n",
    "            features['lstm_output_max'] = np.max(all_outputs[:, -1, :])\n",
    "            features['lstm_output_min'] = np.min(all_outputs[:, -1, :])\n",
    "            \n",
    "            # Features from hidden states (last layer)\n",
    "            final_hidden = all_hidden[-2:, :, :]  # Last bidirectional layer\n",
    "            features['lstm_hidden_mean'] = np.mean(final_hidden)\n",
    "            features['lstm_hidden_std'] = np.std(final_hidden)\n",
    "            \n",
    "            # Temporal evolution features\n",
    "            temporal_means = np.mean(all_outputs, axis=(0, 2))  # Mean across batch and features\n",
    "            features['lstm_temporal_trend'] = np.polyfit(range(len(temporal_means)), temporal_means, 1)[0]\n",
    "            \n",
    "            # Forward vs backward differences (bidirectional specific)\n",
    "            output_forward = all_outputs[:, :, :hidden_size]\n",
    "            output_backward = all_outputs[:, :, hidden_size:]\n",
    "            features['lstm_forward_backward_diff'] = np.mean(np.abs(output_forward - output_backward))\n",
    "            \n",
    "            # Energy in different parts of sequence\n",
    "            features['lstm_early_energy'] = np.mean(np.square(all_outputs[:, :sequence_length//3, :]))\n",
    "            features['lstm_middle_energy'] = np.mean(np.square(all_outputs[:, sequence_length//3:2*sequence_length//3, :]))\n",
    "            features['lstm_late_energy'] = np.mean(np.square(all_outputs[:, 2*sequence_length//3:, :]))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in LSTM feature extraction: {e}\")\n",
    "        features['lstm_processing_error'] = str(e)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f78dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_layer(seizures_df, patients_df, \n",
    "                              data_root_paths=['data/siena_scalp', 'data/chb-mit']):\n",
    "    all_records = []\n",
    "    \n",
    "    # Convert Polars to pandas for iteration (if needed)\n",
    "    if isinstance(seizures_df, pl.DataFrame):\n",
    "        seizures_pd = seizures_df.to_pandas()\n",
    "        patients_pd = patients_df.to_pandas()\n",
    "    else:\n",
    "        seizures_pd = seizures_df\n",
    "        patients_pd = patients_df\n",
    "    \n",
    "    # for idx, seizure_row in tqdm(seizures_pd.iloc[:5].iterrows(), total=5):\n",
    "    # for idx, seizure_row in tqdm(seizures_pd.iterrows(), total=len(seizures_pd)):\n",
    "    for idx, seizure_row in tqdm(seizures_pd.iterrows(), total=len(seizures_pd)):\n",
    "        record = {}\n",
    "        \n",
    "        # Add all seizure metadata\n",
    "        for col in seizure_row.index:\n",
    "            record[f'{col}'] = seizure_row[col]\n",
    "        \n",
    "        # Find corresponding patient and seizure info\n",
    "        patient_id = seizure_row['patient_id']\n",
    "        patient_info = patients_pd[patients_pd['patient_id'] == patient_id]\n",
    "        registration_start = seizure_row['registration_start_time']\n",
    "        registration_end = seizure_row['registration_end_time']\n",
    "        seizure_start = seizure_row['seizure_start_time']\n",
    "        seizure_end = seizure_row['seizure_end_time']\n",
    "\n",
    "        # Calculate registration and seizure total seconds\n",
    "        registration_start_seconds = registration_start.hour * 3600 + registration_start.minute * 60 + registration_start.second\n",
    "        registration_end_seconds = registration_end.hour * 3600 + registration_end.minute * 60 + registration_end.second\n",
    "        seizure_start_seconds = seizure_start.hour * 3600 + seizure_start.minute * 60 + seizure_start.second\n",
    "        seizure_end_seconds = seizure_end.hour * 3600 + seizure_end.minute * 60 + seizure_end.second\n",
    "        seizure_duration = (seizure_end_seconds - seizure_start_seconds)\n",
    "\n",
    "        if seizure_start_seconds < registration_start_seconds:\n",
    "            reg_adjusted = (86400 - registration_start_seconds)\n",
    "            onset_sec = (seizure_start_seconds + reg_adjusted)\n",
    "            postictal_onset_sec = (seizure_end_seconds + reg_adjusted)\n",
    "        else:\n",
    "            onset_sec = (seizure_start_seconds - registration_start_seconds)\n",
    "            postictal_onset_sec = (seizure_end_seconds - registration_start_seconds)\n",
    "        \n",
    "        if registration_start_seconds > registration_end_seconds:\n",
    "            edf_file_duration = (registration_end_seconds - (registration_start_seconds + 86400))\n",
    "        else:\n",
    "            edf_file_duration = (registration_end_seconds - registration_start_seconds)\n",
    "            \n",
    "        postictal_end_sec = (edf_file_duration - 1)\n",
    "\n",
    "        # Find EDF file\n",
    "        file_name = seizure_row['file_name']\n",
    "        edf_path = None\n",
    "        \n",
    "        if not patient_info.empty:\n",
    "            for col in patient_info.columns:\n",
    "                if col != 'patient_id':  # Avoid duplication\n",
    "                    record[f'{col}'] = patient_info.iloc[0][col]\n",
    "        \n",
    "        for root_path in data_root_paths:\n",
    "            possible_paths = [\n",
    "                os.path.join(root_path, patient_id, file_name),\n",
    "                os.path.join(root_path, patient_id.lower(), file_name),\n",
    "                os.path.join(root_path, patient_id.upper(), file_name),\n",
    "            ]\n",
    "            \n",
    "            for path in possible_paths:\n",
    "                if os.path.exists(path):\n",
    "                    edf_path = path\n",
    "                    break\n",
    "            \n",
    "            if edf_path:\n",
    "                break\n",
    "        \n",
    "        if edf_path:\n",
    "            try:\n",
    "                # Load EDF file\n",
    "                raw = mne.io.read_raw_edf(str(edf_path), preload=True, verbose=False)\n",
    "                \n",
    "                # Apply bandpass filter\n",
    "                raw.filter(0.4, 50, fir_design='firwin', verbose=False)\n",
    "                \n",
    "                # Get data\n",
    "                sfreq = raw.info['sfreq']\n",
    "            \n",
    "                # Create raw seizure section\n",
    "                seizure_raw = raw.copy().crop(tmin=onset_sec, tmax=postictal_onset_sec)\n",
    "                seizure_data = seizure_raw.get_data()\n",
    "                \n",
    "                # Process EDF and extract features - pass the entire row as dict\n",
    "                lstm_features = extract_lstm_features(\n",
    "                                        seizure_data, \n",
    "                                        sfreq, \n",
    "                                        window_size=1.0, \n",
    "                                        batch_size=64,\n",
    "                                        num_layers=6)\n",
    "                record.update(lstm_features)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {edf_path}: {e}\")\n",
    "                \n",
    "        else:\n",
    "            record['processing_success'] = False\n",
    "            record['error_message'] = 'EDF file not found'\n",
    "        \n",
    "        all_records.append(record)\n",
    "    \n",
    "    lstm_df = pl.DataFrame(all_records)\n",
    "    \n",
    "    return lstm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb9c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM usage\n",
    "lstm_df = build_lstm_layer(\n",
    "                        seizures_df, \n",
    "                        patients_df, \n",
    "                        data_root_paths=['data/siena_scalp', 'data/chb-mit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e393f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab762fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save LSTM features to parquet\n",
    "save_to_parquet(lstm_df, output_path='processed_data/lstm_features.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28605b3d",
   "metadata": {},
   "source": [
    "## Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab25b09",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load processed data set\n",
    "processed_df = pl.read_parquet(\"processed_data/comprehensive_eeg_features.parquet\")\n",
    "\n",
    "print(f\"Shape: {processed_df.shape}\")\n",
    "print(f\"Columns: {processed_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1279b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LSTM dataset\n",
    "processed_lstm_df = pl.read_parquet(\"processed_data/lstm_features.parquet\")\n",
    "\n",
    "print(f\"Shape: {processed_lstm_df.shape}\")\n",
    "print(f\"Columns: {processed_lstm_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48cd6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_dataframes(df1, df2, join_key='seizure_index', how='left'):\n",
    "    \n",
    "    return df1.join(df2, on=join_key, how=how)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a31079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets\n",
    "full_processed_df = join_dataframes(processed_df, processed_lstm_df)\n",
    "print(f\"Joined shape: {full_processed_df.shape}\")\n",
    "print(f\"Joined columns: {full_processed_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e460c12e",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get column names and dtypes\n",
    "for col, dtype in zip(full_processed_df.columns, full_processed_df.dtypes):\n",
    "    print(f\"{col}: {dtype}\")\n",
    "\n",
    "# Alternative: as a list of tuples\n",
    "column_info = [(col, dtype) for col, dtype in zip(full_processed_df.columns, full_processed_df.dtypes)]\n",
    "\n",
    "# Alternative: as a dictionary\n",
    "column_dtypes = dict(zip(full_processed_df.columns, full_processed_df.dtypes))\n",
    "\n",
    "# Display schema (formatted output)\n",
    "print(full_processed_df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0867d9",
   "metadata": {},
   "source": [
    "## Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01489a2",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def encode_categoricals(df, output_path='processed_data'):\n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Copy dataframe to avoid modifying original\n",
    "    encoded_df = df.clone()\n",
    "    \n",
    "    # Dictionary to store encoding mappings\n",
    "    encoding_mappings = {}\n",
    "    \n",
    "    # Process each column\n",
    "    for col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "        \n",
    "        # Check if column is string/object type\n",
    "        if dtype == pl.Utf8 or dtype == pl.Object:\n",
    "            # Get unique values and create mapping (starting from 1)\n",
    "            unique_vals = encoded_df[col].unique().drop_nulls().sort()\n",
    "            mapping = {val: i+1 for i, val in enumerate(unique_vals)}\n",
    "            encoding_mappings[col] = mapping\n",
    "            \n",
    "            # Apply the mapping directly\n",
    "            encoded_df = encoded_df.with_columns(\n",
    "                pl.col(col).replace(mapping).alias(col).cast(pl.Categorical).to_physical()\n",
    "            )\n",
    "            \n",
    "    # Save mappings to JSON\n",
    "    json_path = Path(output_path) / 'categorical_encodings.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(encoding_mappings, f, indent=2)\n",
    "    \n",
    "    return encoded_df, encoding_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34a27c",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoded_df, encoding_mappings = encode_categoricals(full_processed_df, 'processed_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a43355",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = fill_missing_values_polars(encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf2acf",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6819b03",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the plot output path\n",
    "plot_output_path = Path('plot_outputs')\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "plot_output_path.mkdir(exist_ok=True)\n",
    "print(f\"Directory '{plot_output_path}' is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5081408e",
   "metadata": {},
   "source": [
    "## Feature Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73436fff",
   "metadata": {},
   "source": [
    "### Seizures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_features = [\n",
    "    'seizure_de_delta_mean', \n",
    "    'seizure_de_delta_std', # high for all\n",
    "    'seizure_de_delta_median', \n",
    "    'seizure_de_delta_max', # high for all\n",
    "    'seizure_de_delta_min', \n",
    "    'seizure_de_delta_asymmetry_mean', \n",
    "    'seizure_de_delta_asymmetry_std', # high for all\n",
    "    'seizure_de_theta_mean', \n",
    "    'seizure_de_theta_std', # high for all\n",
    "    'seizure_de_theta_median', \n",
    "    'seizure_de_theta_max', # high for all\n",
    "    'seizure_de_theta_min', \n",
    "    'seizure_de_theta_asymmetry_mean', \n",
    "    'seizure_de_theta_asymmetry_std', # high for all\n",
    "    'seizure_de_alpha_mean', \n",
    "    'seizure_de_alpha_std', # high for all\n",
    "    'seizure_de_alpha_median', \n",
    "    'seizure_de_alpha_max', # high for all\n",
    "    'seizure_de_alpha_min', \n",
    "    'seizure_de_alpha_asymmetry_mean', \n",
    "    'seizure_de_alpha_asymmetry_std', # high for all\n",
    "    'seizure_de_low_beta_mean', \n",
    "    'seizure_de_low_beta_std', # high for all\n",
    "    'seizure_de_low_beta_median', \n",
    "    'seizure_de_low_beta_max', # high for all\n",
    "    'seizure_de_low_beta_min', \n",
    "    'seizure_de_low_beta_asymmetry_mean', \n",
    "    'seizure_de_low_beta_asymmetry_std', # high for all\n",
    "    'seizure_de_high_beta_mean', \n",
    "    'seizure_de_high_beta_std', # high for all\n",
    "    'seizure_de_high_beta_median', \n",
    "    'seizure_de_high_beta_max', # high for all\n",
    "    'seizure_de_high_beta_min', \n",
    "    'seizure_de_high_beta_asymmetry_mean', \n",
    "    'seizure_de_high_beta_asymmetry_std', # high for all\n",
    "    'seizure_de_gamma_mean', \n",
    "    'seizure_de_gamma_std', # high for all\n",
    "    'seizure_de_gamma_median', \n",
    "    'seizure_de_gamma_max', # high for all\n",
    "    'seizure_de_gamma_min', \n",
    "    'seizure_de_gamma_asymmetry_mean', \n",
    "    'seizure_de_gamma_asymmetry_std', # high for all\n",
    "    'seizure_de_high_gamma_mean', \n",
    "    'seizure_de_high_gamma_std', # high for all\n",
    "    'seizure_de_high_gamma_median', \n",
    "    'seizure_de_high_gamma_max', # high for all\n",
    "    'seizure_de_high_gamma_min', \n",
    "    'seizure_de_high_gamma_asymmetry_mean', \n",
    "    'seizure_de_high_gamma_asymmetry_std', # high for all\n",
    "    'seizures_sample_entropy_mean', \n",
    "    'seizures_sample_entropy_std', \n",
    "    'seizures_permutation_entropy_mean', \n",
    "    'seizures_permutation_entropy_std', \n",
    "    'seizures_approx_entropy_mean', \n",
    "    'seizures_approx_entropy_std',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753907cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "psd_features = [\n",
    "    'seizure_psd_delta_mean', \n",
    "    'seizure_psd_delta_std', \n",
    "    'seizure_psd_delta_cv', \n",
    "    'seizure_psd_theta_mean', \n",
    "    'seizure_psd_theta_std', \n",
    "    'seizure_psd_theta_cv', \n",
    "    'seizure_psd_alpha_mean', \n",
    "    'seizure_psd_alpha_std', \n",
    "    'seizure_psd_alpha_cv', \n",
    "    'seizure_psd_low_beta_mean', \n",
    "    'seizure_psd_low_beta_std', \n",
    "    'seizure_psd_low_beta_cv', \n",
    "    'seizure_psd_high_beta_mean', \n",
    "    'seizure_psd_high_beta_std', \n",
    "    'seizure_psd_high_beta_cv', \n",
    "    'seizure_psd_gamma_mean', \n",
    "    'seizure_psd_gamma_std', \n",
    "    'seizure_psd_gamma_cv', \n",
    "    'seizure_psd_theta_alpha_ratio', \n",
    "    'seizure_psd_delta_alpha_ratio', \n",
    "    'seizure_psd_beta_ratio', \n",
    "    'seizure_psd_sef50', # high for all\n",
    "    'seizure_psd_sef75', # high for all\n",
    "    'seizure_psd_sef90', # high for all\n",
    "    'seizure_psd_sef95', # high for all\n",
    "    'seizure_psd_spectral_centroid', \n",
    "    'seizure_psd_spectral_spread', \n",
    "    'seizure_psd_spectral_skewness', \n",
    "    'seizure_psd_spectral_kurtosis', \n",
    "]\n",
    "\n",
    "frontal_psd_features = [\n",
    "    'frontal_seizure_psd_delta_mean', \n",
    "    'frontal_seizure_psd_delta_std', \n",
    "    'frontal_seizure_psd_delta_cv', # very high for all\n",
    "    'frontal_seizure_psd_theta_mean', \n",
    "    'frontal_seizure_psd_theta_std', \n",
    "    'frontal_seizure_psd_theta_cv', # very high for all\n",
    "    'frontal_seizure_psd_alpha_mean', \n",
    "    'frontal_seizure_psd_alpha_std', \n",
    "    'frontal_seizure_psd_alpha_cv', # very high for all\n",
    "    'frontal_seizure_psd_low_beta_mean', \n",
    "    'frontal_seizure_psd_low_beta_std', # highish for localization\n",
    "    'frontal_seizure_psd_low_beta_cv', # very high for all\n",
    "    'frontal_seizure_psd_high_beta_mean', \n",
    "    'frontal_seizure_psd_high_beta_std', \n",
    "    'frontal_seizure_psd_high_beta_cv', # very high for all\n",
    "    'frontal_seizure_psd_gamma_mean', # higher for seizure type\n",
    "    'frontal_seizure_psd_gamma_std', # higher for seizure type\n",
    "    'frontal_seizure_psd_gamma_cv', # high for all\n",
    "    'frontal_seizure_psd_high_gamma_mean', \n",
    "    'frontal_seizure_psd_high_gamma_std', \n",
    "    'frontal_seizure_psd_high_gamma_cv', # high for all\n",
    "    'frontal_seizure_psd_theta_alpha_ratio', \n",
    "    'frontal_seizure_psd_delta_alpha_ratio', \n",
    "    'frontal_seizure_psd_beta_ratio', \n",
    "    'frontal_seizure_psd_sef50', # high for seizure type\n",
    "    'frontal_seizure_psd_sef75', # high for seizure type, highish for other two\n",
    "    'frontal_seizure_psd_sef90', # high for seizure type, highish for other two\n",
    "    'frontal_seizure_psd_sef95', \n",
    "    'frontal_seizure_psd_spectral_centroid', # high for seizure type\n",
    "    'frontal_seizure_psd_spectral_spread', # highish for all\n",
    "    'frontal_seizure_psd_spectral_skewness', \n",
    "    'frontal_seizure_psd_spectral_kurtosis', \n",
    "]\n",
    "\n",
    "# all negative correlations\n",
    "temporal_psd_features = [\n",
    "    'temporal_seizure_psd_delta_mean', \n",
    "    'temporal_seizure_psd_delta_std', \n",
    "    'temporal_seizure_psd_delta_cv', \n",
    "    'temporal_seizure_psd_theta_mean', \n",
    "    'temporal_seizure_psd_theta_std', \n",
    "    'temporal_seizure_psd_theta_cv', \n",
    "    'temporal_seizure_psd_alpha_mean', \n",
    "    'temporal_seizure_psd_alpha_std', \n",
    "    'temporal_seizure_psd_alpha_cv', \n",
    "    'temporal_seizure_psd_low_beta_mean', \n",
    "    'temporal_seizure_psd_low_beta_std', \n",
    "    'temporal_seizure_psd_low_beta_cv', \n",
    "    'temporal_seizure_psd_high_beta_mean', \n",
    "    'temporal_seizure_psd_high_beta_std', \n",
    "    'temporal_seizure_psd_high_beta_cv', \n",
    "    'temporal_seizure_psd_gamma_mean', \n",
    "    'temporal_seizure_psd_gamma_std', \n",
    "    'temporal_seizure_psd_gamma_cv', \n",
    "    'temporal_seizure_psd_high_gamma_mean', \n",
    "    'temporal_seizure_psd_high_gamma_std', \n",
    "    'temporal_seizure_psd_high_gamma_cv', # high for seizure type\n",
    "    'temporal_seizure_psd_theta_alpha_ratio', \n",
    "    'temporal_seizure_psd_delta_alpha_ratio', \n",
    "    'temporal_seizure_psd_beta_ratio', \n",
    "    'temporal_seizure_psd_sef50', # high for seizure type\n",
    "    'temporal_seizure_psd_sef75', \n",
    "    'temporal_seizure_psd_sef90', \n",
    "    'temporal_seizure_psd_sef95', \n",
    "    'temporal_seizure_psd_spectral_centroid', \n",
    "    'temporal_seizure_psd_spectral_spread', \n",
    "    'temporal_seizure_psd_spectral_skewness', # high for localization and lateralization\n",
    "    'temporal_seizure_psd_spectral_kurtosis', # high for localization and lateralization\n",
    "]\n",
    "\n",
    "frontotemporal_psd_features = [\n",
    "    'frontotemporal_seizure_psd_delta_mean', \n",
    "    'frontotemporal_seizure_psd_delta_std', \n",
    "    'frontotemporal_seizure_psd_delta_cv', \n",
    "    'frontotemporal_seizure_psd_theta_mean', \n",
    "    'frontotemporal_seizure_psd_theta_std', \n",
    "    'frontotemporal_seizure_psd_theta_cv', \n",
    "    'frontotemporal_seizure_psd_alpha_mean', \n",
    "    'frontotemporal_seizure_psd_alpha_std', \n",
    "    'frontotemporal_seizure_psd_alpha_cv', \n",
    "    'frontotemporal_seizure_psd_low_beta_mean', \n",
    "    'frontotemporal_seizure_psd_low_beta_std', \n",
    "    'frontotemporal_seizure_psd_low_beta_cv', \n",
    "    'frontotemporal_seizure_psd_high_beta_mean', \n",
    "    'frontotemporal_seizure_psd_high_beta_std', \n",
    "    'frontotemporal_seizure_psd_high_beta_cv', \n",
    "    'frontotemporal_seizure_psd_gamma_mean', \n",
    "    'frontotemporal_seizure_psd_gamma_std', \n",
    "    'frontotemporal_seizure_psd_gamma_cv', \n",
    "    'frontotemporal_seizure_psd_high_gamma_mean', \n",
    "    'frontotemporal_seizure_psd_high_gamma_std', \n",
    "    'frontotemporal_seizure_psd_high_gamma_cv', \n",
    "    'frontotemporal_seizure_psd_theta_alpha_ratio', \n",
    "    'frontotemporal_seizure_psd_delta_alpha_ratio', \n",
    "    'frontotemporal_seizure_psd_beta_ratio', \n",
    "    'frontotemporal_seizure_psd_sef50', \n",
    "    'frontotemporal_seizure_psd_sef75', \n",
    "    'frontotemporal_seizure_psd_sef90', \n",
    "    'frontotemporal_seizure_psd_sef95', \n",
    "    'frontotemporal_seizure_psd_spectral_centroid', \n",
    "    'frontotemporal_seizure_psd_spectral_spread', \n",
    "    'frontotemporal_seizure_psd_spectral_skewness', \n",
    "    'frontotemporal_seizure_psd_spectral_kurtosis', \n",
    "]\n",
    "\n",
    "parietal_psd_features = [\n",
    "    'parietal_seizure_psd_delta_mean', \n",
    "    'parietal_seizure_psd_delta_std', \n",
    "    'parietal_seizure_psd_delta_cv', \n",
    "    'parietal_seizure_psd_theta_mean', \n",
    "    'parietal_seizure_psd_theta_std', \n",
    "    'parietal_seizure_psd_theta_cv', \n",
    "    'parietal_seizure_psd_alpha_mean', \n",
    "    'parietal_seizure_psd_alpha_std', \n",
    "    'parietal_seizure_psd_alpha_cv', \n",
    "    'parietal_seizure_psd_low_beta_mean', \n",
    "    'parietal_seizure_psd_low_beta_std', \n",
    "    'parietal_seizure_psd_low_beta_cv', \n",
    "    'parietal_seizure_psd_high_beta_mean', \n",
    "    'parietal_seizure_psd_high_beta_std', \n",
    "    'parietal_seizure_psd_high_beta_cv', \n",
    "    'parietal_seizure_psd_gamma_mean', \n",
    "    'parietal_seizure_psd_gamma_std', \n",
    "    'parietal_seizure_psd_gamma_cv', \n",
    "    'parietal_seizure_psd_high_gamma_mean', \n",
    "    'parietal_seizure_psd_high_gamma_std', \n",
    "    'parietal_seizure_psd_high_gamma_cv', # high for seizure type\n",
    "    'parietal_seizure_psd_theta_alpha_ratio', \n",
    "    'parietal_seizure_psd_delta_alpha_ratio', \n",
    "    'parietal_seizure_psd_beta_ratio', \n",
    "    'parietal_seizure_psd_sef50', \n",
    "    'parietal_seizure_psd_sef75', \n",
    "    'parietal_seizure_psd_sef90', \n",
    "    'parietal_seizure_psd_sef95', \n",
    "    'parietal_seizure_psd_spectral_centroid', \n",
    "    'parietal_seizure_psd_spectral_spread', \n",
    "    'parietal_seizure_psd_spectral_skewness', # high for localization and lateralization\n",
    "    'parietal_seizure_psd_spectral_kurtosis', # high for localization and lateralization\n",
    "]\n",
    "\n",
    "occipital_psd_features = [\n",
    "    'occipital_seizure_psd_delta_mean', \n",
    "    'occipital_seizure_psd_delta_std', \n",
    "    'occipital_seizure_psd_delta_cv', \n",
    "    'occipital_seizure_psd_theta_mean', \n",
    "    'occipital_seizure_psd_theta_std', \n",
    "    'occipital_seizure_psd_theta_cv', \n",
    "    'occipital_seizure_psd_alpha_mean', \n",
    "    'occipital_seizure_psd_alpha_std', \n",
    "    'occipital_seizure_psd_alpha_cv', \n",
    "    'occipital_seizure_psd_low_beta_mean', \n",
    "    'occipital_seizure_psd_low_beta_std', \n",
    "    'occipital_seizure_psd_low_beta_cv', \n",
    "    'occipital_seizure_psd_high_beta_mean', \n",
    "    'occipital_seizure_psd_high_beta_std', \n",
    "    'occipital_seizure_psd_high_beta_cv', \n",
    "    'occipital_seizure_psd_gamma_mean', \n",
    "    'occipital_seizure_psd_gamma_std', \n",
    "    'occipital_seizure_psd_gamma_cv', \n",
    "    'occipital_seizure_psd_high_gamma_mean', \n",
    "    'occipital_seizure_psd_high_gamma_std', \n",
    "    'occipital_seizure_psd_high_gamma_cv', \n",
    "    'occipital_seizure_psd_theta_alpha_ratio', \n",
    "    'occipital_seizure_psd_delta_alpha_ratio', \n",
    "    'occipital_seizure_psd_beta_ratio', \n",
    "    'occipital_seizure_psd_sef50', # high for all\n",
    "    'occipital_seizure_psd_sef75', # hihg for all\n",
    "    'occipital_seizure_psd_sef90', # high for all\n",
    "    'occipital_seizure_psd_sef95', # high for all\n",
    "    'occipital_seizure_psd_spectral_centroid', \n",
    "    'occipital_seizure_psd_spectral_spread', \n",
    "    'occipital_seizure_psd_spectral_skewness', \n",
    "    'occipital_seizure_psd_spectral_kurtosis', \n",
    "]\n",
    "\n",
    "central_psd_features = [\n",
    "    'central_seizure_psd_delta_mean', \n",
    "    'central_seizure_psd_delta_std', \n",
    "    'central_seizure_psd_delta_cv', \n",
    "    'central_seizure_psd_theta_mean', \n",
    "    'central_seizure_psd_theta_std', \n",
    "    'central_seizure_psd_theta_cv', \n",
    "    'central_seizure_psd_alpha_mean', \n",
    "    'central_seizure_psd_alpha_std', \n",
    "    'central_seizure_psd_alpha_cv', \n",
    "    'central_seizure_psd_low_beta_mean', \n",
    "    'central_seizure_psd_low_beta_std', \n",
    "    'central_seizure_psd_low_beta_cv', \n",
    "    'central_seizure_psd_high_beta_mean', \n",
    "    'central_seizure_psd_high_beta_std', \n",
    "    'central_seizure_psd_high_beta_cv', \n",
    "    'central_seizure_psd_gamma_mean', \n",
    "    'central_seizure_psd_gamma_std', \n",
    "    'central_seizure_psd_gamma_cv', # high for seizure type\n",
    "    'central_seizure_psd_high_gamma_mean', \n",
    "    'central_seizure_psd_high_gamma_std', \n",
    "    'central_seizure_psd_high_gamma_cv', # high for seizure type\n",
    "    'central_seizure_psd_theta_alpha_ratio', \n",
    "    'central_seizure_psd_delta_alpha_ratio', \n",
    "    'central_seizure_psd_beta_ratio', \n",
    "    'central_seizure_psd_sef50', # high for seizure type\n",
    "    'central_seizure_psd_sef75', # high for seizure type\n",
    "    'central_seizure_psd_sef90', # high for seizure type and lateralization\n",
    "    'central_seizure_psd_sef95', # high for all \n",
    "    'central_seizure_psd_spectral_centroid', # high for seizure type\n",
    "    'central_seizure_psd_spectral_spread', # high for seizure type\n",
    "    'central_seizure_psd_spectral_skewness', # hihg for localization and lateralization\n",
    "    'central_seizure_psd_spectral_kurtosis', \n",
    "]\n",
    "\n",
    "left_psd_features = [\n",
    "    'left_seizure_psd_delta_mean', \n",
    "    'left_seizure_psd_delta_std', \n",
    "    'left_seizure_psd_delta_cv', # high for all, higher for localization \n",
    "    'left_seizure_psd_theta_mean', \n",
    "    'left_seizure_psd_theta_std', \n",
    "    'left_seizure_psd_theta_cv', # higher for localization and lateralization\n",
    "    'left_seizure_psd_alpha_mean', \n",
    "    'left_seizure_psd_alpha_std', \n",
    "    'left_seizure_psd_alpha_cv', # high for all\n",
    "    'left_seizure_psd_low_beta_mean', \n",
    "    'left_seizure_psd_low_beta_std', \n",
    "    'left_seizure_psd_low_beta_cv', \n",
    "    'left_seizure_psd_high_beta_mean', \n",
    "    'left_seizure_psd_high_beta_std', \n",
    "    'left_seizure_psd_high_beta_cv', # high for all\n",
    "    'left_seizure_psd_gamma_mean', \n",
    "    'left_seizure_psd_gamma_std', \n",
    "    'left_seizure_psd_gamma_cv', # high for all\n",
    "    'left_seizure_psd_high_gamma_mean', \n",
    "    'left_seizure_psd_high_gamma_std', \n",
    "    'left_seizure_psd_high_gamma_cv', # high for all\n",
    "    'left_seizure_psd_theta_alpha_ratio', \n",
    "    'left_seizure_psd_delta_alpha_ratio', \n",
    "    'left_seizure_psd_beta_ratio', \n",
    "    'left_seizure_psd_sef50', # high for seizure type\n",
    "    'left_seizure_psd_sef75', # high for seizure type\n",
    "    'left_seizure_psd_sef90', # high for seizure type\n",
    "    'left_seizure_psd_sef95', \n",
    "    'left_seizure_psd_spectral_centroid', # high for seizure type\n",
    "    'left_seizure_psd_spectral_spread', # high for seizure type\n",
    "    'left_seizure_psd_spectral_skewness', \n",
    "    'left_seizure_psd_spectral_kurtosis',\n",
    "    'left_lateral_chain_seizure_psd_delta_mean', \n",
    "    'left_lateral_chain_seizure_psd_delta_std', \n",
    "    'left_lateral_chain_seizure_psd_delta_cv', \n",
    "    'left_lateral_chain_seizure_psd_theta_mean', \n",
    "    'left_lateral_chain_seizure_psd_theta_std', \n",
    "    'left_lateral_chain_seizure_psd_theta_cv', \n",
    "    'left_lateral_chain_seizure_psd_alpha_mean', \n",
    "    'left_lateral_chain_seizure_psd_alpha_std', \n",
    "    'left_lateral_chain_seizure_psd_alpha_cv', \n",
    "    'left_lateral_chain_seizure_psd_low_beta_mean', \n",
    "    'left_lateral_chain_seizure_psd_low_beta_std', \n",
    "    'left_lateral_chain_seizure_psd_low_beta_cv', \n",
    "    'left_lateral_chain_seizure_psd_high_beta_mean', \n",
    "    'left_lateral_chain_seizure_psd_high_beta_std', \n",
    "    'left_lateral_chain_seizure_psd_high_beta_cv', \n",
    "    'left_lateral_chain_seizure_psd_gamma_mean', \n",
    "    'left_lateral_chain_seizure_psd_gamma_std', \n",
    "    'left_lateral_chain_seizure_psd_gamma_cv', \n",
    "    'left_lateral_chain_seizure_psd_high_gamma_mean', \n",
    "    'left_lateral_chain_seizure_psd_high_gamma_std', \n",
    "    'left_lateral_chain_seizure_psd_high_gamma_cv', \n",
    "    'left_lateral_chain_seizure_psd_theta_alpha_ratio', \n",
    "    'left_lateral_chain_seizure_psd_delta_alpha_ratio', \n",
    "    'left_lateral_chain_seizure_psd_beta_ratio', \n",
    "    'left_lateral_chain_seizure_psd_sef50', \n",
    "    'left_lateral_chain_seizure_psd_sef75', \n",
    "    'left_lateral_chain_seizure_psd_sef90', \n",
    "    'left_lateral_chain_seizure_psd_sef95', \n",
    "    'left_lateral_chain_seizure_psd_spectral_centroid', \n",
    "    'left_lateral_chain_seizure_psd_spectral_spread', \n",
    "    'left_lateral_chain_seizure_psd_spectral_skewness', \n",
    "    'left_lateral_chain_seizure_psd_spectral_kurtosis', \n",
    "    'left_parasagittal_chain_seizure_psd_delta_mean', \n",
    "    'left_parasagittal_chain_seizure_psd_delta_std', \n",
    "    'left_parasagittal_chain_seizure_psd_delta_cv', \n",
    "    'left_parasagittal_chain_seizure_psd_theta_mean', \n",
    "    'left_parasagittal_chain_seizure_psd_theta_std', \n",
    "    'left_parasagittal_chain_seizure_psd_theta_cv', \n",
    "    'left_parasagittal_chain_seizure_psd_alpha_mean', \n",
    "    'left_parasagittal_chain_seizure_psd_alpha_std', \n",
    "    'left_parasagittal_chain_seizure_psd_alpha_cv', \n",
    "    'left_parasagittal_chain_seizure_psd_low_beta_mean', \n",
    "    'left_parasagittal_chain_seizure_psd_low_beta_std', \n",
    "    'left_parasagittal_chain_seizure_psd_low_beta_cv', \n",
    "    'left_parasagittal_chain_seizure_psd_high_beta_mean', \n",
    "    'left_parasagittal_chain_seizure_psd_high_beta_std', \n",
    "    'left_parasagittal_chain_seizure_psd_high_beta_cv', \n",
    "    'left_parasagittal_chain_seizure_psd_gamma_mean', \n",
    "    'left_parasagittal_chain_seizure_psd_gamma_std', \n",
    "    'left_parasagittal_chain_seizure_psd_gamma_cv', \n",
    "    'left_parasagittal_chain_seizure_psd_high_gamma_mean', \n",
    "    'left_parasagittal_chain_seizure_psd_high_gamma_std', \n",
    "    'left_parasagittal_chain_seizure_psd_high_gamma_cv', \n",
    "    'left_parasagittal_chain_seizure_psd_theta_alpha_ratio', \n",
    "    'left_parasagittal_chain_seizure_psd_delta_alpha_ratio', \n",
    "    'left_parasagittal_chain_seizure_psd_beta_ratio', \n",
    "    'left_parasagittal_chain_seizure_psd_sef50', \n",
    "    'left_parasagittal_chain_seizure_psd_sef75', \n",
    "    'left_parasagittal_chain_seizure_psd_sef90', \n",
    "    'left_parasagittal_chain_seizure_psd_sef95', \n",
    "    'left_parasagittal_chain_seizure_psd_spectral_centroid', \n",
    "    'left_parasagittal_chain_seizure_psd_spectral_spread', \n",
    "    'left_parasagittal_chain_seizure_psd_spectral_skewness', \n",
    "    'left_parasagittal_chain_seizure_psd_spectral_kurtosis', \n",
    "]\n",
    "\n",
    "right_psd_features = [\n",
    "    'right_seizure_psd_delta_mean', \n",
    "    'right_seizure_psd_delta_std', \n",
    "    'right_seizure_psd_delta_cv', \n",
    "    'right_seizure_psd_theta_mean', \n",
    "    'right_seizure_psd_theta_std', \n",
    "    'right_seizure_psd_theta_cv', \n",
    "    'right_seizure_psd_alpha_mean', \n",
    "    'right_seizure_psd_alpha_std', \n",
    "    'right_seizure_psd_alpha_cv', \n",
    "    'right_seizure_psd_low_beta_mean', \n",
    "    'right_seizure_psd_low_beta_std', \n",
    "    'right_seizure_psd_low_beta_cv', \n",
    "    'right_seizure_psd_high_beta_mean', \n",
    "    'right_seizure_psd_high_beta_std', \n",
    "    'right_seizure_psd_high_beta_cv', \n",
    "    'right_seizure_psd_gamma_mean', \n",
    "    'right_seizure_psd_gamma_std', \n",
    "    'right_seizure_psd_gamma_cv', \n",
    "    'right_seizure_psd_high_gamma_mean', \n",
    "    'right_seizure_psd_high_gamma_std', \n",
    "    'right_seizure_psd_high_gamma_cv', \n",
    "    'right_seizure_psd_theta_alpha_ratio', \n",
    "    'right_seizure_psd_delta_alpha_ratio', \n",
    "    'right_seizure_psd_beta_ratio', \n",
    "    'right_seizure_psd_sef50', # high for all\n",
    "    'right_seizure_psd_sef75', # high for all\n",
    "    'right_seizure_psd_sef90', # high for all\n",
    "    'right_seizure_psd_sef95', # high for all\n",
    "    'right_seizure_psd_spectral_centroid', \n",
    "    'right_seizure_psd_spectral_spread', \n",
    "    'right_seizure_psd_spectral_skewness', \n",
    "    'right_seizure_psd_spectral_kurtosis',\n",
    "    'right_parasagittal_chain_seizure_psd_delta_mean', \n",
    "    'right_parasagittal_chain_seizure_psd_delta_std', \n",
    "    'right_parasagittal_chain_seizure_psd_delta_cv', \n",
    "    'right_parasagittal_chain_seizure_psd_theta_mean', \n",
    "    'right_parasagittal_chain_seizure_psd_theta_std', \n",
    "    'right_parasagittal_chain_seizure_psd_theta_cv', \n",
    "    'right_parasagittal_chain_seizure_psd_alpha_mean', \n",
    "    'right_parasagittal_chain_seizure_psd_alpha_std', \n",
    "    'right_parasagittal_chain_seizure_psd_alpha_cv', \n",
    "    'right_parasagittal_chain_seizure_psd_low_beta_mean', \n",
    "    'right_parasagittal_chain_seizure_psd_low_beta_std', \n",
    "    'right_parasagittal_chain_seizure_psd_low_beta_cv', \n",
    "    'right_parasagittal_chain_seizure_psd_high_beta_mean', \n",
    "    'right_parasagittal_chain_seizure_psd_high_beta_std', \n",
    "    'right_parasagittal_chain_seizure_psd_high_beta_cv', \n",
    "    'right_parasagittal_chain_seizure_psd_gamma_mean', \n",
    "    'right_parasagittal_chain_seizure_psd_gamma_std', \n",
    "    'right_parasagittal_chain_seizure_psd_gamma_cv', \n",
    "    'right_parasagittal_chain_seizure_psd_high_gamma_mean', \n",
    "    'right_parasagittal_chain_seizure_psd_high_gamma_std', \n",
    "    'right_parasagittal_chain_seizure_psd_high_gamma_cv', \n",
    "    'right_parasagittal_chain_seizure_psd_theta_alpha_ratio', \n",
    "    'right_parasagittal_chain_seizure_psd_delta_alpha_ratio', \n",
    "    'right_parasagittal_chain_seizure_psd_beta_ratio', \n",
    "    'right_parasagittal_chain_seizure_psd_sef50', \n",
    "    'right_parasagittal_chain_seizure_psd_sef75', \n",
    "    'right_parasagittal_chain_seizure_psd_sef90', \n",
    "    'right_parasagittal_chain_seizure_psd_sef95', \n",
    "    'right_parasagittal_chain_seizure_psd_spectral_centroid', \n",
    "    'right_parasagittal_chain_seizure_psd_spectral_spread', \n",
    "    'right_parasagittal_chain_seizure_psd_spectral_skewness', \n",
    "    'right_parasagittal_chain_seizure_psd_spectral_kurtosis', \n",
    "    'right_lateral_chain_seizure_psd_delta_mean', \n",
    "    'right_lateral_chain_seizure_psd_delta_std', \n",
    "    'right_lateral_chain_seizure_psd_delta_cv', \n",
    "    'right_lateral_chain_seizure_psd_theta_mean', \n",
    "    'right_lateral_chain_seizure_psd_theta_std', \n",
    "    'right_lateral_chain_seizure_psd_theta_cv', \n",
    "    'right_lateral_chain_seizure_psd_alpha_mean', \n",
    "    'right_lateral_chain_seizure_psd_alpha_std', \n",
    "    'right_lateral_chain_seizure_psd_alpha_cv', \n",
    "    'right_lateral_chain_seizure_psd_low_beta_mean', \n",
    "    'right_lateral_chain_seizure_psd_low_beta_std', \n",
    "    'right_lateral_chain_seizure_psd_low_beta_cv', \n",
    "    'right_lateral_chain_seizure_psd_high_beta_mean', \n",
    "    'right_lateral_chain_seizure_psd_high_beta_std', \n",
    "    'right_lateral_chain_seizure_psd_high_beta_cv', \n",
    "    'right_lateral_chain_seizure_psd_gamma_mean', \n",
    "    'right_lateral_chain_seizure_psd_gamma_std', \n",
    "    'right_lateral_chain_seizure_psd_gamma_cv', \n",
    "    'right_lateral_chain_seizure_psd_high_gamma_mean', \n",
    "    'right_lateral_chain_seizure_psd_high_gamma_std', \n",
    "    'right_lateral_chain_seizure_psd_high_gamma_cv', \n",
    "    'right_lateral_chain_seizure_psd_theta_alpha_ratio', \n",
    "    'right_lateral_chain_seizure_psd_delta_alpha_ratio', \n",
    "    'right_lateral_chain_seizure_psd_beta_ratio', \n",
    "    'right_lateral_chain_seizure_psd_sef50', \n",
    "    'right_lateral_chain_seizure_psd_sef75', \n",
    "    'right_lateral_chain_seizure_psd_sef90', \n",
    "    'right_lateral_chain_seizure_psd_sef95', \n",
    "    'right_lateral_chain_seizure_psd_spectral_centroid', \n",
    "    'right_lateral_chain_seizure_psd_spectral_spread', \n",
    "    'right_lateral_chain_seizure_psd_spectral_skewness', \n",
    "    'right_lateral_chain_seizure_psd_spectral_kurtosis',  \n",
    "]\n",
    "\n",
    "midline_psd_features = [\n",
    "    'midline_seizure_psd_delta_mean', \n",
    "    'midline_seizure_psd_delta_std', \n",
    "    'midline_seizure_psd_delta_cv', \n",
    "    'midline_seizure_psd_theta_mean', \n",
    "    'midline_seizure_psd_theta_std', \n",
    "    'midline_seizure_psd_theta_cv', \n",
    "    'midline_seizure_psd_alpha_mean', \n",
    "    'midline_seizure_psd_alpha_std', \n",
    "    'midline_seizure_psd_alpha_cv', \n",
    "    'midline_seizure_psd_low_beta_mean', \n",
    "    'midline_seizure_psd_low_beta_std', \n",
    "    'midline_seizure_psd_low_beta_cv', \n",
    "    'midline_seizure_psd_high_beta_mean', \n",
    "    'midline_seizure_psd_high_beta_std', \n",
    "    'midline_seizure_psd_high_beta_cv', \n",
    "    'midline_seizure_psd_gamma_mean', \n",
    "    'midline_seizure_psd_gamma_std', \n",
    "    'midline_seizure_psd_gamma_cv', \n",
    "    'midline_seizure_psd_high_gamma_mean', \n",
    "    'midline_seizure_psd_high_gamma_std', \n",
    "    'midline_seizure_psd_high_gamma_cv', \n",
    "    'midline_seizure_psd_theta_alpha_ratio', \n",
    "    'midline_seizure_psd_delta_alpha_ratio', \n",
    "    'midline_seizure_psd_beta_ratio', \n",
    "    'midline_seizure_psd_sef50', \n",
    "    'midline_seizure_psd_sef75', \n",
    "    'midline_seizure_psd_sef90', \n",
    "    'midline_seizure_psd_sef95', \n",
    "    'midline_seizure_psd_spectral_centroid', \n",
    "    'midline_seizure_psd_spectral_spread', \n",
    "    'midline_seizure_psd_spectral_skewness', \n",
    "    'midline_seizure_psd_spectral_kurtosis',\n",
    "    'midline_chain_seizure_psd_delta_mean', \n",
    "    'midline_chain_seizure_psd_delta_std', \n",
    "    'midline_chain_seizure_psd_delta_cv', \n",
    "    'midline_chain_seizure_psd_theta_mean', \n",
    "    'midline_chain_seizure_psd_theta_std', \n",
    "    'midline_chain_seizure_psd_theta_cv', \n",
    "    'midline_chain_seizure_psd_alpha_mean', \n",
    "    'midline_chain_seizure_psd_alpha_std', \n",
    "    'midline_chain_seizure_psd_alpha_cv', \n",
    "    'midline_chain_seizure_psd_low_beta_mean', \n",
    "    'midline_chain_seizure_psd_low_beta_std', \n",
    "    'midline_chain_seizure_psd_low_beta_cv', \n",
    "    'midline_chain_seizure_psd_high_beta_mean', \n",
    "    'midline_chain_seizure_psd_high_beta_std', \n",
    "    'midline_chain_seizure_psd_high_beta_cv', \n",
    "    'midline_chain_seizure_psd_gamma_mean', \n",
    "    'midline_chain_seizure_psd_gamma_std', \n",
    "    'midline_chain_seizure_psd_gamma_cv', \n",
    "    'midline_chain_seizure_psd_high_gamma_mean', \n",
    "    'midline_chain_seizure_psd_high_gamma_std', \n",
    "    'midline_chain_seizure_psd_high_gamma_cv', \n",
    "    'midline_chain_seizure_psd_theta_alpha_ratio', \n",
    "    'midline_chain_seizure_psd_delta_alpha_ratio', \n",
    "    'midline_chain_seizure_psd_beta_ratio', \n",
    "    'midline_chain_seizure_psd_sef50', \n",
    "    'midline_chain_seizure_psd_sef75', \n",
    "    'midline_chain_seizure_psd_sef90', \n",
    "    'midline_chain_seizure_psd_sef95', \n",
    "    'midline_chain_seizure_psd_spectral_centroid', \n",
    "    'midline_chain_seizure_psd_spectral_spread', \n",
    "    'midline_chain_seizure_psd_spectral_skewness', \n",
    "    'midline_chain_seizure_psd_spectral_kurtosis',  \n",
    "]\n",
    "\n",
    "# all cross hemisphere features have high negative correlation\n",
    "cross_hemisphere_psd_features = [\n",
    "    'cross_hemisphere_seizure_psd_delta_mean', \n",
    "    'cross_hemisphere_seizure_psd_delta_std', \n",
    "    'cross_hemisphere_seizure_psd_delta_cv', \n",
    "    'cross_hemisphere_seizure_psd_theta_mean', \n",
    "    'cross_hemisphere_seizure_psd_theta_std', \n",
    "    'cross_hemisphere_seizure_psd_theta_cv', \n",
    "    'cross_hemisphere_seizure_psd_alpha_mean', \n",
    "    'cross_hemisphere_seizure_psd_alpha_std', \n",
    "    'cross_hemisphere_seizure_psd_alpha_cv', \n",
    "    'cross_hemisphere_seizure_psd_low_beta_mean', \n",
    "    'cross_hemisphere_seizure_psd_low_beta_std', \n",
    "    'cross_hemisphere_seizure_psd_low_beta_cv', \n",
    "    'cross_hemisphere_seizure_psd_high_beta_mean', \n",
    "    'cross_hemisphere_seizure_psd_high_beta_std', \n",
    "    'cross_hemisphere_seizure_psd_high_beta_cv', \n",
    "    'cross_hemisphere_seizure_psd_gamma_mean', \n",
    "    'cross_hemisphere_seizure_psd_gamma_std', \n",
    "    'cross_hemisphere_seizure_psd_gamma_cv', \n",
    "    'cross_hemisphere_seizure_psd_high_gamma_mean', \n",
    "    'cross_hemisphere_seizure_psd_high_gamma_std', \n",
    "    'cross_hemisphere_seizure_psd_high_gamma_cv', \n",
    "    'cross_hemisphere_seizure_psd_theta_alpha_ratio', \n",
    "    'cross_hemisphere_seizure_psd_delta_alpha_ratio', \n",
    "    'cross_hemisphere_seizure_psd_beta_ratio', \n",
    "    'cross_hemisphere_seizure_psd_sef50', \n",
    "    'cross_hemisphere_seizure_psd_sef75', \n",
    "    'cross_hemisphere_seizure_psd_sef90', \n",
    "    'cross_hemisphere_seizure_psd_sef95', \n",
    "    'cross_hemisphere_seizure_psd_spectral_centroid', \n",
    "    'cross_hemisphere_seizure_psd_spectral_spread', \n",
    "    'cross_hemisphere_seizure_psd_spectral_skewness', \n",
    "    'cross_hemisphere_seizure_psd_spectral_kurtosis', \n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8acfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wt features\n",
    "wt_features = [\n",
    "    'seizure_wt_level0_energy_mean', \n",
    "    'seizure_wt_level0_energy_std', \n",
    "    'seizure_wt_level0_entropy_mean', \n",
    "    'seizure_wt_level0_entropy_std', \n",
    "    'seizure_wt_level0_mean_mean', \n",
    "    'seizure_wt_level0_mean_std', # high for all\n",
    "    'seizure_wt_level0_std_mean', \n",
    "    'seizure_wt_level0_std_std', # high for all\n",
    "    'seizure_wt_level0_max_mean', \n",
    "    'seizure_wt_level0_max_std', # high for all\n",
    "    'seizure_wt_level1_energy_mean', \n",
    "    'seizure_wt_level1_energy_std', \n",
    "    'seizure_wt_level1_entropy_mean', \n",
    "    'seizure_wt_level1_entropy_std', # high for all\n",
    "    'seizure_wt_level1_mean_mean', \n",
    "    'seizure_wt_level1_mean_std', \n",
    "    'seizure_wt_level1_std_mean', \n",
    "    'seizure_wt_level1_std_std', # high for all\n",
    "    'seizure_wt_level1_max_mean', \n",
    "    'seizure_wt_level1_max_std', # high for all\n",
    "    'seizure_wt_level2_energy_mean', \n",
    "    'seizure_wt_level2_energy_std', \n",
    "    'seizure_wt_level2_entropy_mean', \n",
    "    'seizure_wt_level2_entropy_std', # high for all\n",
    "    'seizure_wt_level2_mean_mean', \n",
    "    'seizure_wt_level2_mean_std', # high for all\n",
    "    'seizure_wt_level2_std_mean', \n",
    "    'seizure_wt_level2_std_std', # high for all\n",
    "    'seizure_wt_level2_max_mean', \n",
    "    'seizure_wt_level2_max_std', # high for all\n",
    "    'seizure_wt_level3_energy_mean', \n",
    "    'seizure_wt_level3_energy_std', \n",
    "    'seizure_wt_level3_entropy_mean', \n",
    "    'seizure_wt_level3_entropy_std', # high for all\n",
    "    'seizure_wt_level3_mean_mean', # high for all\n",
    "    'seizure_wt_level3_mean_std', # high for all\n",
    "    'seizure_wt_level3_std_mean', # high for all\n",
    "    'seizure_wt_level3_std_std', # high for all\n",
    "    'seizure_wt_level3_max_mean', \n",
    "    'seizure_wt_level3_max_std', # high for all\n",
    "    'seizure_wt_level4_energy_mean', # high for all\n",
    "    'seizure_wt_level4_energy_std', \n",
    "    'seizure_wt_level4_entropy_mean', \n",
    "    'seizure_wt_level4_entropy_std', \n",
    "    'seizure_wt_level4_mean_mean', \n",
    "    'seizure_wt_level4_mean_std', # high for all\n",
    "    'seizure_wt_level4_std_mean', \n",
    "    'seizure_wt_level4_std_std', # high for all\n",
    "    'seizure_wt_level4_max_mean', \n",
    "    'seizure_wt_level4_max_std', \n",
    "    'seizure_wt_level5_energy_mean', \n",
    "    'seizure_wt_level5_energy_std', \n",
    "    'seizure_wt_level5_entropy_mean', \n",
    "    'seizure_wt_level5_entropy_std', \n",
    "    'seizure_wt_level5_mean_mean', \n",
    "    'seizure_wt_level5_mean_std', # high for all\n",
    "    'seizure_wt_level5_std_mean', \n",
    "    'seizure_wt_level5_std_std', \n",
    "    'seizure_wt_level5_max_mean', \n",
    "    'seizure_wt_level5_max_std', \n",
    "    'seizure_wt_packet_entropy', \n",
    "]\n",
    "\n",
    "time_features = [\n",
    "    'seizure_time_mean_mean', \n",
    "    'seizure_time_mean_std', # high for seizure type\n",
    "    'seizure_time_mean_max', # high for seizure type\n",
    "    'seizure_time_mean_min', \n",
    "    'seizure_time_std_mean', \n",
    "    'seizure_time_std_std', # high for all\n",
    "    'seizure_time_std_max', # high for all\n",
    "    'seizure_time_std_min', \n",
    "    'seizure_time_var_mean', \n",
    "    'seizure_time_var_std', \n",
    "    'seizure_time_var_max', \n",
    "    'seizure_time_var_min', \n",
    "    'seizure_time_skewness_mean', \n",
    "    'seizure_time_skewness_std', \n",
    "    'seizure_time_skewness_max', \n",
    "    'seizure_time_skewness_min', \n",
    "    'seizure_time_kurtosis_mean', \n",
    "    'seizure_time_kurtosis_std', \n",
    "    'seizure_time_kurtosis_max', \n",
    "    'seizure_time_kurtosis_min', \n",
    "    'seizure_time_rms_mean', \n",
    "    'seizure_time_rms_std', # high for all\n",
    "    'seizure_time_rms_max', # high for all\n",
    "    'seizure_time_rms_min', \n",
    "    'seizure_time_peak_to_peak_mean', \n",
    "    'seizure_time_peak_to_peak_std', # high for all\n",
    "    'seizure_time_peak_to_peak_max', # high for all\n",
    "    'seizure_time_peak_to_peak_min', \n",
    "    'seizure_time_zero_crossings_mean', # high for all\n",
    "    'seizure_time_zero_crossings_std', # high for all\n",
    "    'seizure_time_zero_crossings_max', # high for all\n",
    "    'seizure_time_zero_crossings_min', \n",
    "    'seizure_time_hjorth_activity_mean', \n",
    "    'seizure_time_hjorth_activity_std', \n",
    "    'seizure_time_hjorth_activity_max', \n",
    "    'seizure_time_hjorth_activity_min', \n",
    "    'seizure_time_hjorth_mobility_mean', \n",
    "    'seizure_time_hjorth_mobility_std', \n",
    "    'seizure_time_hjorth_mobility_max', \n",
    "    'seizure_time_hjorth_mobility_min', \n",
    "    'seizure_time_hjorth_complexity_mean', \n",
    "    'seizure_time_hjorth_complexity_std', # high for localization and lateralization\n",
    "    'seizure_time_hjorth_complexity_max', # high for localization and lateralization\n",
    "    'seizure_time_hjorth_complexity_min', \n",
    "    'seizure_time_line_length_mean', \n",
    "    'seizure_time_line_length_std', # high for all\n",
    "    'seizure_time_line_length_max', # high for all\n",
    "    'seizure_time_line_length_min', # high for all\n",
    "    'seizure_time_nonlinear_energy_mean', \n",
    "    'seizure_time_nonlinear_energy_std', \n",
    "    'seizure_time_nonlinear_energy_max', \n",
    "    'seizure_time_nonlinear_energy_min', \n",
    "]\n",
    "\n",
    "node_pac_features = [\n",
    "    'seizure_node_strength_mean', \n",
    "    'seizure_node_strength_std', \n",
    "    'seizure_node_strength_max', \n",
    "    'seizure_pac_theta_gamma', \n",
    "    'seizure_pac_theta_high_gamma', \n",
    "    'seizure_pac_alpha_gamma', \n",
    "    'seizure_pac_alpha_high_gamma', \n",
    "    'seizure_pac_mean', 'seizure_pac_max', \n",
    "]\n",
    "\n",
    "rythmic_features = [\n",
    "    'seizure_rhythmic_theta_power_mean', \n",
    "    'seizure_rhythmic_theta_power_std', \n",
    "    'seizure_rhythmic_delta_slow_power_mean', \n",
    "    'seizure_rhythmic_delta_slow_power_std', \n",
    "    'seizure_rhythmic_fast_power_mean', \n",
    "    'seizure_rhythmic_fast_power_std', \n",
    "    'seizure_rhythmic_spike_rate_mean', \n",
    "    'seizure_rhythmic_spike_rate_std', \n",
    "    'seizure_rhythmic_theta_delta_ratio', \n",
    "    'seizure_rhythmic_fast_theta_ratio', # high for seizure type\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc150955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagation speeds\n",
    "propagation_features = [\n",
    "    'seizure_mean_propagation_speed', # high for all\n",
    "    'seizure_median_propagation_speed', \n",
    "    'seizure_std_propagation_speed', # high for all\n",
    "    'seizure_max_propagation_speed', # high for all\n",
    "    'seizure_min_propagation_speed', \n",
    "    'seizure_num_propagation_events', \n",
    "    'seizure_earliest_onset', \n",
    "    'seizure_latest_onset', \n",
    "    'seizure_onset_spread', \n",
    "    'seizure_mean_onset_delay', \n",
    "    'seizure_max_onset_delay', \n",
    "    'seizure_mean_onset_time', \n",
    "    'seizure_peak_spread', \n",
    "    'seizure_mean_peak_time', \n",
    "]\n",
    "\n",
    "# LSTM Features\n",
    "lstm_features = [\n",
    "    'lstm_output_mean', \n",
    "    'lstm_output_std', \n",
    "    'lstm_output_max', \n",
    "    'lstm_output_min', \n",
    "    'lstm_hidden_mean', \n",
    "    'lstm_hidden_std', \n",
    "    'lstm_temporal_trend', \n",
    "    'lstm_forward_backward_diff', \n",
    "    'lstm_early_energy', \n",
    "    'lstm_middle_energy', \n",
    "    'lstm_late_energy', \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5835ce",
   "metadata": {},
   "source": [
    "### Post-Ictal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db79014",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_ictal_features = [\n",
    "    'post_ictal_psd_delta_mean', \n",
    "    'post_ictal_psd_delta_std', \n",
    "    'post_ictal_psd_delta_cv', \n",
    "    'post_ictal_psd_theta_mean', \n",
    "    'post_ictal_psd_theta_std', \n",
    "    'post_ictal_psd_theta_cv', \n",
    "    'post_ictal_psd_alpha_mean', \n",
    "    'post_ictal_psd_alpha_std', \n",
    "    'post_ictal_psd_alpha_cv', \n",
    "    'post_ictal_psd_low_beta_mean', \n",
    "    'post_ictal_psd_low_beta_std', \n",
    "    'post_ictal_psd_low_beta_cv', \n",
    "    'post_ictal_psd_high_beta_mean', \n",
    "    'post_ictal_psd_high_beta_std', \n",
    "    'post_ictal_psd_high_beta_cv', \n",
    "    'post_ictal_psd_gamma_mean', \n",
    "    'post_ictal_psd_gamma_std', \n",
    "    'post_ictal_psd_gamma_cv', \n",
    "    'post_ictal_psd_theta_alpha_ratio', \n",
    "    'post_ictal_psd_delta_alpha_ratio', \n",
    "    'post_ictal_psd_beta_ratio', \n",
    "    'post_ictal_psd_sef50', \n",
    "    'post_ictal_psd_sef75', \n",
    "    'post_ictal_psd_sef90', \n",
    "    'post_ictal_psd_sef95', \n",
    "    'post_ictal_psd_spectral_centroid', \n",
    "    'post_ictal_psd_spectral_spread', \n",
    "    'post_ictal_psd_spectral_skewness', \n",
    "    'post_ictal_psd_spectral_kurtosis', \n",
    "    'frontal_post_ictal_psd_delta_mean', \n",
    "    'frontal_post_ictal_psd_delta_std', \n",
    "    'frontal_post_ictal_psd_delta_cv', \n",
    "    'frontal_post_ictal_psd_theta_mean', \n",
    "    'frontal_post_ictal_psd_theta_std', \n",
    "    'frontal_post_ictal_psd_theta_cv', \n",
    "    'frontal_post_ictal_psd_alpha_mean', \n",
    "    'frontal_post_ictal_psd_alpha_std', \n",
    "    'frontal_post_ictal_psd_alpha_cv', \n",
    "    'frontal_post_ictal_psd_low_beta_mean', \n",
    "    'frontal_post_ictal_psd_low_beta_std', \n",
    "    'frontal_post_ictal_psd_low_beta_cv', \n",
    "    'frontal_post_ictal_psd_high_beta_mean', \n",
    "    'frontal_post_ictal_psd_high_beta_std', \n",
    "    'frontal_post_ictal_psd_high_beta_cv', \n",
    "    'frontal_post_ictal_psd_gamma_mean', \n",
    "    'frontal_post_ictal_psd_gamma_std', \n",
    "    'frontal_post_ictal_psd_gamma_cv', \n",
    "    'frontal_post_ictal_psd_high_gamma_mean', \n",
    "    'frontal_post_ictal_psd_high_gamma_std', \n",
    "    'frontal_post_ictal_psd_high_gamma_cv', \n",
    "    'frontal_post_ictal_psd_theta_alpha_ratio', \n",
    "    'frontal_post_ictal_psd_delta_alpha_ratio', \n",
    "    'frontal_post_ictal_psd_beta_ratio', \n",
    "    'frontal_post_ictal_psd_sef50', \n",
    "    'frontal_post_ictal_psd_sef75', \n",
    "    'frontal_post_ictal_psd_sef90', \n",
    "    'frontal_post_ictal_psd_sef95', \n",
    "    'frontal_post_ictal_psd_spectral_centroid', \n",
    "    'frontal_post_ictal_psd_spectral_spread', \n",
    "    'frontal_post_ictal_psd_spectral_skewness', \n",
    "    'frontal_post_ictal_psd_spectral_kurtosis', \n",
    "    'temporal_post_ictal_psd_delta_mean', \n",
    "    'temporal_post_ictal_psd_delta_std', \n",
    "    'temporal_post_ictal_psd_delta_cv', \n",
    "    'temporal_post_ictal_psd_theta_mean', \n",
    "    'temporal_post_ictal_psd_theta_std', \n",
    "    'temporal_post_ictal_psd_theta_cv', \n",
    "    'temporal_post_ictal_psd_alpha_mean', \n",
    "    'temporal_post_ictal_psd_alpha_std', \n",
    "    'temporal_post_ictal_psd_alpha_cv', \n",
    "    'temporal_post_ictal_psd_low_beta_mean', \n",
    "    'temporal_post_ictal_psd_low_beta_std', \n",
    "    'temporal_post_ictal_psd_low_beta_cv', \n",
    "    'temporal_post_ictal_psd_high_beta_mean', \n",
    "    'temporal_post_ictal_psd_high_beta_std', \n",
    "    'temporal_post_ictal_psd_high_beta_cv', \n",
    "    'temporal_post_ictal_psd_gamma_mean', \n",
    "    'temporal_post_ictal_psd_gamma_std', \n",
    "    'temporal_post_ictal_psd_gamma_cv', \n",
    "    'temporal_post_ictal_psd_high_gamma_mean', \n",
    "    'temporal_post_ictal_psd_high_gamma_std', \n",
    "    'temporal_post_ictal_psd_high_gamma_cv', \n",
    "    'temporal_post_ictal_psd_theta_alpha_ratio', \n",
    "    'temporal_post_ictal_psd_delta_alpha_ratio', \n",
    "    'temporal_post_ictal_psd_beta_ratio', \n",
    "    'temporal_post_ictal_psd_sef50', \n",
    "    'temporal_post_ictal_psd_sef75', \n",
    "    'temporal_post_ictal_psd_sef90', \n",
    "    'temporal_post_ictal_psd_sef95', \n",
    "    'temporal_post_ictal_psd_spectral_centroid', \n",
    "    'temporal_post_ictal_psd_spectral_spread', \n",
    "    'temporal_post_ictal_psd_spectral_skewness', \n",
    "    'temporal_post_ictal_psd_spectral_kurtosis', \n",
    "    'frontotemporal_post_ictal_psd_delta_mean', \n",
    "    'frontotemporal_post_ictal_psd_delta_std', \n",
    "    'frontotemporal_post_ictal_psd_delta_cv', \n",
    "    'frontotemporal_post_ictal_psd_theta_mean', \n",
    "    'frontotemporal_post_ictal_psd_theta_std', \n",
    "    'frontotemporal_post_ictal_psd_theta_cv', \n",
    "    'frontotemporal_post_ictal_psd_alpha_mean', \n",
    "    'frontotemporal_post_ictal_psd_alpha_std', \n",
    "    'frontotemporal_post_ictal_psd_alpha_cv', \n",
    "    'frontotemporal_post_ictal_psd_low_beta_mean', \n",
    "    'frontotemporal_post_ictal_psd_low_beta_std', \n",
    "    'frontotemporal_post_ictal_psd_low_beta_cv', \n",
    "    'frontotemporal_post_ictal_psd_high_beta_mean', \n",
    "    'frontotemporal_post_ictal_psd_high_beta_std', \n",
    "    'frontotemporal_post_ictal_psd_high_beta_cv', \n",
    "    'frontotemporal_post_ictal_psd_gamma_mean', \n",
    "    'frontotemporal_post_ictal_psd_gamma_std', \n",
    "    'frontotemporal_post_ictal_psd_gamma_cv', \n",
    "    'frontotemporal_post_ictal_psd_high_gamma_mean', \n",
    "    'frontotemporal_post_ictal_psd_high_gamma_std', \n",
    "    'frontotemporal_post_ictal_psd_high_gamma_cv', \n",
    "    'frontotemporal_post_ictal_psd_theta_alpha_ratio', \n",
    "    'frontotemporal_post_ictal_psd_delta_alpha_ratio', \n",
    "    'frontotemporal_post_ictal_psd_beta_ratio', \n",
    "    'frontotemporal_post_ictal_psd_sef50', \n",
    "    'frontotemporal_post_ictal_psd_sef75', \n",
    "    'frontotemporal_post_ictal_psd_sef90', \n",
    "    'frontotemporal_post_ictal_psd_sef95', \n",
    "    'frontotemporal_post_ictal_psd_spectral_centroid', \n",
    "    'frontotemporal_post_ictal_psd_spectral_spread', \n",
    "    'frontotemporal_post_ictal_psd_spectral_skewness', \n",
    "    'frontotemporal_post_ictal_psd_spectral_kurtosis', \n",
    "    'parietal_post_ictal_psd_delta_mean', \n",
    "    'parietal_post_ictal_psd_delta_std', \n",
    "    'parietal_post_ictal_psd_delta_cv', \n",
    "    'parietal_post_ictal_psd_theta_mean', \n",
    "    'parietal_post_ictal_psd_theta_std', \n",
    "    'parietal_post_ictal_psd_theta_cv', \n",
    "    'parietal_post_ictal_psd_alpha_mean', \n",
    "    'parietal_post_ictal_psd_alpha_std', \n",
    "    'parietal_post_ictal_psd_alpha_cv', \n",
    "    'parietal_post_ictal_psd_low_beta_mean', \n",
    "    'parietal_post_ictal_psd_low_beta_std', \n",
    "    'parietal_post_ictal_psd_low_beta_cv', \n",
    "    'parietal_post_ictal_psd_high_beta_mean', \n",
    "    'parietal_post_ictal_psd_high_beta_std', \n",
    "    'parietal_post_ictal_psd_high_beta_cv', \n",
    "    'parietal_post_ictal_psd_gamma_mean', \n",
    "    'parietal_post_ictal_psd_gamma_std', \n",
    "    'parietal_post_ictal_psd_gamma_cv', \n",
    "    'parietal_post_ictal_psd_high_gamma_mean', \n",
    "    'parietal_post_ictal_psd_high_gamma_std', \n",
    "    'parietal_post_ictal_psd_high_gamma_cv', \n",
    "    'parietal_post_ictal_psd_theta_alpha_ratio', \n",
    "    'parietal_post_ictal_psd_delta_alpha_ratio', \n",
    "    'parietal_post_ictal_psd_beta_ratio', \n",
    "    'parietal_post_ictal_psd_sef50', \n",
    "    'parietal_post_ictal_psd_sef75', \n",
    "    'parietal_post_ictal_psd_sef90', \n",
    "    'parietal_post_ictal_psd_sef95', \n",
    "    'parietal_post_ictal_psd_spectral_centroid', \n",
    "    'parietal_post_ictal_psd_spectral_spread', \n",
    "    'parietal_post_ictal_psd_spectral_skewness', \n",
    "    'parietal_post_ictal_psd_spectral_kurtosis', \n",
    "    'occipital_post_ictal_psd_delta_mean', \n",
    "    'occipital_post_ictal_psd_delta_std', \n",
    "    'occipital_post_ictal_psd_delta_cv', \n",
    "    'occipital_post_ictal_psd_theta_mean', \n",
    "    'occipital_post_ictal_psd_theta_std', \n",
    "    'occipital_post_ictal_psd_theta_cv', \n",
    "    'occipital_post_ictal_psd_alpha_mean', \n",
    "    'occipital_post_ictal_psd_alpha_std', \n",
    "    'occipital_post_ictal_psd_alpha_cv', \n",
    "    'occipital_post_ictal_psd_low_beta_mean', \n",
    "    'occipital_post_ictal_psd_low_beta_std', \n",
    "    'occipital_post_ictal_psd_low_beta_cv', \n",
    "    'occipital_post_ictal_psd_high_beta_mean', \n",
    "    'occipital_post_ictal_psd_high_beta_std', \n",
    "    'occipital_post_ictal_psd_high_beta_cv', \n",
    "    'occipital_post_ictal_psd_gamma_mean', \n",
    "    'occipital_post_ictal_psd_gamma_std', \n",
    "    'occipital_post_ictal_psd_gamma_cv', \n",
    "    'occipital_post_ictal_psd_high_gamma_mean', \n",
    "    'occipital_post_ictal_psd_high_gamma_std', \n",
    "    'occipital_post_ictal_psd_high_gamma_cv', \n",
    "    'occipital_post_ictal_psd_theta_alpha_ratio', \n",
    "    'occipital_post_ictal_psd_delta_alpha_ratio', \n",
    "    'occipital_post_ictal_psd_beta_ratio', \n",
    "    'occipital_post_ictal_psd_sef50', \n",
    "    'occipital_post_ictal_psd_sef75', \n",
    "    'occipital_post_ictal_psd_sef90', \n",
    "    'occipital_post_ictal_psd_sef95', \n",
    "    'occipital_post_ictal_psd_spectral_centroid', \n",
    "    'occipital_post_ictal_psd_spectral_spread', \n",
    "    'occipital_post_ictal_psd_spectral_skewness', \n",
    "    'occipital_post_ictal_psd_spectral_kurtosis', \n",
    "    'central_post_ictal_psd_delta_mean', \n",
    "    'central_post_ictal_psd_delta_std', \n",
    "    'central_post_ictal_psd_delta_cv', \n",
    "    'central_post_ictal_psd_theta_mean', \n",
    "    'central_post_ictal_psd_theta_std', \n",
    "    'central_post_ictal_psd_theta_cv', \n",
    "    'central_post_ictal_psd_alpha_mean', \n",
    "    'central_post_ictal_psd_alpha_std', \n",
    "    'central_post_ictal_psd_alpha_cv', \n",
    "    'central_post_ictal_psd_low_beta_mean', \n",
    "    'central_post_ictal_psd_low_beta_std', \n",
    "    'central_post_ictal_psd_low_beta_cv', \n",
    "    'central_post_ictal_psd_high_beta_mean', \n",
    "    'central_post_ictal_psd_high_beta_std', \n",
    "    'central_post_ictal_psd_high_beta_cv', \n",
    "    'central_post_ictal_psd_gamma_mean', \n",
    "    'central_post_ictal_psd_gamma_std', \n",
    "    'central_post_ictal_psd_gamma_cv', \n",
    "    'central_post_ictal_psd_high_gamma_mean', \n",
    "    'central_post_ictal_psd_high_gamma_std', \n",
    "    'central_post_ictal_psd_high_gamma_cv', \n",
    "    'central_post_ictal_psd_theta_alpha_ratio', \n",
    "    'central_post_ictal_psd_delta_alpha_ratio', \n",
    "    'central_post_ictal_psd_beta_ratio', \n",
    "    'central_post_ictal_psd_sef50', \n",
    "    'central_post_ictal_psd_sef75', \n",
    "    'central_post_ictal_psd_sef90', \n",
    "    'central_post_ictal_psd_sef95', \n",
    "    'central_post_ictal_psd_spectral_centroid', \n",
    "    'central_post_ictal_psd_spectral_spread', \n",
    "    'central_post_ictal_psd_spectral_skewness', \n",
    "    'central_post_ictal_psd_spectral_kurtosis', \n",
    "    'left_post_ictal_psd_delta_mean', \n",
    "    'left_post_ictal_psd_delta_std', \n",
    "    'left_post_ictal_psd_delta_cv', \n",
    "    'left_post_ictal_psd_theta_mean', \n",
    "    'left_post_ictal_psd_theta_std', \n",
    "    'left_post_ictal_psd_theta_cv', \n",
    "    'left_post_ictal_psd_alpha_mean', \n",
    "    'left_post_ictal_psd_alpha_std', \n",
    "    'left_post_ictal_psd_alpha_cv', \n",
    "    'left_post_ictal_psd_low_beta_mean', \n",
    "    'left_post_ictal_psd_low_beta_std', \n",
    "    'left_post_ictal_psd_low_beta_cv', \n",
    "    'left_post_ictal_psd_high_beta_mean', \n",
    "    'left_post_ictal_psd_high_beta_std', \n",
    "    'left_post_ictal_psd_high_beta_cv', \n",
    "    'left_post_ictal_psd_gamma_mean', \n",
    "    'left_post_ictal_psd_gamma_std', \n",
    "    'left_post_ictal_psd_gamma_cv', \n",
    "    'left_post_ictal_psd_high_gamma_mean', \n",
    "    'left_post_ictal_psd_high_gamma_std', \n",
    "    'left_post_ictal_psd_high_gamma_cv', \n",
    "    'left_post_ictal_psd_theta_alpha_ratio', \n",
    "    'left_post_ictal_psd_delta_alpha_ratio', \n",
    "    'left_post_ictal_psd_beta_ratio', \n",
    "    'left_post_ictal_psd_sef50', \n",
    "    'left_post_ictal_psd_sef75', \n",
    "    'left_post_ictal_psd_sef90', \n",
    "    'left_post_ictal_psd_sef95', \n",
    "    'left_post_ictal_psd_spectral_centroid', \n",
    "    'left_post_ictal_psd_spectral_spread', \n",
    "    'left_post_ictal_psd_spectral_skewness', \n",
    "    'left_post_ictal_psd_spectral_kurtosis', \n",
    "    'right_post_ictal_psd_delta_mean', \n",
    "    'right_post_ictal_psd_delta_std', \n",
    "    'right_post_ictal_psd_delta_cv', \n",
    "    'right_post_ictal_psd_theta_mean', \n",
    "    'right_post_ictal_psd_theta_std', \n",
    "    'right_post_ictal_psd_theta_cv', \n",
    "    'right_post_ictal_psd_alpha_mean', \n",
    "    'right_post_ictal_psd_alpha_std', \n",
    "    'right_post_ictal_psd_alpha_cv', \n",
    "    'right_post_ictal_psd_low_beta_mean', \n",
    "    'right_post_ictal_psd_low_beta_std', \n",
    "    'right_post_ictal_psd_low_beta_cv', \n",
    "    'right_post_ictal_psd_high_beta_mean', \n",
    "    'right_post_ictal_psd_high_beta_std', \n",
    "    'right_post_ictal_psd_high_beta_cv', \n",
    "    'right_post_ictal_psd_gamma_mean', \n",
    "    'right_post_ictal_psd_gamma_std', \n",
    "    'right_post_ictal_psd_gamma_cv', \n",
    "    'right_post_ictal_psd_high_gamma_mean', \n",
    "    'right_post_ictal_psd_high_gamma_std', \n",
    "    'right_post_ictal_psd_high_gamma_cv', \n",
    "    'right_post_ictal_psd_theta_alpha_ratio', \n",
    "    'right_post_ictal_psd_delta_alpha_ratio', \n",
    "    'right_post_ictal_psd_beta_ratio', \n",
    "    'right_post_ictal_psd_sef50', \n",
    "    'right_post_ictal_psd_sef75', \n",
    "    'right_post_ictal_psd_sef90', \n",
    "    'right_post_ictal_psd_sef95', \n",
    "    'right_post_ictal_psd_spectral_centroid', \n",
    "    'right_post_ictal_psd_spectral_spread', \n",
    "    'right_post_ictal_psd_spectral_skewness', \n",
    "    'right_post_ictal_psd_spectral_kurtosis', \n",
    "    'midline_post_ictal_psd_delta_mean', \n",
    "    'midline_post_ictal_psd_delta_std', \n",
    "    'midline_post_ictal_psd_delta_cv', \n",
    "    'midline_post_ictal_psd_theta_mean', \n",
    "    'midline_post_ictal_psd_theta_std', \n",
    "    'midline_post_ictal_psd_theta_cv', \n",
    "    'midline_post_ictal_psd_alpha_mean', \n",
    "    'midline_post_ictal_psd_alpha_std', \n",
    "    'midline_post_ictal_psd_alpha_cv', \n",
    "    'midline_post_ictal_psd_low_beta_mean', \n",
    "    'midline_post_ictal_psd_low_beta_std', \n",
    "    'midline_post_ictal_psd_low_beta_cv', \n",
    "    'midline_post_ictal_psd_high_beta_mean', \n",
    "    'midline_post_ictal_psd_high_beta_std', \n",
    "    'midline_post_ictal_psd_high_beta_cv', \n",
    "    'midline_post_ictal_psd_gamma_mean', \n",
    "    'midline_post_ictal_psd_gamma_std', \n",
    "    'midline_post_ictal_psd_gamma_cv', \n",
    "    'midline_post_ictal_psd_high_gamma_mean', \n",
    "    'midline_post_ictal_psd_high_gamma_std', \n",
    "    'midline_post_ictal_psd_high_gamma_cv', \n",
    "    'midline_post_ictal_psd_theta_alpha_ratio', \n",
    "    'midline_post_ictal_psd_delta_alpha_ratio', \n",
    "    'midline_post_ictal_psd_beta_ratio', \n",
    "    'midline_post_ictal_psd_sef50', \n",
    "    'midline_post_ictal_psd_sef75', \n",
    "    'midline_post_ictal_psd_sef90', \n",
    "    'midline_post_ictal_psd_sef95', \n",
    "    'midline_post_ictal_psd_spectral_centroid', \n",
    "    'midline_post_ictal_psd_spectral_spread', \n",
    "    'midline_post_ictal_psd_spectral_skewness', \n",
    "    'midline_post_ictal_psd_spectral_kurtosis', \n",
    "    'cross_hemisphere_post_ictal_psd_delta_mean', \n",
    "    'cross_hemisphere_post_ictal_psd_delta_std', \n",
    "    'cross_hemisphere_post_ictal_psd_delta_cv', \n",
    "    'cross_hemisphere_post_ictal_psd_theta_mean', \n",
    "    'cross_hemisphere_post_ictal_psd_theta_std', \n",
    "    'cross_hemisphere_post_ictal_psd_theta_cv', \n",
    "    'cross_hemisphere_post_ictal_psd_alpha_mean', \n",
    "    'cross_hemisphere_post_ictal_psd_alpha_std', \n",
    "    'cross_hemisphere_post_ictal_psd_alpha_cv', \n",
    "    'cross_hemisphere_post_ictal_psd_low_beta_mean', \n",
    "    'cross_hemisphere_post_ictal_psd_low_beta_std', \n",
    "    'cross_hemisphere_post_ictal_psd_low_beta_cv', \n",
    "    'cross_hemisphere_post_ictal_psd_high_beta_mean', \n",
    "    'cross_hemisphere_post_ictal_psd_high_beta_std', \n",
    "    'cross_hemisphere_post_ictal_psd_high_beta_cv', \n",
    "    'cross_hemisphere_post_ictal_psd_gamma_mean', \n",
    "    'cross_hemisphere_post_ictal_psd_gamma_std', \n",
    "    'cross_hemisphere_post_ictal_psd_gamma_cv', \n",
    "    'cross_hemisphere_post_ictal_psd_high_gamma_mean', \n",
    "    'cross_hemisphere_post_ictal_psd_high_gamma_std', \n",
    "    'cross_hemisphere_post_ictal_psd_high_gamma_cv', \n",
    "    'cross_hemisphere_post_ictal_psd_theta_alpha_ratio', \n",
    "    'cross_hemisphere_post_ictal_psd_delta_alpha_ratio', \n",
    "    'cross_hemisphere_post_ictal_psd_beta_ratio', \n",
    "    'cross_hemisphere_post_ictal_psd_sef50', \n",
    "    'cross_hemisphere_post_ictal_psd_sef75', \n",
    "    'cross_hemisphere_post_ictal_psd_sef90', \n",
    "    'cross_hemisphere_post_ictal_psd_sef95', \n",
    "    'cross_hemisphere_post_ictal_psd_spectral_centroid', \n",
    "    'cross_hemisphere_post_ictal_psd_spectral_spread', \n",
    "    'cross_hemisphere_post_ictal_psd_spectral_skewness', \n",
    "    'cross_hemisphere_post_ictal_psd_spectral_kurtosis', \n",
    "    'left_lateral_chain_post_ictal_psd_delta_mean', \n",
    "    'left_lateral_chain_post_ictal_psd_delta_std', \n",
    "    'left_lateral_chain_post_ictal_psd_delta_cv', \n",
    "    'left_lateral_chain_post_ictal_psd_theta_mean', \n",
    "    'left_lateral_chain_post_ictal_psd_theta_std', \n",
    "    'left_lateral_chain_post_ictal_psd_theta_cv', \n",
    "    'left_lateral_chain_post_ictal_psd_alpha_mean', \n",
    "    'left_lateral_chain_post_ictal_psd_alpha_std', \n",
    "    'left_lateral_chain_post_ictal_psd_alpha_cv', \n",
    "    'left_lateral_chain_post_ictal_psd_low_beta_mean', \n",
    "    'left_lateral_chain_post_ictal_psd_low_beta_std', \n",
    "    'left_lateral_chain_post_ictal_psd_low_beta_cv', \n",
    "    'left_lateral_chain_post_ictal_psd_high_beta_mean', \n",
    "    'left_lateral_chain_post_ictal_psd_high_beta_std', \n",
    "    'left_lateral_chain_post_ictal_psd_high_beta_cv', \n",
    "    'left_lateral_chain_post_ictal_psd_gamma_mean', \n",
    "    'left_lateral_chain_post_ictal_psd_gamma_std', \n",
    "    'left_lateral_chain_post_ictal_psd_gamma_cv', \n",
    "    'left_lateral_chain_post_ictal_psd_high_gamma_mean', \n",
    "    'left_lateral_chain_post_ictal_psd_high_gamma_std', \n",
    "    'left_lateral_chain_post_ictal_psd_high_gamma_cv', \n",
    "    'left_lateral_chain_post_ictal_psd_theta_alpha_ratio', \n",
    "    'left_lateral_chain_post_ictal_psd_delta_alpha_ratio', \n",
    "    'left_lateral_chain_post_ictal_psd_beta_ratio', \n",
    "    'left_lateral_chain_post_ictal_psd_sef50', \n",
    "    'left_lateral_chain_post_ictal_psd_sef75', \n",
    "    'left_lateral_chain_post_ictal_psd_sef90', \n",
    "    'left_lateral_chain_post_ictal_psd_sef95', \n",
    "    'left_lateral_chain_post_ictal_psd_spectral_centroid', \n",
    "    'left_lateral_chain_post_ictal_psd_spectral_spread', \n",
    "    'left_lateral_chain_post_ictal_psd_spectral_skewness', \n",
    "    'left_lateral_chain_post_ictal_psd_spectral_kurtosis', \n",
    "    'left_parasagittal_chain_post_ictal_psd_delta_mean', \n",
    "    'left_parasagittal_chain_post_ictal_psd_delta_std', \n",
    "    'left_parasagittal_chain_post_ictal_psd_delta_cv', \n",
    "    'left_parasagittal_chain_post_ictal_psd_theta_mean', \n",
    "    'left_parasagittal_chain_post_ictal_psd_theta_std', \n",
    "    'left_parasagittal_chain_post_ictal_psd_theta_cv', \n",
    "    'left_parasagittal_chain_post_ictal_psd_alpha_mean', \n",
    "    'left_parasagittal_chain_post_ictal_psd_alpha_std', \n",
    "    'left_parasagittal_chain_post_ictal_psd_alpha_cv', \n",
    "    'left_parasagittal_chain_post_ictal_psd_low_beta_mean', \n",
    "    'left_parasagittal_chain_post_ictal_psd_low_beta_std', \n",
    "    'left_parasagittal_chain_post_ictal_psd_low_beta_cv', \n",
    "    'left_parasagittal_chain_post_ictal_psd_high_beta_mean', \n",
    "    'left_parasagittal_chain_post_ictal_psd_high_beta_std', \n",
    "    'left_parasagittal_chain_post_ictal_psd_high_beta_cv', \n",
    "    'left_parasagittal_chain_post_ictal_psd_gamma_mean', \n",
    "    'left_parasagittal_chain_post_ictal_psd_gamma_std', \n",
    "    'left_parasagittal_chain_post_ictal_psd_gamma_cv', \n",
    "    'left_parasagittal_chain_post_ictal_psd_high_gamma_mean', \n",
    "    'left_parasagittal_chain_post_ictal_psd_high_gamma_std', \n",
    "    'left_parasagittal_chain_post_ictal_psd_high_gamma_cv', \n",
    "    'left_parasagittal_chain_post_ictal_psd_theta_alpha_ratio', \n",
    "    'left_parasagittal_chain_post_ictal_psd_delta_alpha_ratio', \n",
    "    'left_parasagittal_chain_post_ictal_psd_beta_ratio', \n",
    "    'left_parasagittal_chain_post_ictal_psd_sef50', \n",
    "    'left_parasagittal_chain_post_ictal_psd_sef75', \n",
    "    'left_parasagittal_chain_post_ictal_psd_sef90', \n",
    "    'left_parasagittal_chain_post_ictal_psd_sef95', \n",
    "    'left_parasagittal_chain_post_ictal_psd_spectral_centroid', \n",
    "    'left_parasagittal_chain_post_ictal_psd_spectral_spread', \n",
    "    'left_parasagittal_chain_post_ictal_psd_spectral_skewness', \n",
    "    'left_parasagittal_chain_post_ictal_psd_spectral_kurtosis', \n",
    "    'right_parasagittal_chain_post_ictal_psd_delta_mean', \n",
    "    'right_parasagittal_chain_post_ictal_psd_delta_std', \n",
    "    'right_parasagittal_chain_post_ictal_psd_delta_cv', \n",
    "    'right_parasagittal_chain_post_ictal_psd_theta_mean', \n",
    "    'right_parasagittal_chain_post_ictal_psd_theta_std', \n",
    "    'right_parasagittal_chain_post_ictal_psd_theta_cv', \n",
    "    'right_parasagittal_chain_post_ictal_psd_alpha_mean', \n",
    "    'right_parasagittal_chain_post_ictal_psd_alpha_std', \n",
    "    'right_parasagittal_chain_post_ictal_psd_alpha_cv', \n",
    "    'right_parasagittal_chain_post_ictal_psd_low_beta_mean', \n",
    "    'right_parasagittal_chain_post_ictal_psd_low_beta_std', \n",
    "    'right_parasagittal_chain_post_ictal_psd_low_beta_cv', \n",
    "    'right_parasagittal_chain_post_ictal_psd_high_beta_mean', \n",
    "    'right_parasagittal_chain_post_ictal_psd_high_beta_std', \n",
    "    'right_parasagittal_chain_post_ictal_psd_high_beta_cv', \n",
    "    'right_parasagittal_chain_post_ictal_psd_gamma_mean', \n",
    "    'right_parasagittal_chain_post_ictal_psd_gamma_std', \n",
    "    'right_parasagittal_chain_post_ictal_psd_gamma_cv', \n",
    "    'right_parasagittal_chain_post_ictal_psd_high_gamma_mean', \n",
    "    'right_parasagittal_chain_post_ictal_psd_high_gamma_std', \n",
    "    'right_parasagittal_chain_post_ictal_psd_high_gamma_cv', \n",
    "    'right_parasagittal_chain_post_ictal_psd_theta_alpha_ratio', \n",
    "    'right_parasagittal_chain_post_ictal_psd_delta_alpha_ratio', \n",
    "    'right_parasagittal_chain_post_ictal_psd_beta_ratio', \n",
    "    'right_parasagittal_chain_post_ictal_psd_sef50', \n",
    "    'right_parasagittal_chain_post_ictal_psd_sef75', \n",
    "    'right_parasagittal_chain_post_ictal_psd_sef90', \n",
    "    'right_parasagittal_chain_post_ictal_psd_sef95', \n",
    "    'right_parasagittal_chain_post_ictal_psd_spectral_centroid', \n",
    "    'right_parasagittal_chain_post_ictal_psd_spectral_spread', \n",
    "    'right_parasagittal_chain_post_ictal_psd_spectral_skewness', \n",
    "    'right_parasagittal_chain_post_ictal_psd_spectral_kurtosis', \n",
    "    'right_lateral_chain_post_ictal_psd_delta_mean', \n",
    "    'right_lateral_chain_post_ictal_psd_delta_std', \n",
    "    'right_lateral_chain_post_ictal_psd_delta_cv', \n",
    "    'right_lateral_chain_post_ictal_psd_theta_mean', \n",
    "    'right_lateral_chain_post_ictal_psd_theta_std', \n",
    "    'right_lateral_chain_post_ictal_psd_theta_cv', \n",
    "    'right_lateral_chain_post_ictal_psd_alpha_mean', \n",
    "    'right_lateral_chain_post_ictal_psd_alpha_std', \n",
    "    'right_lateral_chain_post_ictal_psd_alpha_cv', \n",
    "    'right_lateral_chain_post_ictal_psd_low_beta_mean', \n",
    "    'right_lateral_chain_post_ictal_psd_low_beta_std', \n",
    "    'right_lateral_chain_post_ictal_psd_low_beta_cv', \n",
    "    'right_lateral_chain_post_ictal_psd_high_beta_mean', \n",
    "    'right_lateral_chain_post_ictal_psd_high_beta_std', \n",
    "    'right_lateral_chain_post_ictal_psd_high_beta_cv', \n",
    "    'right_lateral_chain_post_ictal_psd_gamma_mean', \n",
    "    'right_lateral_chain_post_ictal_psd_gamma_std', \n",
    "    'right_lateral_chain_post_ictal_psd_gamma_cv', \n",
    "    'right_lateral_chain_post_ictal_psd_high_gamma_mean', \n",
    "    'right_lateral_chain_post_ictal_psd_high_gamma_std', \n",
    "    'right_lateral_chain_post_ictal_psd_high_gamma_cv', \n",
    "    'right_lateral_chain_post_ictal_psd_theta_alpha_ratio', \n",
    "    'right_lateral_chain_post_ictal_psd_delta_alpha_ratio', \n",
    "    'right_lateral_chain_post_ictal_psd_beta_ratio', \n",
    "    'right_lateral_chain_post_ictal_psd_sef50', \n",
    "    'right_lateral_chain_post_ictal_psd_sef75', \n",
    "    'right_lateral_chain_post_ictal_psd_sef90', \n",
    "    'right_lateral_chain_post_ictal_psd_sef95', \n",
    "    'right_lateral_chain_post_ictal_psd_spectral_centroid', \n",
    "    'right_lateral_chain_post_ictal_psd_spectral_spread', \n",
    "    'right_lateral_chain_post_ictal_psd_spectral_skewness', \n",
    "    'right_lateral_chain_post_ictal_psd_spectral_kurtosis', \n",
    "    'midline_chain_post_ictal_psd_delta_mean', \n",
    "    'midline_chain_post_ictal_psd_delta_std', \n",
    "    'midline_chain_post_ictal_psd_delta_cv', \n",
    "    'midline_chain_post_ictal_psd_theta_mean', \n",
    "    'midline_chain_post_ictal_psd_theta_std', \n",
    "    'midline_chain_post_ictal_psd_theta_cv', \n",
    "    'midline_chain_post_ictal_psd_alpha_mean', \n",
    "    'midline_chain_post_ictal_psd_alpha_std', \n",
    "    'midline_chain_post_ictal_psd_alpha_cv', \n",
    "    'midline_chain_post_ictal_psd_low_beta_mean', \n",
    "    'midline_chain_post_ictal_psd_low_beta_std', \n",
    "    'midline_chain_post_ictal_psd_low_beta_cv', \n",
    "    'midline_chain_post_ictal_psd_high_beta_mean', \n",
    "    'midline_chain_post_ictal_psd_high_beta_std', \n",
    "    'midline_chain_post_ictal_psd_high_beta_cv', \n",
    "    'midline_chain_post_ictal_psd_gamma_mean', \n",
    "    'midline_chain_post_ictal_psd_gamma_std', \n",
    "    'midline_chain_post_ictal_psd_gamma_cv', \n",
    "    'midline_chain_post_ictal_psd_high_gamma_mean', \n",
    "    'midline_chain_post_ictal_psd_high_gamma_std', \n",
    "    'midline_chain_post_ictal_psd_high_gamma_cv', \n",
    "    'midline_chain_post_ictal_psd_theta_alpha_ratio', \n",
    "    'midline_chain_post_ictal_psd_delta_alpha_ratio', \n",
    "    'midline_chain_post_ictal_psd_beta_ratio', \n",
    "    'midline_chain_post_ictal_psd_sef50', \n",
    "    'midline_chain_post_ictal_psd_sef75', \n",
    "    'midline_chain_post_ictal_psd_sef90', \n",
    "    'midline_chain_post_ictal_psd_sef95', \n",
    "    'midline_chain_post_ictal_psd_spectral_centroid', \n",
    "    'midline_chain_post_ictal_psd_spectral_spread', \n",
    "    'midline_chain_post_ictal_psd_spectral_skewness', \n",
    "    'midline_chain_post_ictal_psd_spectral_kurtosis', \n",
    "    'post_ictal_wt_level0_energy_mean', \n",
    "    'post_ictal_wt_level0_energy_std', \n",
    "    'post_ictal_wt_level0_entropy_mean', \n",
    "    'post_ictal_wt_level0_entropy_std', \n",
    "    'post_ictal_wt_level0_mean_mean', \n",
    "    'post_ictal_wt_level0_mean_std', \n",
    "    'post_ictal_wt_level0_std_mean', \n",
    "    'post_ictal_wt_level0_std_std', \n",
    "    'post_ictal_wt_level0_max_mean', \n",
    "    'post_ictal_wt_level0_max_std', \n",
    "    'post_ictal_wt_level1_energy_mean', \n",
    "    'post_ictal_wt_level1_energy_std', \n",
    "    'post_ictal_wt_level1_entropy_mean', \n",
    "    'post_ictal_wt_level1_entropy_std', \n",
    "    'post_ictal_wt_level1_mean_mean', \n",
    "    'post_ictal_wt_level1_mean_std', \n",
    "    'post_ictal_wt_level1_std_mean', \n",
    "    'post_ictal_wt_level1_std_std', \n",
    "    'post_ictal_wt_level1_max_mean', \n",
    "    'post_ictal_wt_level1_max_std', \n",
    "    'post_ictal_wt_level2_energy_mean', \n",
    "    'post_ictal_wt_level2_energy_std', \n",
    "    'post_ictal_wt_level2_entropy_mean', \n",
    "    'post_ictal_wt_level2_entropy_std', \n",
    "    'post_ictal_wt_level2_mean_mean', \n",
    "    'post_ictal_wt_level2_mean_std', \n",
    "    'post_ictal_wt_level2_std_mean', \n",
    "    'post_ictal_wt_level2_std_std', \n",
    "    'post_ictal_wt_level2_max_mean', \n",
    "    'post_ictal_wt_level2_max_std', \n",
    "    'post_ictal_wt_level3_energy_mean', \n",
    "    'post_ictal_wt_level3_energy_std', \n",
    "    'post_ictal_wt_level3_entropy_mean', \n",
    "    'post_ictal_wt_level3_entropy_std', \n",
    "    'post_ictal_wt_level3_mean_mean', \n",
    "    'post_ictal_wt_level3_mean_std', \n",
    "    'post_ictal_wt_level3_std_mean', \n",
    "    'post_ictal_wt_level3_std_std', \n",
    "    'post_ictal_wt_level3_max_mean', \n",
    "    'post_ictal_wt_level3_max_std', \n",
    "    'post_ictal_wt_level4_energy_mean', \n",
    "    'post_ictal_wt_level4_energy_std', \n",
    "    'post_ictal_wt_level4_entropy_mean', \n",
    "    'post_ictal_wt_level4_entropy_std', \n",
    "    'post_ictal_wt_level4_mean_mean', \n",
    "    'post_ictal_wt_level4_mean_std', \n",
    "    'post_ictal_wt_level4_std_mean', \n",
    "    'post_ictal_wt_level4_std_std', \n",
    "    'post_ictal_wt_level4_max_mean', \n",
    "    'post_ictal_wt_level4_max_std', \n",
    "    'post_ictal_wt_level5_energy_mean', \n",
    "    'post_ictal_wt_level5_energy_std', \n",
    "    'post_ictal_wt_level5_entropy_mean', \n",
    "    'post_ictal_wt_level5_entropy_std', \n",
    "    'post_ictal_wt_level5_mean_mean', \n",
    "    'post_ictal_wt_level5_mean_std', \n",
    "    'post_ictal_wt_level5_std_mean', \n",
    "    'post_ictal_wt_level5_std_std', \n",
    "    'post_ictal_wt_level5_max_mean', \n",
    "    'post_ictal_wt_level5_max_std', \n",
    "    'post_ictal_wt_packet_entropy', \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d5a35",
   "metadata": {},
   "source": [
    "## Plot Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ea82c",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_numerical_distributions(df, output_path=\"plot_outputs\"):\n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get numerical columns\n",
    "    numerical_cols = [col for col in df.columns \n",
    "                     if df[col].dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "                                          pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "                                          pl.Float32, pl.Float64]]\n",
    "    \n",
    "    print(f\"Found {len(numerical_cols)} numerical columns to plot\")\n",
    "    \n",
    "    # Plot each numerical column\n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        # Get the data for this column (remove nulls)\n",
    "        data = df[col].drop_nulls().to_numpy()\n",
    "        \n",
    "        if len(data) > 0:\n",
    "            # Create histogram\n",
    "            ax.hist(data, bins='auto', alpha=0.7, color='steelblue', edgecolor='black')\n",
    "            \n",
    "            # Add labels and title\n",
    "            ax.set_xlabel(col, fontsize=12)\n",
    "            ax.set_ylabel('Frequency', fontsize=12)\n",
    "            ax.set_title(f'Distribution of {col}', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # Add grid\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add statistics text box\n",
    "            stats_text = (f'Mean: {np.mean(data):.2e}\\n'\n",
    "                         f'Std: {np.std(data):.2e}\\n'\n",
    "                         f'Min: {np.min(data):.2e}\\n'\n",
    "                         f'Max: {np.max(data):.2e}')\n",
    "            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, \n",
    "                    verticalalignment='top', \n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "            \n",
    "            # Save the figure\n",
    "            plt.tight_layout()\n",
    "            save_path = Path(output_path) / f'{col}_distribution.png'\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Plotted {i + 1}/{len(numerical_cols)} columns\")\n",
    "        else:\n",
    "            print(f\"Skipped {col} - no non-null values\")\n",
    "            plt.close()\n",
    "    \n",
    "    print(f\"Completed! All {len(numerical_cols)} plots saved to '{output_path}' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a245ad",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#plot_numerical_distributions(full_processed_df, \"plot_outputs/processed_data_distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a42e7d",
   "metadata": {},
   "source": [
    "## Plot Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517cfc3",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_categorical_distributions(df, output_path=\"plot_outputs\", max_categories=50):\n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get categorical, boolean, and string columns\n",
    "    categorical_cols = [col for col in df.columns \n",
    "                       if df[col].dtype in [pl.Utf8, pl.Boolean, pl.Categorical]]\n",
    "    \n",
    "    print(f\"Found {len(categorical_cols)} categorical/boolean/object columns to plot\")\n",
    "    \n",
    "    # Plot each categorical column\n",
    "    for i, col in enumerate(categorical_cols):\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Calculate value counts\n",
    "        value_counts = df[col].value_counts().sort(by='count', descending=True)\n",
    "        \n",
    "        # Handle null counts\n",
    "        null_count = df[col].null_count()\n",
    "        total_count = len(df)\n",
    "        \n",
    "        # Limit to max_categories if necessary\n",
    "        if len(value_counts) > max_categories:\n",
    "            value_counts = value_counts.head(max_categories)\n",
    "            truncated = True\n",
    "        else:\n",
    "            truncated = False\n",
    "        \n",
    "        if len(value_counts) > 0:\n",
    "            # Extract categories and counts\n",
    "            categories = value_counts[col].to_list()\n",
    "            counts = value_counts['count'].to_list()\n",
    "            \n",
    "            # Convert None to 'NULL' for display\n",
    "            categories = ['NULL' if cat is None else str(cat) for cat in categories]\n",
    "            \n",
    "            # Create bar plot\n",
    "            bars = ax.bar(range(len(categories)), counts, color='steelblue', \n",
    "                          alpha=0.7, edgecolor='black')\n",
    "            \n",
    "            # Customize x-axis\n",
    "            ax.set_xticks(range(len(categories)))\n",
    "            ax.set_xticklabels(categories)\n",
    "            \n",
    "            # Rotate labels if there are many categories\n",
    "            if len(categories) > 10:\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "            elif len(categories) > 5:\n",
    "                plt.xticks(rotation=30, ha='right')\n",
    "            \n",
    "            # Truncate long labels\n",
    "            current_labels = ax.get_xticklabels()\n",
    "            new_labels = []\n",
    "            for label in current_labels:\n",
    "                text = label.get_text()\n",
    "                if len(text) > 30:\n",
    "                    text = text[:27] + '...'\n",
    "                new_labels.append(text)\n",
    "            ax.set_xticklabels(new_labels)\n",
    "            \n",
    "            # Add value labels on top of bars\n",
    "            for bar, count in zip(bars, counts):\n",
    "                height = bar.get_height()\n",
    "                percentage = (count / total_count) * 100\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{count}\\n({percentage:.1f}%)',\n",
    "                       ha='center', va='bottom', fontsize=8)\n",
    "            \n",
    "            # Add labels and title\n",
    "            ax.set_xlabel(col, fontsize=12)\n",
    "            ax.set_ylabel('Frequency', fontsize=12)\n",
    "            title = f'Distribution of {col}'\n",
    "            if truncated:\n",
    "                title += f' (Top {max_categories} categories)'\n",
    "            ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # Add grid\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Add statistics text box\n",
    "            unique_count = df[col].n_unique()\n",
    "            stats_text = (f'Unique values: {unique_count}\\n'\n",
    "                         f'Total records: {total_count}\\n'\n",
    "                         f'Null values: {null_count} ({(null_count/total_count)*100:.1f}%)')\n",
    "            if truncated:\n",
    "                stats_text += f'\\n(Showing top {max_categories} only)'\n",
    "            \n",
    "            ax.text(0.98, 0.98, stats_text, transform=ax.transAxes, \n",
    "                    verticalalignment='top', horizontalalignment='right',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "            \n",
    "            # Adjust layout to prevent label cutoff\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save the figure\n",
    "            save_path = Path(output_path) / f'{col}_distribution.png'\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"Plotted {i + 1}/{len(categorical_cols)} columns\")\n",
    "        else:\n",
    "            print(f\"Skipped {col} - no data to plot\")\n",
    "            plt.close()\n",
    "    \n",
    "    print(f\"Completed! All {len(categorical_cols)} plots saved to '{output_path}' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a815",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#plot_categorical_distributions(processed_df, \"plot_outputs/processed_data_distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53943fd7",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4e4f78",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(df, output_path):\n",
    "    # Feature lists - empty for manual configuration\n",
    "    features_to_remove = [\n",
    "        'seizure_time_hjorth_mobility_mean', \n",
    "        'seizure_time_hjorth_complexity_mean',\n",
    "    ]\n",
    "    features_to_keep = []\n",
    "    key_features = [\n",
    "          'lstm_output_mean', \n",
    "    'lstm_output_std', \n",
    "    'lstm_output_max', \n",
    "    'lstm_output_min', \n",
    "    'lstm_hidden_mean', \n",
    "    'lstm_hidden_std', \n",
    "    'lstm_temporal_trend', \n",
    "    'lstm_forward_backward_diff', \n",
    "    'lstm_early_energy', \n",
    "    'lstm_middle_energy', \n",
    "    'lstm_late_energy', \n",
    "    \n",
    "    ]\n",
    "    target_variables = [\n",
    "       'seizure_type', 'localization', 'lateralization'\n",
    "        ]\n",
    "    \n",
    "    # Combine features and targets\n",
    "    features_to_analyze = key_features + target_variables\n",
    "    \n",
    "    if len(features_to_analyze) > 1:\n",
    "        plt.figure(figsize=(28, 28))\n",
    "        \n",
    "        # Calculate correlation matrix directly in polars\n",
    "        correlation_matrix = df.select(features_to_analyze).corr()\n",
    "        \n",
    "        # Convert just the correlation matrix to numpy for plotting\n",
    "        corr_array = correlation_matrix.to_numpy()\n",
    "        \n",
    "        # Create mask to highlight target correlations\n",
    "        num_targets = len(target_variables)\n",
    "        mask = np.zeros_like(corr_array)\n",
    "        if num_targets > 0:\n",
    "            mask[:-num_targets, :-num_targets] = True\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(corr_array, \n",
    "                   annot=True, \n",
    "                   cmap='coolwarm', \n",
    "                   center=0,\n",
    "                   fmt='.2f',\n",
    "                   mask=mask,\n",
    "                   vmin=-1, \n",
    "                   vmax=1,\n",
    "                   xticklabels=features_to_analyze,\n",
    "                   yticklabels=features_to_analyze)\n",
    "        \n",
    "        plt.title('Feature-Target Correlations')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_path}/lstm_correlation_matrix.png', bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        # Print sorted correlations with targets\n",
    "        if len(target_variables) > 0:\n",
    "            print(\"\\nCorrelations with targets (sorted):\")\n",
    "            for i, target in enumerate(target_variables):\n",
    "                target_idx = features_to_analyze.index(target)\n",
    "                correlations = [(features_to_analyze[j], corr_array[target_idx, j]) \n",
    "                               for j in range(len(features_to_analyze)) if j != target_idx]\n",
    "                correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "                \n",
    "                print(f\"\\n{target} correlations:\")\n",
    "                for feat, corr in correlations:\n",
    "                    print(f\"  {feat}: {corr:.3f}\")\n",
    "\n",
    "    return features_to_keep, features_to_remove, key_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e8859a",
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T21:57:32.022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#features_to_keep, features_to_remove, key_features = plot_correlation_matrix(encoded_df, plot_output_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8168099,
     "sourceId": 12997471,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
